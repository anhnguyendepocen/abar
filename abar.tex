\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods},
            pdfauthor={Bradley C. Boehmke and Brandon M. Greenwell},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Advanced Business Analytics with R: Descriptive, Predictive, and
Prescriptive Methods}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Bradley C. Boehmke and Brandon M. Greenwell}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2018-10-21}

\usepackage{booktabs}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\textbf{TODO:}

\begin{itemize}
\item
  Rewrite/fill in each section
\item
  Somewhere we need to highlight that a lot of good tips are given in
  the code comments as well!!
\item
  Is it ``in R'' or ``with R'' in the book title/description? I've seen
  both in a few places. I'm starting to think ``with R'' is the better
  choice? 🤔 I agree\ldots{}it should be ``\ldots{}with R''
\item
  Damn, do I need to get rid of the \texttt{emo::ji}s in this book!?
\item
  Rewrite more specific to this book
\end{itemize}

Welcome to Advanced Business Analytics in R. This book provides
practical modules for many of the descriptive, predictive, and
prescriptive analytic methodologies\ldots{}

\hypertarget{who-this-book-is-for}{%
\section*{Who this book is for}\label{who-this-book-is-for}}
\addcontentsline{toc}{section}{Who this book is for}

We intend this work to be a practitioner's guide to the machine learning
process and a place where one can come to learn about the approach and
to gain intuition about the many commonly used, modern, and powerful
methods accepted in the machine learning community. If you are familiar
with the analytic methodologies, this book may still serve as a
reference for how to work with the various R packages for
implementation. While an abundance of videos, blog posts, and tutorials
exist online, I've long been frustrated by the lack of consistency,
completeness, and bias towards singular packages for implementation.
This is what inspired this book. Part I of this book assumes basic
knowledge of statistics (e.g., hypothesis testing, effect sizes, and
confidence intervals). Part II assumes knowledge of regression and some
knowledge of optimization (e.g., through business calculus). Part III
\ldots{}

\hypertarget{who-this-book-is-not-for}{%
\section*{Who this book is not for}\label{who-this-book-is-not-for}}
\addcontentsline{toc}{section}{Who this book is not for}

\textbf{FIXME:} But this book is not just about machine learning? I
know, this was just filler material I copied-n-pasted.

Instead, this book is meant to help R users learn to use the machine
learning stack within R, which includes using various R packages such as
\texttt{glmnet}, \texttt{h2o}, \texttt{ranger}, \texttt{xgboost},
\texttt{lime}, and others to effectively model and gain insight from
your data. The book favors a hands-on approach, growing an intuitive
understanding of machine learning through concrete examples and just a
little bit of theory. While you can read this book without opening R, I
highly recommend you experiment with the code examples provided
throughout.

\hypertarget{who-this-book-is-really-not-for}{%
\section*{Who this book is really not
for}\label{who-this-book-is-really-not-for}}
\addcontentsline{toc}{section}{Who this book is really not for}

TBD.

\hypertarget{why-r}{%
\section*{Why R}\label{why-r}}
\addcontentsline{toc}{section}{Why R}

R has emerged over the last couple decades as a first-class tool for
scientific computing tasks, and has been a consistent leader in
implementing fundamental and advanced methodologies for analyzing data.
The usefulness of R for data science stems from the large, active, and
growing ecosystem of third-party packages: \texttt{tidyverse} for common
data analysis activities; \texttt{glmnet}, \texttt{ranger},
\texttt{xgboost}, and others for fast and scalable machine learning;
\texttt{lime}, \texttt{pdp}, \texttt{DALEX}, and others for machine
learning interpretability; and many more tools will be mentioned
throughout the pages that follow.

\hypertarget{how-this-book-is-organized}{%
\section*{How this book is organized}\label{how-this-book-is-organized}}
\addcontentsline{toc}{section}{How this book is organized}

Each chapter of this book focuses on a particular part of the
descriptive, predictive, and prescriptive analytic processes along with
various packages to perform those processes.

TBD\ldots{}

There are many great resources available to learn about machine
learning. At the end of each chapter we provide a \emph{Learn More}
section that lists resources that we have found extremely useful for
digging deeper into the methodology and applying with code.

\hypertarget{conventions-used-in-this-book}{%
\section*{Conventions used in this
book}\label{conventions-used-in-this-book}}
\addcontentsline{toc}{section}{Conventions used in this book}

The following typographical conventions are used in this book:

\begin{itemize}
\tightlist
\item
  \emph{italic}: indicates new terms,
\item
  \textbf{bold}: indicates package \& file names,
\item
  \texttt{inline\ code}: indicates commands or other text that could be
  typed literally by the user (function names are followed by
  parentheses (i.e. \texttt{blogdown::serve\_site()})),
\item
  use of double-colon operator \texttt{::} means accessing an object
  from a package (i.e. \texttt{dplyr::filter()} uses the
  \texttt{filter()} function from the \textbf{dplyr} package),
\item
  code chunk: indicates commands or other text that could be typed
  literally by the user. We do not add prompts (\texttt{\textgreater{}}
  and \texttt{+}) to R source code in this book, and we comment out the
  text output with two hashes \texttt{\#\#} by default
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2}
\NormalTok{## [1] 3}
\end{Highlighting}
\end{Shaded}

In addition to the general text used throughout, you will notice the
following code chunks with images, which signify:

\begin{tip}
This block signifies a tip or suggestion
\end{tip}

\begin{note}
This block signifies a general note
\end{note}

\begin{warning}
This block signifies a warning or caution
\end{warning}

\hypertarget{using-code-examples}{%
\section*{Using code examples}\label{using-code-examples}}
\addcontentsline{toc}{section}{Using code examples}

TBD.

\hypertarget{feedback}{%
\section*{Feedback}\label{feedback}}
\addcontentsline{toc}{section}{Feedback}

Reader comments are greatly appreciated. To report errors or bugs please
post an issue at \url{https://github.com/koalaverse/abar/issues}.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

TBD

\hypertarget{software-information}{%
\section*{Software information}\label{software-information}}
\addcontentsline{toc}{section}{Software information}

An online version of this book is available at \textbf{TBD}. The source
of the book is available at \url{https://github.com/koalaverse/abar}.
The book is powered by \url{https://bookdown.org} which makes it easy to
turn R markdown files into HTML, PDF, and EPUB.

This book was built with the following packages and R version.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# packages used}
\NormalTok{pkgs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
  \StringTok{"AmesHousing"}\NormalTok{,}
  \StringTok{"arules"}\NormalTok{,}
  \StringTok{"boot"}\NormalTok{, }
  \StringTok{"bookdown"}\NormalTok{,}
  \StringTok{"caret"}\NormalTok{,}
  \StringTok{"cluster"}\NormalTok{,}
  \StringTok{"earth"}\NormalTok{,}
  \StringTok{"factoextra"}\NormalTok{,}
  \StringTok{"ggplot2"}\NormalTok{,}
  \StringTok{"glmnet"}\NormalTok{,}
  \StringTok{"gridExtra"}\NormalTok{,}
  \StringTok{"knitr"}\NormalTok{, }
  \StringTok{"pdp"}\NormalTok{,}
  \StringTok{"randomForest"}\NormalTok{,}
  \StringTok{"ranger"}\NormalTok{,}
  \StringTok{"rmarkdown"}\NormalTok{,}
  \StringTok{"rsample"}\NormalTok{,}
  \StringTok{"stats"}\NormalTok{,}
  \StringTok{"tibble"}\NormalTok{,}
  \StringTok{"vip"}
\NormalTok{)}

\CommentTok{# package & session info}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{session_info}\NormalTok{(pkgs)}
\NormalTok{## Session info -----------------------------------------}
\NormalTok{##  setting  value                       }
\NormalTok{##  version  R version 3.5.1 (2018-07-02)}
\NormalTok{##  system   x86_64, darwin15.6.0        }
\NormalTok{##  ui       X11                         }
\NormalTok{##  language (EN)                        }
\NormalTok{##  collate  en_US.UTF-8                 }
\NormalTok{##  tz       America/New_York            }
\NormalTok{##  date     2018-10-21}
\NormalTok{## Packages ---------------------------------------------}
\NormalTok{##  package       * version    date      }
\NormalTok{##  abind           1.4-5      2016-07-21}
\NormalTok{##  AmesHousing     0.0.3      2017-12-17}
\NormalTok{##  arules          1.6-1      2018-04-07}
\NormalTok{##  assertthat      0.2.0      2017-04-11}
\NormalTok{##  backports       1.1.2      2017-12-13}
\NormalTok{##  base64enc       0.1-3      2015-07-28}
\NormalTok{##  BH              1.66.0-1   2018-02-13}
\NormalTok{##  bindr           0.1.1      2018-03-13}
\NormalTok{##  bindrcpp        0.2.2      2018-03-29}
\NormalTok{##  bookdown        0.7.17     2018-08-14}
\NormalTok{##  boot            1.3-20     2017-08-06}
\NormalTok{##  broom           0.5.0      2018-07-17}
\NormalTok{##  car             3.0-0      2018-04-02}
\NormalTok{##  carData         3.0-1      2018-03-28}
\NormalTok{##  caret           6.0-80     2018-05-26}
\NormalTok{##  cellranger      1.1.0      2016-07-27}
\NormalTok{##  class           7.3-14     2015-08-30}
\NormalTok{##  cli             1.0.0      2017-11-05}
\NormalTok{##  cluster         2.0.7-1    2018-04-13}
\NormalTok{##  codetools       0.2-15     2016-10-05}
\NormalTok{##  colorspace      1.3-2      2016-12-14}
\NormalTok{##  compiler        3.5.1      2018-07-05}
\NormalTok{##  cowplot         0.9.3      2018-07-15}
\NormalTok{##  crayon          1.3.4      2017-09-16}
\NormalTok{##  curl            3.2        2018-03-28}
\NormalTok{##  CVST            0.2-2      2018-05-26}
\NormalTok{##  data.table      1.11.4     2018-05-27}
\NormalTok{##  datasets      * 3.5.1      2018-07-05}
\NormalTok{##  ddalpha         1.3.4      2018-06-23}
\NormalTok{##  dendextend      1.8.0      2018-05-03}
\NormalTok{##  DEoptimR        1.0-8      2016-11-19}
\NormalTok{##  digest          0.6.18     2018-10-10}
\NormalTok{##  dimRed          0.1.0      2017-05-04}
\NormalTok{##  diptest         0.75-7     2016-12-05}
\NormalTok{##  dplyr           0.7.6      2018-06-29}
\NormalTok{##  DRR             0.0.3      2018-01-06}
\NormalTok{##  earth           4.6.3      2018-05-07}
\NormalTok{##  ellipse         0.4.1      2018-01-05}
\NormalTok{##  evaluate        0.11       2018-07-17}
\NormalTok{##  factoextra      1.0.5      2017-08-22}
\NormalTok{##  FactoMineR      1.41       2018-05-04}
\NormalTok{##  fansi           0.2.3      2018-05-06}
\NormalTok{##  flashClust      1.01-2     2012-08-21}
\NormalTok{##  flexmix         2.3-14     2017-04-28}
\NormalTok{##  forcats         0.3.0      2018-02-19}
\NormalTok{##  foreach         1.4.4      2017-12-12}
\NormalTok{##  foreign         0.8-70     2017-11-28}
\NormalTok{##  fpc             2.1-11.1   2018-07-20}
\NormalTok{##  geometry        0.3-6      2015-09-09}
\NormalTok{##  ggplot2         3.0.0      2018-07-03}
\NormalTok{##  ggpubr          0.1.7      2018-06-23}
\NormalTok{##  ggrepel         0.8.0      2018-05-09}
\NormalTok{##  ggsci           2.9        2018-05-14}
\NormalTok{##  ggsignif        0.4.0      2017-08-03}
\NormalTok{##  glmnet          2.0-16     2018-04-02}
\NormalTok{##  glue            1.3.0      2018-08-14}
\NormalTok{##  gower           0.1.2      2017-02-23}
\NormalTok{##  graphics      * 3.5.1      2018-07-05}
\NormalTok{##  grDevices     * 3.5.1      2018-07-05}
\NormalTok{##  grid            3.5.1      2018-07-05}
\NormalTok{##  gridExtra       2.3        2017-09-09}
\NormalTok{##  gtable          0.2.0      2016-02-26}
\NormalTok{##  haven           1.1.2      2018-06-27}
\NormalTok{##  highr           0.7        2018-06-09}
\NormalTok{##  hms             0.4.2      2018-03-10}
\NormalTok{##  htmltools       0.3.6      2017-04-28}
\NormalTok{##  ipred           0.9-6      2017-03-01}
\NormalTok{##  iterators       1.0.10     2018-07-13}
\NormalTok{##  jsonlite        1.5        2017-06-01}
\NormalTok{##  kernlab         0.9-26     2018-04-30}
\NormalTok{##  KernSmooth      2.23-15    2015-06-29}
\NormalTok{##  knitr           1.20       2018-02-20}
\NormalTok{##  labeling        0.3        2014-08-23}
\NormalTok{##  lattice         0.20-35    2017-03-25}
\NormalTok{##  lava            1.6.2      2018-07-02}
\NormalTok{##  lazyeval        0.2.1      2017-10-29}
\NormalTok{##  leaps           3.0        2017-01-10}
\NormalTok{##  lme4            1.1-17     2018-04-03}
\NormalTok{##  lubridate       1.7.4      2018-04-11}
\NormalTok{##  magic           1.5-8      2018-01-26}
\NormalTok{##  magrittr        1.5        2014-11-22}
\NormalTok{##  maptools        0.9-3      2018-07-31}
\NormalTok{##  markdown        0.8        2017-04-20}
\NormalTok{##  MASS            7.3-50     2018-04-30}
\NormalTok{##  Matrix          1.2-14     2018-04-13}
\NormalTok{##  MatrixModels    0.4-1      2015-08-22}
\NormalTok{##  mclust          5.4.1      2018-06-27}
\NormalTok{##  methods       * 3.5.1      2018-07-05}
\NormalTok{##  mgcv            1.8-24     2018-06-23}
\NormalTok{##  mime            0.5        2016-07-07}
\NormalTok{##  minqa           1.2.4      2014-10-09}
\NormalTok{##  ModelMetrics    1.2.0      2018-08-10}
\NormalTok{##  modeltools      0.2-22     2018-07-16}
\NormalTok{##  munsell         0.5.0      2018-06-12}
\NormalTok{##  mvtnorm         1.0-8      2018-05-31}
\NormalTok{##  nlme            3.1-137    2018-04-07}
\NormalTok{##  nloptr          1.0.4      2017-08-22}
\NormalTok{##  nnet            7.3-12     2016-02-02}
\NormalTok{##  numDeriv        2016.8-1   2016-08-27}
\NormalTok{##  openxlsx        4.1.0      2018-05-26}
\NormalTok{##  parallel        3.5.1      2018-07-05}
\NormalTok{##  pbkrtest        0.4-7      2017-03-15}
\NormalTok{##  pdp             0.6.0      2017-07-20}
\NormalTok{##  pillar          1.3.0      2018-07-14}
\NormalTok{##  pkgconfig       2.0.1      2017-03-21}
\NormalTok{##  plogr           0.2.0      2018-03-25}
\NormalTok{##  plotmo          3.4.2      2018-07-03}
\NormalTok{##  plotrix         3.7-2      2018-05-27}
\NormalTok{##  pls             2.6-0      2016-12-18}
\NormalTok{##  plyr            1.8.4      2016-06-08}
\NormalTok{##  prabclus        2.2-6      2015-01-14}
\NormalTok{##  prodlim         2018.04.18 2018-04-18}
\NormalTok{##  purrr           0.2.5      2018-05-29}
\NormalTok{##  quantreg        5.36       2018-05-29}
\NormalTok{##  R6              2.2.2      2017-06-17}
\NormalTok{##  randomForest    4.6-14     2018-03-25}
\NormalTok{##  ranger          0.10.1     2018-06-04}
\NormalTok{##  RColorBrewer    1.1-2      2014-12-07}
\NormalTok{##  Rcpp            0.12.18    2018-07-23}
\NormalTok{##  RcppEigen       0.3.3.4.0  2018-02-07}
\NormalTok{##  RcppRoll        0.3.0      2018-06-05}
\NormalTok{##  readr           1.1.1      2017-05-16}
\NormalTok{##  readxl          1.1.0      2018-04-20}
\NormalTok{##  recipes         0.1.3      2018-06-16}
\NormalTok{##  rematch         1.0.1      2016-04-21}
\NormalTok{##  reshape2        1.4.3      2017-12-11}
\NormalTok{##  rio             0.5.10     2018-03-29}
\NormalTok{##  rlang           0.2.2      2018-08-16}
\NormalTok{##  rmarkdown       1.10       2018-06-11}
\NormalTok{##  robustbase      0.93-2     2018-07-27}
\NormalTok{##  rpart           4.1-13     2018-02-23}
\NormalTok{##  rprojroot       1.3-2      2018-01-03}
\NormalTok{##  rsample         0.0.2      2017-11-12}
\NormalTok{##  scales          1.0.0      2018-08-09}
\NormalTok{##  scatterplot3d   0.3-41     2018-03-14}
\NormalTok{##  sfsmisc         1.1-2      2018-03-05}
\NormalTok{##  sp              1.3-1      2018-06-05}
\NormalTok{##  SparseM         1.77       2017-04-23}
\NormalTok{##  splines         3.5.1      2018-07-05}
\NormalTok{##  SQUAREM         2017.10-1  2017-10-07}
\NormalTok{##  stats         * 3.5.1      2018-07-05}
\NormalTok{##  stats4          3.5.1      2018-07-05}
\NormalTok{##  stringi         1.2.4      2018-07-20}
\NormalTok{##  stringr         1.3.1      2018-05-10}
\NormalTok{##  survival        2.42-3     2018-04-16}
\NormalTok{##  TeachingDemos   2.10       2016-02-12}
\NormalTok{##  tibble          1.4.2      2018-01-22}
\NormalTok{##  tidyr           0.8.1      2018-05-18}
\NormalTok{##  tidyselect      0.2.4      2018-02-26}
\NormalTok{##  timeDate        3043.102   2018-02-21}
\NormalTok{##  tinytex         0.6        2018-07-07}
\NormalTok{##  tools           3.5.1      2018-07-05}
\NormalTok{##  trimcluster     0.1-2.1    2018-07-20}
\NormalTok{##  utf8            1.1.4      2018-05-24}
\NormalTok{##  utils         * 3.5.1      2018-07-05}
\NormalTok{##  vip             0.1.0      2018-08-22}
\NormalTok{##  viridis         0.5.1      2018-03-29}
\NormalTok{##  viridisLite     0.3.0      2018-02-01}
\NormalTok{##  whisker         0.3-2      2013-04-28}
\NormalTok{##  withr           2.1.2      2018-03-15}
\NormalTok{##  xfun            0.3        2018-07-06}
\NormalTok{##  yaml            2.2.0      2018-07-25}
\NormalTok{##  zip             1.0.0      2017-04-25}
\NormalTok{##  source                           }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  Github (rstudio/bookdown@02b0fd1)}
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  cran (@0.6.18)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  Github (tidyverse/glue@a292148)  }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  local                            }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  cran (@1.2.0)                    }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  cran (@0.2.2)                    }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.1)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  local                            }
\NormalTok{##  Github (koalaverse/vip@32890d0)  }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)                   }
\NormalTok{##  CRAN (R 3.5.0)}
\end{Highlighting}
\end{Shaded}

\hypertarget{author}{%
\chapter*{Who are these Guys?}\label{author}}
\addcontentsline{toc}{chapter}{Who are these Guys?}

This book is\ldots{}

\hypertarget{bradley-c.-boehmke}{%
\section*{Bradley C. Boehmke}\label{bradley-c.-boehmke}}
\addcontentsline{toc}{section}{Bradley C. Boehmke}

Brad Boehmke is a Data Scientist at 84.51°, where he and his team
develop algorithmic processes, solutions, and tools that enable 84.51°
and its analysts to efficiently extract insights from data and provide
solution alternatives to decision-makers. Brad is also an adjunct
professor at the University of Cincinnati, Wake Forest, and Air Force
Institute of Technology. He's an active contributor to the R community
through package development (i.e. \texttt{anomalyDetection},
\texttt{sure}, \texttt{vip}) and giving back via advanced machine
learning education available at the \href{http://uc-r.github.io/}{UC
Business Analytics R Programming Guide}. You can follow Brad on Twitter
(\url{https://twitter.com/bradleyboehmke}) and GitHub
(\url{https://github.com/bradleyboehmke}).

\hypertarget{brandon-m.-greenwell}{%
\section*{Brandon M. Greenwell}\label{brandon-m.-greenwell}}
\addcontentsline{toc}{section}{Brandon M. Greenwell}

Brandon Greenwell is\ldots{}

You can follow me on Twitter (\url{https://twitter.com/bgreenwell8}) and
GitHub (\url{https://github.com/bgreenwell}).

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

TODO

\hypertarget{descriptive}{%
\section{Descriptive}\label{descriptive}}

TODO

\hypertarget{predictive}{%
\section{Predictive}\label{predictive}}

TODO

\hypertarget{prescriptive}{%
\section{Prescriptive}\label{prescriptive}}

TODO

\hypertarget{data}{%
\section{Data sets}\label{data}}

We have strived to use data sets throughout the book that represent real
world applications. An introduction to each data set, as well as it's
source and how to import it, is given in the following subsections.

\hypertarget{ames-iowa-housing-data}{%
\subsection*{Ames Iowa housing data}\label{ames-iowa-housing-data}}
\addcontentsline{toc}{subsection}{Ames Iowa housing data}

\textbf{TODO}: introduce theoretical business problem

Suppose that you are an analyst working for a local real estate firm in
Ames, Iowa. You have been asked to develop a model to help real estate
agents

\citet{ames-cock-2011} describes a data set containing the sale of
individual residential property in Ames, Iowa from 2006 to 2010. The
data set contains 2930 observations and a large number of explanatory
variables involved in assessing home values. These data offer a
contemporary alternative to the often used Boston housing data described
in \citet{harrison1978hedonic}.

The Ames housing data, which we refer to as simply the \texttt{ames}
data, are available in the \texttt{AmesHousing} package
\citep{pkg-AmesHousing} which is available from CRAN:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"AmesHousing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The raw data are also available from Kaggle:
\url{https://www.kaggle.com/c/house-prices-advanced-regression-techniques}.
In the code chunk below, we use the \texttt{make\_ames()} function from
\texttt{AmesHousing} to create a processed version of the data. For full
details on the difference between the processed and raw versions of the
\texttt{ames} data, see the help file \texttt{?AmesHousing::make\_ames}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}
\KeywordTok{dim}\NormalTok{(ames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2930   81
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(ames, class))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  factor integer numeric 
##      46      23      12
\end{verbatim}

The function \texttt{make\_ames()} returns a \texttt{"tibble"} object,
rather than just an R data frame. For more information on
\emph{tibbles}, see the corresponding vignette available in the
\texttt{tibble} package \citep{R-tibble}
\texttt{browseVignettes(package\ =\ "tibble")}. Running the code chunk
above, we see that there are 2930 observations on 81 variables (46 are
factors, 23 are integer valued, and 12 are numeric).

\hypertarget{employee-attrition-data}{%
\subsection*{Employee attrition data}\label{employee-attrition-data}}
\addcontentsline{toc}{subsection}{Employee attrition data}

\textbf{TODO}: introduce theoretical business problem

Due to the continuing concerns organizations have with attracting and
retaining top talent, the
\href{https://www.ibm.com/communities/analytics/watson-analytics-blog}{IBM
Watson Analytics Lab} published an employee attrition data set that
allows analysts to explore factors that lead to employee attrition and
investigate important questions such as `show me a breakdown of distance
from home by job role and attrition' or `compare average monthly income
by education and attrition'.

The employee attrition data, which we refer to as simply the
\texttt{churn} data, are available in the \texttt{rsample} package
\citep{pkg-rsample} which is available from CRAN:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"rsample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The raw data are also available from IBM Watson Analytics Lab:
\url{https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/}.
In the code chunk below, we import the attrition data from
\texttt{rsample}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition}
\KeywordTok{dim}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1470   31
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 1,470
## Variables: 31
## $ Age                      <int> 41, 49, 37, 33, 2...
## $ Attrition                <fct> Yes, No, Yes, No,...
## $ BusinessTravel           <fct> Travel_Rarely, Tr...
## $ DailyRate                <int> 1102, 279, 1373, ...
## $ Department               <fct> Sales, Research_D...
## $ DistanceFromHome         <int> 1, 8, 2, 3, 2, 2,...
## $ Education                <ord> College, Below_Co...
## $ EducationField           <fct> Life_Sciences, Li...
## $ EnvironmentSatisfaction  <ord> Medium, High, Ver...
## $ Gender                   <fct> Female, Male, Mal...
## $ HourlyRate               <int> 94, 61, 92, 56, 4...
## $ JobInvolvement           <ord> High, Medium, Med...
## $ JobLevel                 <int> 2, 2, 1, 1, 1, 1,...
## $ JobRole                  <fct> Sales_Executive, ...
## $ JobSatisfaction          <ord> Very_High, Medium...
## $ MaritalStatus            <fct> Single, Married, ...
## $ MonthlyIncome            <int> 5993, 5130, 2090,...
## $ MonthlyRate              <int> 19479, 24907, 239...
## $ NumCompaniesWorked       <int> 8, 1, 6, 1, 9, 0,...
## $ OverTime                 <fct> Yes, No, Yes, Yes...
## $ PercentSalaryHike        <int> 11, 23, 15, 11, 1...
## $ PerformanceRating        <ord> Excellent, Outsta...
## $ RelationshipSatisfaction <ord> Low, Very_High, M...
## $ StockOptionLevel         <int> 0, 1, 0, 0, 1, 0,...
## $ TotalWorkingYears        <int> 8, 10, 7, 8, 6, 8...
## $ TrainingTimesLastYear    <int> 0, 3, 3, 3, 3, 2,...
## $ WorkLifeBalance          <ord> Bad, Better, Bett...
## $ YearsAtCompany           <int> 6, 10, 0, 8, 2, 7...
## $ YearsInCurrentRole       <int> 4, 7, 0, 7, 2, 7,...
## $ YearsSinceLastPromotion  <int> 0, 1, 0, 3, 2, 3,...
## $ YearsWithCurrManager     <int> 5, 7, 0, 0, 2, 6,...
\end{verbatim}

Running the code chunk above, we see that there are 1470 observations on
31 variables, which consist of a mixture of integers, factors, and
ordered factors data types).

\hypertarget{part-descriptive-analytics}{%
\part{Descriptive
Analytics}\label{part-descriptive-analytics}}

\hypertarget{descriptive}{%
\chapter{Descriptive Statistics}\label{descriptive}}

The first step in any data analysis problem is to describe the data
using descriptive statistics and look at the data using appropriate
graphical techniques. In this chapter, we will introduce some basic
descriptive statistics (e.g., various measures of location and spread)
while graphical techniques are discussed in Chapter \ref{visualization}.

Descriptive statistics, in contrast to inferential statistics (Chapter
\ref{inference}), aim to describe a \emph{data sample}, or
\emph{sample}. A sample is simply a set of data collected from some
population of interest (e.g., the annual salaries of males at a
particular company). In this book, we refer to the individual sample
points as observations. Typically, the data are collected in a way such
that the sample is representative of the population from which it came.
Other times, we may have access to the entire population, in which case,
our sample comprises a census. In either case, descriptive statistics
seek to paint a quantitative picture of the data using simple values
such as measures of \emph{location} (i.e., what a typical values looks
like) and \emph{dispersion} (i.e., how spread out the data are). Beyond
measures of location and spread, we also discuss percentiles and simple
ways for detecting \emph{outliers} (i.e., unusually small or large
observations). Various other descriptive statistics and useful R
packages are introduced in the exercises at the end of the chapter.

\hypertarget{prerequisites}{%
\section{Prerequisites}\label{prerequisites}}

The R package \texttt{stats} (which is part of the standard R
distribution) provides functions for many of the descriptive statistics
discussed in this chapter, plus more. For a complete list of functions,
type \texttt{library(help\ =\ "stats")} into the R console.

For illustration, we will use two data sets: (1) the Ames housing data
set \citep{ames-cock-2011} (in particular, the column labelled
\texttt{Sale\_Price}) and (2) the Adult data set \citep{uci}; both of
which are described in Chapter \ref{intro}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ames housing data}
\KeywordTok{dim}\NormalTok{(ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{())  }\CommentTok{# construct data and print dimensions}
\NormalTok{## [1] 2930   81}
\KeywordTok{head}\NormalTok{(Sale_Price <-}\StringTok{ }\NormalTok{ames}\OperatorTok{$}\NormalTok{Sale_Price)  }\CommentTok{# extract Sale_Price column}
\NormalTok{## [1] 215000 105000 172000 244000 189900 195500}

\CommentTok{# Adult data}
\KeywordTok{data}\NormalTok{(AdultUCI, }\DataTypeTok{package =} \StringTok{"arules"}\NormalTok{)  }\CommentTok{# load data from arules package}
\KeywordTok{dim}\NormalTok{(AdultUCI)  }\CommentTok{# print dimensions}
\NormalTok{## [1] 48842    15}
\end{Highlighting}
\end{Shaded}

\hypertarget{measures-of-location}{%
\section{Measures of location}\label{measures-of-location}}

The first question we might ask of the \texttt{ames} data is ``What is a
typical value for the selling price?'' In particular, we are interested
in some measure of \emph{central location} or \emph{central tendency}.
Such measures try to summarize a set of values with a ``typical''
number. There are a number of different measures of location, but the
simplest and most commonly used is the \emph{arithmetic mean} or
\emph{sample mean}.

\hypertarget{the-sample-mean}{%
\subsection{The sample mean}\label{the-sample-mean}}

Suppose we have a set of \(n\) observations denoted
\(x_1, x_2, \dots, x_n\). The sample mean, denoted \(\bar{x}\), is
defined as the sum of the observations divided \(n\): \begin{equation}
\label{eqn:sample-mean}
  \bar{x} = \frac{1}{n}\sum_{i = 1}^n x_i = \frac{1}{n}\left(x_1 + x_2 + \dots + x_n\right),
\end{equation} where \(\sum\) is mathematical notation for summation and
simply means ``add up the values''. Another way to think of the sample
mean is as the ``center of gravity'' for a set of observations. That is,
if the observations were placed on a number line, like a teeter-totter,
the sample mean would be the balancing point. For example, the sample
mean of 1.7, 3.3, 7.5, 8.1, and 8.9 is \(\bar{x} = 5.9\) which is
displayed in Figure \ref{fig:teeter-totter}.

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/teeter-totter-1} 

}

\caption{The sample mean as the balancing point of a set of five observations.}\label{fig:teeter-totter}
\end{figure}

In R, we can obtain the sample mean of a set of observations using the
\texttt{mean()} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{1.7}\NormalTok{, }\FloatTok{3.3}\NormalTok{, }\FloatTok{7.5}\NormalTok{, }\FloatTok{8.1}\NormalTok{, }\FloatTok{8.9}\NormalTok{))}
\NormalTok{## [1] 5.9}
\end{Highlighting}
\end{Shaded}

For example, the sample mean of \texttt{Sale\_Price} can be obtained
using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Sale_Price)}
\NormalTok{## [1] 180796}
\KeywordTok{mean}\NormalTok{(Sale_Price)  }\CommentTok{# alternatively}
\NormalTok{## [1] 180796}
\end{Highlighting}
\end{Shaded}

Thus, a typical or central value of selling price for the \texttt{ames}
data frame would be \$180,796.

\BeginKnitrBlock{note}
Most of the descriptive statistical functions in R include an
\texttt{na.rm} argument which defaults to \texttt{FALSE}. If
\texttt{na.rm\ =\ FALSE}, the return value for these functions will be
\texttt{NA} (i.e., R's representation of a missing value; see
\texttt{?NA} for details.) whenever the sample contains at least one
\texttt{NA}. If set to \texttt{TRUE}, then all \texttt{NA}s will be
removed from the sample prior to computing the statistic. This is
illustrated in the following code chunk:
\EndKnitrBlock{note}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{18}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{## [1] NA}
\KeywordTok{mean}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{## [1] 13.25}
\NormalTok{(}\DecValTok{18} \OperatorTok{+}\StringTok{ }\DecValTok{4} \OperatorTok{+}\StringTok{ }\DecValTok{12} \OperatorTok{+}\StringTok{ }\DecValTok{19}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{4}  \CommentTok{# sanity check}
\NormalTok{## [1] 13.25}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-sample-median}{%
\subsection{The sample median}\label{the-sample-median}}

One problem with the sample mean is that it is not \emph{robust} to
outliers. To illustrate, suppose we have a sample of size two: \(x_1\)
and \(x_2\) so that \(\bar{x} = \left(x_1 + x_2\right) / 2\). Then,
regardless of the value of \(x_1\), we can change \(x_2\) to achieve any
value for the sample mean. Since it only takes one of the \(n\)
observations to arbitrarily change \(\bar{x}\), we say that \(\bar{x}\)
has a finite sample breakdown point (FSBP) of \(1 / n\). The higher the
FSBP of a sample statistic, the less affected it is to outliers (i.e.,
the more robust it is). The highest FSBP a sample statistic can obtain
is 50\%.

A more robust measure of location is given by the \emph{sample median}.
Consider a sample of size \(n\): \(x_1, x_2, \dots, x_n\). Let
\(x_{\left(i\right)}\) denote the \(i\)-th observation after the sample
has been sorted in ascending order. The sample median, denoted \(M\), is
defined as \begin{equation*}
  M = 
  \begin{cases}
    x_{\left(m\right)} & \quad \text{if } n \text{ is odd}\\
    \left(x_{\left(m\right)} + x_{\left(m + 1\right)}\right) / 2 & \quad \text{if } n \text{ is even},
  \end{cases}
\end{equation*} where \(m = \left(n + 1\right) / 2\). In other words, if
\(n\) is odd, the sample median is just the middle number, otherwise, we
take the sample mean of the two middle numbers.

Since the sample median only depends on the middle number (or middle two
numbers), it is far more robust than the sample mean. In fact, the
sample median has an FSBP of roughly 50\%. In other words, close to 50\%
of the observations have to be outliers in order to affect the sample
median. This makes the sample median more useful in practice when
dealing with data that contain outliers or come from skewed
distributions.

In R, we can compute the sample median using the \texttt{median()}
function. For the Ames housing data, the median sale price is \$160,000,
which can be computed using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(Sale_Price)}
\NormalTok{## [1] 160000}
\KeywordTok{median}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)  }\CommentTok{# alternatively}
\NormalTok{## [1] 160000}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-mean-or-the-median}{%
\subsection{The mean or the median}\label{the-mean-or-the-median}}

So which should be used in practice, the sample mean or the sample
median? If the data are roughly \emph{symmetric}, then the sample mean
and sample median will be approximately equal. In fact, when the sample
mean is most useful for reporting, it will typically be close to the
sample median. If, however, the data are skewed to the left or right,
then the sample mean will tend to get pulled in the same direction. For
example, for right (or positively) skewed data, the sample mean will
typically be larger than the sample median (sometimes much larger). In
these cases, the sample median is a more reliable measure of location.
However, there is nothing wrong with reporting both statistics.

For the \texttt{ames} data frame, the sample median was smaller than the
sample mean by \$20,796.06. This is not surprising since data on housing
and sale prices tend to be skewed right which can inflate the sample
mean. In this case, the sample median will be a better measure of
location than the sample mean. Different approaches to detecting
skewness will be discussed in Chapter \ref{visualization} when we talk
about visualizing data.

\hypertarget{measures-of-spread}{%
\section{Measures of spread}\label{measures-of-spread}}

Measures of location by themselves are not very useful. We often want to
know how ``spread out'' the data are. This can be summarized using
various measures of \emph{spread} or \emph{dispersion}.

The most common measure of spread is the \emph{sample variance}, denoted
by \(s^2\). The sample variance of a sample is defined as
\begin{equation*}
  s ^ 2 = \sum_{i = 1} ^ n \left(x_i - \bar{x}\right) ^ 2 / \left(n - 1\right).
\end{equation*} That is, the sample variance is just the sum of the
squared deviations of each observation from the sample mean dived by
\(n - 1\). The \(n - 1\) is used to make \(s ^ 2\) an \emph{unbiased
estimator}\footnote{An unbiased estimator is one that does not, from
  sample to sample, systematically underestimate or overestimate a
  population parameter of interest. The sample mean is another example,
  as it is provides an unbiased estiamte of the population mean; that
  is, on average, the sample mean will be equal to the true population
  mean.} of the population variance. Since the sample variance involves
squaring the differences, it does not retain the original units, unlike
the sample mean and variance. Oftentimes the positive square root of the
sample variance, called the \emph{sample standard deviation}, is used
instead. The sample standard deviation, denoted \(s\), is more useful
because is has the same units as the original observations (e.g., feet,
dollars, etc.).

In R, the sample variance and sample standard deviation can be computed
using the functions \texttt{var()} and \texttt{sd()}, respectively. The
following example illustrates their use on the \texttt{ames} data frame
using the variable \texttt{Sale\_Price}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(Sale_Price)  }\CommentTok{# sample variance}
\NormalTok{## [1] 6.382e+09}
\KeywordTok{sd}\NormalTok{(Sale_Price)  }\CommentTok{# sample standard deviation}
\NormalTok{## [1] 79887}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{var}\NormalTok{(Sale_Price))  }\CommentTok{# sanity check}
\NormalTok{## [1] 79887}
\end{Highlighting}
\end{Shaded}

Since the sample standard deviation retains the original units, we can
report this in dollar amount (e.g., the standard deviation of sale
prices for homes sold from 2006--2010 is \$79,886.69).

\hypertarget{empirical-rule}{%
\subsection{The empirical rule}\label{empirical-rule}}

It is possible for the distribution of some of the variables in a data
sets to exhibit a ``bell shape'' 🔔. For bell-shaped distributions, the
\emph{empirical rule}, also known as the \emph{68-95-99.7 rule}, states
that (roughly) 68\% of the observations should fall within one standard
deviation of the mean, 95\% should fall within two standard deviations
of the mean, and 99.7\% should fall within three standard deviations of
the mean---this is illustrated in Figure \ref{fig:empiricial-rule}.
Therefore, data from bell-shaped distributions can be adequately
described my the sample mean and and standard deviation (if it exists).
The most important takeaway is that the majority of observations from a
bell-shaped distribution should be within a couple of standard
deviations from the mean, and it is extremely unlikely for an
observation to be beyond three standard deviations from the mean. This
provides an intuitive and simple rule for identifying potential outliers
(see Section \ref{outliers}).

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/empiricial-rule-1} 

}

\caption{The empricial rule for bell-shaped distributions. (In progress!)}\label{fig:empiricial-rule}
\end{figure}

To reiterate, the empirical rule applies to data that are (at least
approximately) bell-shaped. To ascertain the shape of the distribution
of a sample, a \emph{histogram} or \emph{kernel density estimate} can be
used---these are discussed in Chapter \ref{visualization}. A histogram
of the \texttt{Sale\_Price} data is displayed in the left side of Figure
\ref{fig:sale-price-hist}. These data are not bell-shaped, or even
symmetric---in fact, \texttt{Sale\_Price} appears to be skew right. Data
that are skew right can often be transformed to appear more bell-shaped
by taking a logarithm or square root transformation. A histogram of
\texttt{log(Sale\_Price)} is displayed in the right side of Figure
\ref{fig:sale-price-hist}. From the histograms, it is clear that
\texttt{Sale\_Price} is in fact skew right and that taking the logarithm
makes the distribution appear more bell-shaped---such transformations
are useful for some of the statistical inference procedures discussed in
Chapter \ref{inference} which assume that the data are approximately
normally distributed. \textbf{FIXME: What other methods in this book
assume normality (and therefore symmetry)? For example, regression.}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{abar_files/figure-latex/sale-price-hist-1}

\}

\textbackslash{}caption\{Histogram estimates of the distribution of
\texttt{Sale\_Price} (left) and \texttt{log(Sale\_Price)}
(right).\}\label{fig:sale-price-hist} \textbackslash{}end\{figure\}

\hypertarget{percentiles}{%
\section{Percentiles}\label{percentiles}}

The \(p\)-th percentile of a sample, denoted \(x_p\), is the value for
which \(p\) percent of the observations are less than \(x_p\). The
median, for example, is the 50-th percentile (i.e., the middle number).
Names are given to special groups of percentiles. \emph{Deciles}, for
example, divide a sample into ten equally sized buckets.
\emph{Quartiles} are the values that divide the data into four equally
sized groups---in other words, the quartiles of a sample consists of the
25-th, 50-th, and 75-th percentiles. The quartiles, denoted \(Q_1\),
\(Q_2\), and \(Q_3\) (\(Q_1\) and \(Q_3\) are also referred to as the
lower and upper quartiles, respectively), play an important part in many
descriptive and graphical analyses. Together with measures of location
and spread, the percentiles help describe the shape of the sample (i.e.,
what its distribution looks like). In fact, one of the most useful
graphics for describing a sample is the \emph{boxplot} which is
described in Chapter \ref{visualization}. The boxplot is a simple
visualization capturing the quartiles, median, as well as the maximum
and minimum values and is extremely effective at showing the shape of a
set of data (these five summary statistics are collectively known as
Tukey's \emph{five-number summary}). A modern alternative to boxplots,
called \emph{violin plots}, will also be discussed in Chapter
\ref{visualization}.

The formula for computing the \(p\)-th percentile for a sample is not
unique and many definitions exist. In fact, R includes nine different
algorithms (controlled via the \texttt{type} argument) for computing
percentiles! Therefore, it is important to realize that different
software may produce slightly different results when computing
percentiles.

\BeginKnitrBlock{note}
To reproduce the same quantiles provided by SAS\footnote{TBD.}, specify
\texttt{type\ =\ 3} in the call to \texttt{quantile()}. \textbf{FIXME:
This needs to be verified!}
\EndKnitrBlock{note}

The R function for computing percentiles is \texttt{quantile()}.
Quantiles are essentially the same as percentiles, but specified using
decimals rather than percentages. For example, the 5-th percentile is
equivalent to the 0.05 quantile. The following code chunk computes the
quartiles of \texttt{Sale\_Price} from the \texttt{ames} data frame. We
use the default algorithm (i.e., \texttt{type\ =\ 7}); for specifics,
see the help page \texttt{?stats::quantile}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(Sale_Price, }\DataTypeTok{probs =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\NormalTok{##    25%    50%    75% }
\NormalTok{## 129500 160000 213500}
\end{Highlighting}
\end{Shaded}

In other words, 25\% of the sale prices were below \$129,500, 25\%
between \$129,500 and \$160,000, 25\% between \$160,000 and \$213,500,
and the rest greater than \$213,500.

\hypertarget{robust-measures-of-spread}{%
\section{Robust measures of spread}\label{robust-measures-of-spread}}

Since the sample standard deviation relies on squared deviations from
the sample mean (a non-robust measure of location), it is also sensitive
to outliers. A measure of spread less affected by outliers is the
\emph{interquartile range} (IQR). The IQR is defined as the difference
between the upper and lower quartiles: \begin{equation*}
  \text{IQR} = Q_3 - Q_1.
\end{equation*} The IQR describes the variability of the middle 50\% of
the data and has an FSBP of 25\%. Therefore, the IQR is a more robust
measure of spread than the sample standard deviation.

Perhaps a more useful, but less often used, robust measure of spread is
the \emph{median absolute deviation} (MAD). For a sample
\(x_1, x_2, \dots, x_n\) with sample median \(M\), the MAD is given by
the median of the absolute value of the deviations from \(M\):
\begin{equation*}
  \text{MAD} = median\left(\left|x_1 - M\right|, \left|x_2 - M\right|, \dots, \left|x_n - M\right|\right).
\end{equation*} The MAD, like the median, has an FSBP of 50\%, meaning
nearly half the observations could be outliers without affecting the
MAD. Some software, including R by default, actually computes an
adjusted version of MAD, called MADN, by multiplying by the constant
\(1.4826\): \(\text{MADN} = 1.4826 \times \text{MAD}\). This adjustment
is to make MAD \emph{asymptotically normally consistent} for the
population standard deviation \(\sigma\). In other words, as the sample
size \(n\) gets larger, MADN is a good estimator of \(\sigma\) for a
normally distributed population. In practice, MADN is typically
used/reported.

To compute the IQR or MAD(N) in R, we can use the \texttt{IQR()} and
\texttt{mad()} functions, respectively. For the \texttt{ames} data
frame, the IQR and MAD(N) for \texttt{Sale\_Price} are computed below.
Note that since the IQR is based on the 25-th and 75-th percentiles, the
\texttt{IQR()} function also includes the option \texttt{type} for
specifying which algorithm to use for computing \(Q_1\) and \(Q_3\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{IQR}\NormalTok{(Sale_Price)}
\NormalTok{## [1] 84000}
\KeywordTok{mad}\NormalTok{(Sale_Price)  }\CommentTok{# MADN}
\NormalTok{## [1] 54856}
\KeywordTok{mad}\NormalTok{(Sale_Price, }\DataTypeTok{constant =} \DecValTok{1}\NormalTok{)  }\CommentTok{# MAD}
\NormalTok{## [1] 37000}
\KeywordTok{mad}\NormalTok{(Sale_Price, }\DataTypeTok{constant =} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\FloatTok{1.4826}  \CommentTok{# sanity check}
\NormalTok{## [1] 54856}
\end{Highlighting}
\end{Shaded}

In practice, it is common (and important) to report some measure of
spread whenever reporting measures of location (and vice versa). The
standard deviation is often reported with the mean. Whenever the median
is used, the IQR or MAD(N) is often reported as well.

\hypertarget{outliers}{%
\section{Outlier detection}\label{outliers}}

In this section, we present a few simple rules for detecting potential
outliers in univariate data; that is, data on a single variable. In
later chapters, we present more sophisticated methods for detecting
potential outliers and anomalies in multivariate data (i.e., data on
more than one variable).

The empirical rule (Section \ref{empirical-rule}) probably offers the
simplest method for detecting outliers, at least for reasonably
bell-shaped data. Recall that for approximately bell-shaped
distributions, 95\% of the observations should lie within two standard
deviations of the mean. Therefore, for approximately bell-shaped
distributions, \(z\)-scores greater than two in absolute value might be
considered unusual (a cutoff of 2.24 has been shown to be more useful in
practice \citep{wilcox-applying-2003}). To make this rule more robust,
we can compute a modified \(z\)-score based on the median and MADN.
Below, we define a simple function for detecting outliers according to
the empirical rule:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{detect_outliers <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, }\DataTypeTok{robust =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{cutoff =} \DecValTok{2}\NormalTok{) \{}
\NormalTok{  z_score <-}\StringTok{ }\ControlFlowTok{if}\NormalTok{ (robust) \{}
\NormalTok{    (x }\OperatorTok{-}\StringTok{ }\KeywordTok{median}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\KeywordTok{mad}\NormalTok{(x)  }\CommentTok{# modified z-score}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    (x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(x)  }\CommentTok{# z-score}
\NormalTok{  \}}
  \KeywordTok{sort}\NormalTok{(x[}\KeywordTok{abs}\NormalTok{(z_score) }\OperatorTok{>}\StringTok{ }\NormalTok{cutoff])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Next, we simulate some data from a standard normal distribution (i.e., a
bell-shaped distribution with mean zero and standard deviation one) with
two outliers (5 and -100) and test out our \texttt{detect\_outliers()}
function. For a standard normal distribution, we would expect any
observations greater than two in absolute value to be unlikely and
should be flagged as potential outliers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)  }\CommentTok{# for reproducibility}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{), }\DecValTok{5}\NormalTok{, }\DecValTok{-100}\NormalTok{)}
\KeywordTok{detect_outliers}\NormalTok{(x)}
\NormalTok{## [1] -100}
\KeywordTok{detect_outliers}\NormalTok{(x, }\DataTypeTok{robust =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{## [1] -100.000   -2.319   -2.073   -2.050    5.000}
\KeywordTok{detect_outliers}\NormalTok{(x, }\DataTypeTok{cutoff =} \FloatTok{2.24}\NormalTok{)  }\CommentTok{# following Wilcox (2003)}
\NormalTok{## [1] -100}
\end{Highlighting}
\end{Shaded}

Notice that five does not get flagged as an outlier when using the
standard \(z\)-score based on the sample mean and standard deviation.
This is due to the fact that the extreme value -100 skews these
descriptive statistics. Using the robust method, however, returns
reasonable results.

Using the empirical rule is rather limited in practice since
distributions are often not bell-shaped, or even symmetric---though, as
seen with \texttt{Sale\_Price}, some can be transformed to appear more
bell-shaped. A better outlier detection rule can be constructed using
the IQR. This is the same method used to flag outliers in boxplots (see
Section \ref{boxplots}). The general rule is to define an observation an
outlier if it lies outside the interval
\(\left(Q_1 - 1.5 \times IQR, Q_2 + 1.5 \times IQR\right)\). While it
would be simple to write our own function for detecting outliers using
the boxplot method, we can use R's built-in function
\texttt{boxplot.stats()} (see \texttt{?grDevices::boxplot.stats} for
details). Notice how this method happens to catch the true outliers!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot.stats}\NormalTok{(x)}\OperatorTok{$}\NormalTok{out}
\NormalTok{## [1]    5 -100}
\end{Highlighting}
\end{Shaded}

We leave it to Exercise \ref{ames-outlier} to further explore outliers
for the \texttt{Sale\_Price} variable in the \texttt{ames} housing data
frame.

\hypertarget{categorical}{%
\section{Describing categorical data}\label{categorical}}

All of the descriptive statistics discussed thus far are appropriate for
\emph{continuous variables}\footnote{Continuous variables, also referred
  to as quantitative variables, can be further categorized as
  \emph{interval} or \emph{ratio} variables. Though we do not make such
  distinction in this book, the interested reader is pointed to
  \textbf{NEED REFERENCE}}---that is, variables that can be measured on
a continuum (e.g., the weight of an object measured in grams).
Technically, no variable can be truly measured on a continuous scale due
to precision limitations with how all variables are measured. So, in a
sense, continuous variables in practice are numeric variables that can
take on a lot of values. Oftentimes the data we are describing contains
\emph{categorical variables}. A categorical variable is a variable whose
measurement scale consists of a set of categories (e.g., manufacturer
and gender). Such variables are easy to describe using
\emph{contingency} or \emph{cross-classification tables} (e.g., tables
of frequencies and proportions).

Categorical variables fall into one of two types: \emph{nominal} and
\emph{ordinal}. Nominal variables are categorical variables whose
categories do no have a natural ordering. The categories of an ordinal
variable do have a natural ordering, but no defined distance between the
categories. For example, the \texttt{AdultUCI} data frame contains the
columns \texttt{income} (with unique categories \texttt{"small"} and
\texttt{"large"}) and \texttt{sex} (with unique categories
\texttt{"Female"} and \texttt{"Male"}). \texttt{income} would be an
example of an ordered variable (since
\texttt{"small"\ \textless{}\ "large"}, but \texttt{"large"\ -\ "small"}
has no meaningful interpretation) while \texttt{sex} is nominal.

In R, categorical variables are typically represented using the
\texttt{"factor"} class, but can also be represented by character
strings (i.e., the more general \texttt{"character"} class). The
\texttt{AdultUCI} data set contains several factors:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(AdultUCI, is.factor))  }\CommentTok{# positions and names of factor columns}
\NormalTok{##      workclass      education marital-status }
\NormalTok{##              2              4              6 }
\NormalTok{##     occupation   relationship           race }
\NormalTok{##              7              8              9 }
\NormalTok{##            sex native-country         income }
\NormalTok{##             10             14             15}
\KeywordTok{which}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(AdultUCI, is.character))  }\CommentTok{# sanity check}
\NormalTok{## named integer(0)}
\KeywordTok{which}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(AdultUCI, is.ordered))  }\CommentTok{# check specifically for ordered factors}
\NormalTok{## education    income }
\NormalTok{##         4        15}
\end{Highlighting}
\end{Shaded}

To coerce a variable into a nominal or ordinal factor, we can use the
functions \texttt{as.factor()} and \texttt{as.ordered()}, respectively.
Some of the techniques discussed in this book can take ordinality into
account, so it is good practice to make sure such variables are coerced
to ordered factors.

\hypertarget{contingency-tables}{%
\subsection{Contingency tables}\label{contingency-tables}}

A \emph{contingency table} is a rectangular table containing the
frequencies (or proportions) of observations within the different
categories of one or more categorical variables. Contingency tables
typically cross-classifiy two categorical variables, but can be used to
cross-classifiy more than two. R contains a number of functions for
creating such tables, the most useful probably being \texttt{xtabs()}
since it has a formula interface. To illustrate, we construct a
\(2 \times 2\) table cross-classifying \texttt{income} and \texttt{sex}
from the \texttt{AdultUCI} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{xtabs}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{sex }\OperatorTok{+}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ AdultUCI))}
\NormalTok{##         income}
\NormalTok{## sex      small large}
\NormalTok{##   Female  9592  1179}
\NormalTok{##   Male   15128  6662}
\end{Highlighting}
\end{Shaded}

This table shows some disparity in income between females and males. In
Chapter \ref{inference}, we discuss ways of testing for association
between a set of categorical variables.

We can also request that margins (i.e., row/column summaries) be added
to the table using the \texttt{addmargins()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{addmargins}\NormalTok{(tab)  }\CommentTok{# defaults to adding row/column totals}
\NormalTok{##         income}
\NormalTok{## sex      small large   Sum}
\NormalTok{##   Female  9592  1179 10771}
\NormalTok{##   Male   15128  6662 21790}
\NormalTok{##   Sum    24720  7841 32561}
\KeywordTok{addmargins}\NormalTok{(tab, }\DataTypeTok{FUN =}\NormalTok{ mean)  }\CommentTok{# add sample means to the margins instead}
\NormalTok{## Margins computed over dimensions}
\NormalTok{## in the following order:}
\NormalTok{## 1: sex}
\NormalTok{## 2: income}
\NormalTok{##         income}
\NormalTok{## sex      small large  mean}
\NormalTok{##   Female  9592  1179  5386}
\NormalTok{##   Male   15128  6662 10895}
\NormalTok{##   mean   12360  3920  8140}
\end{Highlighting}
\end{Shaded}

We can can convert the frequencies to proportions using the
\texttt{prop.table()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(tab)}
\NormalTok{##         income}
\NormalTok{## sex        small   large}
\NormalTok{##   Female 0.29459 0.03621}
\NormalTok{##   Male   0.46460 0.20460}
\end{Highlighting}
\end{Shaded}

In Section \ref{cramers-v}, we discuss a statistic that can be used to
quantify the strength of the association between two categorical
variables.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\BeginKnitrBlock{exercise}[Groupwise descriptive statistics]
\protect\hypertarget{exr:unnamed-chunk-11}{}{\label{exr:unnamed-chunk-11}
\iffalse (Groupwise descriptive statistics) \fi{} }For the \texttt{ames}
data set, we computed various measures of spread and location for the
variable \texttt{Sale\_Price}. However, these data span multiple years
(2006--2010). Therefore, it might be of interest to see how various
descriptive statistics change over time. For this exercise, compute the
sample median and MADN for \texttt{Sale\_Price} stratified by the year
in which the house was sold (i.e., \texttt{Year\_Sold}). \textbf{Hint:}
use the built-in functions \texttt{tapply()} or \texttt{by()}; see
\texttt{?tapply} and \texttt{?by} for example usage. Do typical sale
prices seem to be different across the five years?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[Outlier detection]
\protect\hypertarget{exr:ames-outlier}{}{\label{exr:ames-outlier}
\iffalse (Outlier detection) \fi{} }Use the boxplot outlier detection
method on the variable \texttt{Sale\_Price} from the \texttt{ames}
housing example. How many outliers are detected using this method? How
many outliers are detected using the empirical rule? Does the empirical
rule seem appropriate for these data? Why or why not?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[Kurtosis and skewness]
\protect\hypertarget{exr:unnamed-chunk-14}{}{\label{exr:unnamed-chunk-14}
\iffalse (Kurtosis and skewness) \fi{} }TBD.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[The `apply()` function]
\protect\hypertarget{exr:unnamed-chunk-15}{}{\label{exr:unnamed-chunk-15}
\iffalse (The \texttt{apply()} function) \fi{} }The \texttt{votes.repub}
data set from the standard R package \texttt{cluster}
\citep{pkg-cluster} contains the percentages of votes given to the
republican candidate in presidential elections from 1856 to 1976. The
rows represent the 50 US states, and the columns represent the 31
elections. The data can be loaded into R using
\texttt{data(votes.repub,\ package\ =\ "cluster")}. In this case, we may
be interested in computing various descriptive statistics for each state
across all years. Since the data are stored in rectangular format (i.e.,
state are in rows and years are in columns) we can efficiently compute
descriptive statistics for each state using R's built-in
\texttt{apply()} function (see \texttt{?apply} for details). Using
\texttt{apply()}, compute the sample mean percentage of votes and
standard deviation for each state, taking care to handle \texttt{NA}'s
appropriately. Which state had the highest average percentage of votes?
Which had the lowest?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[Trimmed mean]
\protect\hypertarget{exr:unnamed-chunk-16}{}{\label{exr:unnamed-chunk-16}
\iffalse (Trimmed mean) \fi{} }TBD.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[Sample variance `r emo::ji('scream')`]
\protect\hypertarget{exr:unnamed-chunk-17}{}{\label{exr:unnamed-chunk-17}
\iffalse (Sample variance
\texttt{r\ emo::ji(\textquotesingle{}scream\textquotesingle{})}) \fi{}
}Show that the formula for the sample is equivalent to \[
  s^2 = \left(\frac{1}{n} \sum_{i = 1}^n x_i^2\right) - \bar{x}^2.
\]
\EndKnitrBlock{exercise}

\hypertarget{visualization}{%
\chapter{Visual data exploration}\label{visualization}}

\begin{quote}
``Pie charts do not provide efficient detection of geometric objects
that convey information about differences of values.''

--- William S. Cleveland
\end{quote}

As mentioned at the beginning of Chapter \ref{descriptive}, the first
step in any data analysis problem is to simply ``look'' at the data. In
the previous chapter, we did this using numerical summaries of the data
(i.e., descriptive statistics). In this chapter, we discuss graphical
summaries of the data---these two tasks go hand in hand.

One of the biggest strengths of R is its ability to produce high-quality
static graphics. While base R can produce high quality graphics with
ease, a number of add-on packages provide enhanced graphics
capabilities. Two of the most popular packages are \texttt{lattice}
\citep{pkg-lattice} and \texttt{ggplot2} \citep{pkg-ggplot2}---both of
which are built on top of R's \texttt{grid} graphics system. Dynamic and
interactive graphics are also readily available through add-on packages
like \texttt{plotly} \citep{pkg-plotly}. This chapter makes extensive
use of \texttt{ggplot2}---a popular package for creating graphics based
on \emph{The Grammar of Graphics}; see
\texttt{help("ggplot2-package",\ package\ =\ "ggplot2")} for details and
useful links.

\BeginKnitrBlock{note}
Although this chapter uses \texttt{ggplot2}, the R code for recreating
most of the graphs (where possible) using both \texttt{lattice} and base
R graphics is available on the book's
\href{https://github.com/koalaverse/abar}{GitHub repo}. One advantage of
\texttt{lattice} over \texttt{ggplot2} is its ability to create
3-dimensional graphics and \emph{shingles} (see \texttt{?lattice::cloud}
and \texttt{?lattice::shingle}, respectively, for additional
information).
\EndKnitrBlock{note}

Visual data exploration (VDE) is critical to understanding your data.
When combined with descriptive statistics (Chapter \ref{descriptive}),
VDE provides an effective way to identify relationships and
abnormalities (e.g., outliers), and communicate summaries of the data.
In fact, sometimes no elaborate analysis is necessary at all as most of
the important conclusions are evident by simply examining the data
visually.

Regardless of the intent, VDE is about investigating the characteristics
of your data set. To do this, we typically create numerous plots in an
iterative fashion. This chapter will show you how to create such plots,
and use them to answer fundamental questions about the data.

\hypertarget{prerequisites-1}{%
\section{Prerequisites}\label{prerequisites-1}}

In this chapter we'll illustrate the key ideas of VDE using the Ames
housing data. We'll also use several \texttt{tidyverse} packages,
including: \texttt{dplyr}, for data manipulation; \texttt{ggplot2}, for
creating graphs; as well as some useful graphical functions from other
packages (most of which rely on \texttt{ggplot2}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load required packages}
\KeywordTok{library}\NormalTok{(dplyr)         }\CommentTok{# for data manipulation}
\KeywordTok{library}\NormalTok{(ggplot2)       }\CommentTok{# for creating graphs}
\KeywordTok{library}\NormalTok{(gridExtra)     }\CommentTok{# for displaying multiple (ggplot2) graphs in one figure}
\KeywordTok{library}\NormalTok{(scales)        }\CommentTok{# for better breaks and labels in ggplot2 axes}

\CommentTok{# Additional packages that need to be installed: AmesHousing, forcats, GGally, }
\CommentTok{# ggridges, RColorBrewer, reshape2, purrr, and viridis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()  }\CommentTok{# load the Ames housing data}
\end{Highlighting}
\end{Shaded}

\texttt{ggplot2} constructs plots in layers. For example, add points,
then add trend lines, etc. A general formula for a \texttt{ggplot2}
graph is given below. Notice, we always start by using \texttt{ggplot()}
to construct an empty canvas.

\begin{verbatim}
ggplot() +
  geom_<geom-1-name>(data = <data-1-name>, aes(<aesthetic mappings>), <options>) +
  geom_<geom-2-name>(data = <data-2-name>, aes(<aesthetic mappings>), <options>) +
  ...
\end{verbatim}

If all the layers use the same data set (as in most of this chapter),
you can provide the data and aesthetics to the \texttt{ggplot()}
function instead, as in

\begin{verbatim}
ggplot(<data-name>, aes(<aesthetic mappings>)) +
  geom_<geom-1-name>(<options>) +
  geom_<geom-2-name>(<options>) +
  ...
\end{verbatim}

\hypertarget{univariate-data}{%
\section{Univariate data}\label{univariate-data}}

An important first step in VDE is to to understand how individual
variables are distributed. Visualizing the distribution of a variable
allows us to easily understand and describe many of its features. This
is complementary to describing the distribution of a variable using
various descriptive statistics, as described in Chapter
\ref{descriptive}.

\hypertarget{continuous-variables}{%
\subsection{Continuous Variables}\label{continuous-variables}}

A variable is \emph{continuous} if it can take any of an infinite set of
ordered values (e.g., income and price). There are several different
features we are generally interested in with continuous variables
(i.e.~measures of location, measures of spread, asymmetry, outliers).
Different plots can effectively communicate these different features of
continuous variables.

\BeginKnitrBlock{note}
Techincally, no variable in a data set is truly continuous, as
measurements are never recorded with infinite precision. In a
\emph{census}, for example, age is typically recorded in years, but
typically treated as continuous during analysis. What's important for a
variable to be treated as continuous is our willingness to treat the
differences between values as \emph{quantitative}. For the purposes of
this book, continuous variables are also referred to as numeric or
quantitative variables.
\EndKnitrBlock{note}

\hypertarget{histograms}{%
\subsubsection{Histograms}\label{histograms}}

One of the most effective ways to visualize the distribution (in
particular, the density) of a continuous variable is to use a
\emph{histogram}; our first example of a histogram was seen in Figure
\ref{fig:sale-price-hist} for the variable \texttt{Sale\_Price}.
Although rather crude estimates, histograms were indispensable tools in
the days before computers were mainstream.

Histograms are often overlooked, yet they offer an efficient means for
communicating important features of continuous variables (e.g.,
skewness). Histograms are just special bar charts! First, the variable
is broken into equal sized intervals, called bins, and the number of
observations that fall into each bin is counted (these are called
frequencies). A histogram is nothing more than a simple bar chart of
these frequencies (where each bar represents a different bin).
Histograms quickly signal what the most common observations are for the
variable being assessed (the higher the bar the more frequent those
values are observed in the data); they also signal the shape (spread and
symmetry) of the data by illustrating if the observed values cluster
towards one end or the other of the distribution.

To get a quick sense of how \texttt{Sale\_Price} is distributed, we can
generate a histogram estimate of its density using \texttt{ggplot2}'s
\texttt{geom\_histogram()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-hist-sale-price-01-1}

\}

\textbackslash{}caption\{Default histogram of
\texttt{Sale\_Price}.\}\label{fig:04-hist-sale-price-01}
\textbackslash{}end\{figure\} Figure \ref{fig:04-hist-sale-price-01}
conveys several important features about the distribution of
\texttt{Sale\_Price}:

\begin{itemize}
\tightlist
\item
  \textbf{measures of location} (\ref{measures-of-location}): the most
  common values of \texttt{Sale\_Price} are around \$160K (the median);
\item
  \textbf{measures of spread} (\ref{measures-of-spread}):
  \texttt{Sale\_Price} ranges from near zero to over \$700K;
\item
  \textbf{skewness}: \texttt{Sale\_Price} is skewed right (a common
  feature of financial data);
\item
  \textbf{outliers} (\ref{outliers}): there appears to be some
  abnormally large values of \texttt{Sale\_Price}. Some of the analytic
  techniques we will learn about in later chapters are sensitive to
  outliers and assume that the data are (at least approximately)
  symmetric. Histograms, as well may of the graphical displays in this
  chapter, are invaluable tools for identifying there and other
  potential problems with our data.
\end{itemize}

\textbf{FIXME:} The footnote in this section does not make sense.

By default, \texttt{geom\_histogram()} will divide the values of a
continuous variable into 30 equally sized bins. Since
\texttt{Sale\_Price} ranges from \$12,789--\$755,000, 30 equally sized
bins implies a bin width of
\(\frac{\left\lfloor{\$755,000 - \$12,789}\right\rfloor}{30} = \$24,740\).
So in Figure \ref{fig:04-hist-sale-price-01}, the first bar represents
the frequency of \texttt{Sale\_Price} in the range of (roughly)
\(\left[\$12,500, \ \$37,500\right]\)\footnote{These bounds are
  approximate since the binning will round to whole numbers (e.g.,
  12,800 rather than 12,370).}, the second bar represents the frequency
of \texttt{Sale\_Price} in the range of (roughly)
\(\left[\$37,500, \ \$62,300\right]\), and so on.

\BeginKnitrBlock{tip}
The number of bins (or equivalently, the bin width) in a histogram is a
\emph{tuning parameter} that should be adjusted specifically for each
application. Adjusting the bin width is somewhat subjective and needs to
be done carefully (usually by eye). If the bin width is ``too small'',
the histogram will \emph{overfit} the data and display too much noise.
If the bin width is ``too large'', the histogram will \emph{underfit}
the data and important features (like outliers) will be missed.
\EndKnitrBlock{tip}

In \texttt{geom\_histogram()}, we can use the \texttt{binwidth} and
\texttt{bins} arguments to control the bin width and number of bins,
respectively (but only one argument needs to be specified). By adjusting
one of these parameters, we can change the crudeness of the histogram
estimate of the variable's density. For instance, in the default
histogram, the bin with the largest frequency ranged from
\$136,000--\$161,000; however, as demonstrated below, we can often do
better by manually adjusting the bin width\footnote{The default in
  \texttt{ggplot2} is to always use 30 bins---which will not be optimal
  for most problems.}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Histograms with various bin widths}

\CommentTok{# Too crude (i.e., under fitting)}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{100000}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Bin width = $100,000"}\NormalTok{)  }

\CommentTok{# Less crude}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{50000}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Bin width = $50,000"}\NormalTok{)}

\CommentTok{# Just right?}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{5000}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Bin width = $5,000"}\NormalTok{)}

\CommentTok{# Too flexible (i.e., over fitting)}
\NormalTok{p4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1000}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Bin width = $1,000"}\NormalTok{)  }

\CommentTok{# Display plots in a grid}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/hist2-1} 

}

\caption{Adjusting histogram bin width.}\label{fig:hist2}
\end{figure}

The most common value of \texttt{Sale\_Price} (i.e., the statistical
mode) is \$135,000 (this is roughly where the peak of each histogram
occurs). R does not have a built-in function for finding the most common
value in a set of observations, but we can easily find it using a
combination of \texttt{which.max()} and \texttt{table()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.max}\NormalTok{(}\KeywordTok{table}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price))}
\NormalTok{## 135000 }
\NormalTok{##    270}
\end{Highlighting}
\end{Shaded}

The mode of a continuous variable is another measure of location
\ref(measures-of-location). The mode of a sample is the value that
occurs most frequently---this also makes the mode a useful descriptive
statistic for categorical data as well. If the sample values are not
unique, there will exist multiple modes (i.e., the data are said to be
\emph{multimodal}). For continuous data, unlike the sample mean and
median, computing the mode is not straightforward (e.g., what is the
mode of \(x = \left\{0.001, 3.514, 6.799, \dots\right\}\)). Fortunately,
the modes (or peaks in the density) of a sample are easily seen from
density estimates like histograms. Recall that the mean and median of
\texttt{Sale\_Price} were \$180,796.10 and \$160,000, respectively. For
\emph{unimodal} distributions (i.e., distributions with only a single
peak), the median will often lie between the mean and the mode. For
perfectly symmetric distributions, the mean, median, and mode are all
the same (though, this rarely, if ever, happens in practice). This is
especially true for right skew data like \texttt{Sale\_Price}.

As discussed in Chapter \ref{descriptive}, a log transformation is often
useful for making right skewed data look more symmetric; taking the
square root is also effective for this purpose. While we can simply
apply such transformations to the data ourselves, \texttt{ggplot2}
offers several useful functions for plotting data on various scales; in
our case, we can use the \texttt{scale\_x\_log()} and
\texttt{scale\_x\_sqrt()} functions to apply a \(log_{10}\) and square
root transformation, respectively\footnote{Two things to note here: 1)
  if you want to change the bin width, you need to feed a log or square
  root transformed number to bin width or, as we did, increase the
  number of bins using the \texttt{bins} argument; and 2) if you have
  zeros in your data, use
  \texttt{scale\_x\_continuous(trans\ =\ "log1p")}, which adds 1 prior
  to the log transformation (you cannot take the logarithm of zero!).}.
This is demonstrated in the code below which produces Figure
\ref{fig:04-hist-transformed}. In this case, the \(log_{10}\)
transformation was more effective at making the data appear more
symmetric.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Log 10 scale}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar, }\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{125000}\NormalTok{, }\DecValTok{300000}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{45}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Sale_Price (log10 scale)"}\NormalTok{)}

\CommentTok{# Square root scale}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar, }\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{125000}\NormalTok{, }\DecValTok{300000}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{45}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Sale_Price (square root scale)"}\NormalTok{)}

\CommentTok{# Display both plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-hist-sales-transformed-1}

\}

\textbackslash{}caption\{Histogram of \texttt{Sale\_Price} using
different transformation scales. \emph{Left}: \(log_{10}\) scale.
\emph{Right}: Square root scale.\}\label{fig:04-hist-sales-transformed}
\textbackslash{}end\{figure\}

Oftentimes, it will be useful to ``smooth the histogram'' by overlaying
a \emph{kernel density estimate}: an alternative to histograms for data
assumed to come from a smooth, continuous distribution. To plot a kernel
density estimate in \texttt{ggplot2}, we use the
\texttt{geom\_density()} function. For example, the following code
constructs a simple kernel density estimate for \texttt{Sale\_Price}.
Additionally, we also add a \emph{rug} representation (i.e.,
one-dimensional marginal distribution) of \texttt{Sale\_Price} to the
\(x\)-axis using the \texttt{geom\_rug()} function. Rug displays give
additional insight into the spread of the distribution of the continuous
variable of interest; by default, rug displays plot one tick mark for
each data point, but with lots of data, it may be more useful to plot
certain percentiles (e.g., the deciles of the distribution). Rug
displays are uses in various plots throughout this book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Kernel density estimate}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{()}

\CommentTok{# Add a rug representation to the x-axis}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_rug}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{)}

\CommentTok{# Display both graphs side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/density-1} 

}

\caption{Density plot of sales price.}\label{fig:density}
\end{figure}

\BeginKnitrBlock{tip}
Rug representations can be added to any axis in a plot (as long as that
axis represents a continuous variable). In \texttt{ggplot2}, we can
specify which sides of the plot we want to display a 1-D marginal
distribution using the \texttt{sides} argument. For example, specifying
\texttt{sides\ =\ "bl"} (the default) will display a rug representation
of the data on the bottom and left sides of the plot.
\EndKnitrBlock{tip}

In practice, we suggest using a kernel density estimate in conjunction
with a histogram---as they both compliment each other. To overlay a
kernel density estimate on top of a histogram, we need to plot the
histogram so that the \(y\)-axis is on the density scale, rather than
the frequency scale (which is typically the default for a histogram). In
\texttt{ggplot2}, this can be done by specifying
\texttt{aes(y\ =\ ..density..)} in the call to
\texttt{geom\_histogram()}). We can then overlay a kernel density
estimate by adding the \texttt{geom\_density()} layer. This is displayed
in Figure \ref{fig:04-histogram-density}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"grey65"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-histogram-density-1}

\}

\textbackslash{}caption\{Histogram of \texttt{Sale\_Price} with a kernel
density estimate (red curve).\}\label{fig:04-histogram-density}
\textbackslash{}end\{figure\}

\hypertarget{quantile-quantile-plots}{%
\subsubsection{Quantile-quantile plots}\label{quantile-quantile-plots}}

Quantile-quantile (Q-Q) plots offer a graphical way for assessing how
similar data from two (possibly different) continuous distributions are.
The most common use of Q-Q plots is in assessing normality; that is,
whether or not it is reasonable to assume a variable is (at least
approximately) normally distributed; Q-Q plots in this setting are
referred to as normal Q-Q plots. Normal Q-Q plots graph the observed
quantiles of the observed data against the similar quantiles from a
normal distribution---such plots are often used in assessing the
normality assumption in ordinary linear regression (Chapter
\ref(regression)).

\BeginKnitrBlock{note}
Although commonly used to check normality, Q-Q plots can be constructed
to graphically check if it is reasonable to assume a continuous variable
comes from any parametric distribution!
\EndKnitrBlock{note}

A normal Q-Q plot for \texttt{Sale\_Price} is given in the left side of
Figure \ref{fig:04-normal-qq}. This graph plots the sample quantiles of
\texttt{Sale\_Price} against the theoretical quantiles of a normal
distribution (the default for \texttt{stat\_qq()}). If
\texttt{Sale-Price} is normally distributed, the points on the plot
should lie (roughly) on a 45\(^\circ\) line (imagine trying to cover the
points with a ``fat pencil''). If the observed data deviate from
normality, then the plot will display curvature. The nature of this
curvature would be informative into how \texttt{Sale\_Price} deviates
from normality. To produce a Q-Q plot in \texttt{ggplot2}, we can use
the \texttt{stat\_qq()} function. The following code chunk produces
Figure \ref{fig:04-normal-qq}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Normal Q-Q plot of Sale_Price}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_qq}\NormalTok{()}

\CommentTok{# Normal Q-Q plot of log(Sale_Price)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =} \KeywordTok{log}\NormalTok{(Sale_Price))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_qq}\NormalTok{()}

\CommentTok{# Display both graphs side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-normal-qq-1}

\}

\textbackslash{}caption\{Normal Q-Q plots for \texttt{Sale\_Price}.
\emph{Left}: Original scale. \emph{Right}: Log-transformed
scale.\}\label{fig:04-normal-qq} \textbackslash{}end\{figure\}

Notice how the normal Q-Q plot for \texttt{Sale\_Price} starts to bend
up. This indicates that the distribution of \texttt{Sale\_Price} has a
heavier right tail than expected from a normal distribution (i.e.,
\texttt{Sale\_Price} is more positively skewed than what would be
expected from normally distributed data). The normal Q-Q plot for
\texttt{log(Sale\_Price)} is a bit more well behaved (aside from the two
potential outliers and slight downward curvature in the bottom left of
the graph).

Histograms (and kernel density estimates), and Q-Q plots are great at
visually summarizing the distribution of a continuous variable;
especially a variable with many unique values. Unfortunately, histograms
are less suitable for continuous data with few unique values, and are
not great at displaying outliers (the same goes for Q-Q plots).
Fortunately, a simple display of a few descriptive statistics offers a
perfect compliment to histograms; this display is referred to as a
\emph{box plot}.

\hypertarget{boxplots}{%
\subsubsection{Box plots}\label{boxplots}}

Box plots \citep{frigge-some-1989}---in particular, Tukey's
\emph{box-and-whisker plots} \citep{tukey-exploratory-1977}---are an
effective way to compare a continuous variable between groups. For a
single variable, a box of arbitrary width is drawn around the median
from the lower quartile \(Q_1\) to the upper quartile \(Q_3\); this box,
which has length \(Q_3 - Q_1 = IQR\), contains the middle 50\% of the
data. The ``whiskers'' are formed by extending lines from the edges of
the box out to \(Q_1 - 1.5 \times IQR\) and \(Q_3 + 1.5 \times IQR\).
Any observations lying beyond the whiskers are flagged as potential
outliers and plotted as individual points. An example of a typical box
plot for some skew right data is given in Figure
\ref{fig:04-box-plot-diagram}. Can you easily tell from the box plot
that these data are skew right? 🤔

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/04-box-plot-diagram-1} 

}

\caption{A typical box plot.}\label{fig:04-box-plot-diagram}
\end{figure}

A box plot can be constructed in \texttt{ggplot2} using the
\texttt{geom\_boxplot()} layer. In the code chunk below, we use
\texttt{geom\_boxplot()} to construct box plots for both
\texttt{Sale\_Price} and \texttt{log10(Sale\_Price)} (by specifying a
\(\log\) scale for the \(y\)-axis). The results are displayed in Figure
\ref{fig:04-box-plot-sale-price}. \textbf{Note:} we used \(log_{10}\) as
the scale because \texttt{ggplot2} does not have a natural log scale
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar, }\DataTypeTok{breaks =} \KeywordTok{quantile}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =} \StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Box plot for Sale_Price"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{p1 }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar, }\DataTypeTok{breaks =} \KeywordTok{quantile}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =} \StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Box plot for log10(Sale_Price)"}\NormalTok{)}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{abar_files/figure-latex/04-box-plot-sale-price-1}

\}

\textbackslash{}caption\{Box plots for \texttt{Sale\_Price}.
\emph{Left:} Original scale. \emph{Right:} \(log_{10}\)
scale.\}\label{fig:04-box-plot-sale-price} \textbackslash{}end\{figure\}

A modern alternative to Tukey's box plot, called a \emph{violin} 🎻
\emph{plot}, combines box plots with kernel density estimates; in
\texttt{ggplot2} nomenclature, \texttt{geom\_violin()} \(\approx\)
\texttt{geom\_boxplot()} + \texttt{geom\_density()}.

There are two efficient graphs to get an indication of potential
outliers in our data. The classic box plot on the left will identify
points beyond the whiskers which are beyond \(\$1.5 \times IQR\) from
the first and third quantile. This illustrates there are several
additional observations that we may need to assess as outliers that were
not evident in our histogram. However, when looking at a box plot we
lose insight into the shape of the distribution. A violin plot on the
right provides us a similar chart as the box plot but we lose insight
into the quantiles of our data and outliers are not plotted (hence the
reason we plot \texttt{geom\_point()} prior to \texttt{geom\_violin()}).
Violin plots will come in handy later when we start to visualize
multiple continuous distributions along side each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Box plot (log10 scale)}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\StringTok{"var"}\NormalTok{, Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{outlier.alpha =} \FloatTok{0.25}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}
    \DataTypeTok{labels =}\NormalTok{ dollar, }
    \DataTypeTok{breaks =} \KeywordTok{quantile}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\NormalTok{  )}

\CommentTok{# Violin plot (log10 scale)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\StringTok{"var"}\NormalTok{, Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}
    \DataTypeTok{labels =}\NormalTok{ dollar, }
    \DataTypeTok{breaks =} \KeywordTok{quantile}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price)}
\NormalTok{  )}

\CommentTok{# DIsplay both plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-boxplot-violin-1} 

}

\caption{Box plot (left) and violin plot (right).}\label{fig:04-boxplot-violin}
\end{figure}

The box plot starts to answer the question of what potential outliers
exist in our data. Outliers in data can distort predictions and affect
their accuracy. Consequently, it's important to understand if outliers
are present and, if so, which observations are considered outliers.

As demonstrated, several plots exist for examining univariate continuous
variables. Several examples were provided here but still more exist
(i.e.~frequency polygon, bean plot, shifted histograms). There is some
general advice to follow such as histograms being poor for small data
sets, dot plots being poor for large data sets, histograms being poor
for identifying outlier cut-offs, box plots being good for outliers but
obscuring multimodality. Consequently, it is important to draw a variety
of plots. Moreover, it is important to adjust parameters within plots
(i.e.~bin width, axis transformation for skewed data) to get a
comprehensive picture of the variable of concern.

In this section, we have introduced several fundamental, but useful,
graphics for continuous variables. This is by no mean an exhaustive list
and we will see many different types of graphics throughout the book. In
practice, it is often helpful (encouraged, in fact) to look at many
types of graphics for a particular variable. For continuous variables,
it is often useful to look at histograms, box plots, Q-Q plots, and a
number of other graphics (depending on the situation). A useful
graphical summary for a continuous variable is called the \emph{four
plot} (\textbf{REFERENCE}). A four plot displays a wealth of important
information about continuous variables, especially as it relates to
typical analytic tasks (e.g., density, correlation, outliers, etc.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{color =} \StringTok{"purple2"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_qq}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{ames}\OperatorTok{$}\NormalTok{Index <-}\StringTok{ }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ames))  }\CommentTok{# add row number as a new column}
\NormalTok{p4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Index, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/04-four-plot-1} 

}

\caption{ABC.}\label{fig:04-four-plot}
\end{figure}

\hypertarget{categorical-variables}{%
\subsection{Categorical Variables}\label{categorical-variables}}

Categorical variables, in contrast to numeric, can only take on a finite
number of distinct values. For example, \(\left\{Male, Female\right\}\)
or \(\left\{low, medium, high\right\}\). In statistics, categorical
variables are often referred to as \emph{factors}. With categorical
variables, as discussed in Chapter \ref{descriptive}, we are often
interested in tabulating frequencies and estimating proportions.
Graphically, these are easiest to display via bar charts and dot plots.
In general, dot plots are referred to bar charts as they can display
mostly the same information, but in a more concise plot.

\hypertarget{bar-charts}{%
\subsubsection{Bar charts}\label{bar-charts}}

In a typical bar chart, each bar represents a different category (e.g.,
\(Male\) or \(Female\)) and the height of each bar represents the
frequency (or proportion) of observations within each category. By
default, the \(x\)-axis typically represents categories; however, as we
will see, it is often useful to ``flip'' bar charts horizontally for
readability, especially when the number of categories is large. We have
already seen bar charts in the form of a histogram---the difference
there being that the bars (i.e., categories) were created by binning a
numeric variable into (roughly) equal sized buckets.

If we look at the general zoning classification (\texttt{MS\_Zoning})
for each property sold in our \texttt{ames} data set, we see that the
majority of all properties fall within one category. We can use
\texttt{ggplot2}'s \texttt{geom\_bar()} function to construct simple bar
charts. By default, \texttt{geom\_bar()} simply counts the observations
within each category and displays them in a vertical bar chart. A bar
chart for \texttt{MS\_Zoning} is displayed in Figure
\ref{fig:04-bar-chart-01}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(MS_Zoning)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{55}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-bar-chart-01-1} 

}

\caption{Basic bar chart.}\label{fig:04-bar-chart-01}
\end{figure}

\BeginKnitrBlock{tip}
The visual order of categories is important for bar charts and dot
plots. By default, \texttt{ggplot2} (and most other graphical packages)
plot the categories in alphabetical order---which is not visually
appealing. In \texttt{ggplot2} we can use the \texttt{reorder()}
function to easily plot the categories of a variable in a particular
order, as demonstrated in Figure \ref(04-bar-chart-02).
\EndKnitrBlock{tip}

\texttt{MS\_Zoning} is an example of a \emph{nominal} categorical
variable; a categorical variable for which there is no logical ordering
of the categories (as opposed to \texttt{low}, \texttt{medium}, and
\texttt{high}, for example). To get better clarity of nominal variables
we can make some refinements. We can also draw bar charts manually using
\texttt{geom\_col()}. To do this, we first need to compute the length of
each bar. For frequencies, we can use \texttt{dplyr::count()} to tally
the number of observations within each category. We can then use
\texttt{dplyr::mutate()} to convert frequencies to proportions. With
this approach, we can use \texttt{reorder()} to easily reorder the bar
categories from most frequent to least (or vice versa). For readability,
we can also apply \texttt{coord\_flip()} to rotate the bar chart (or any
\texttt{ggplot2} figure) on its side. These refinements are demonstrated
in the code chunk below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Bar chart of frequencies}
\NormalTok{p1 <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(MS_Zoning) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{reorder}\NormalTok{(MS_Zoning, n), n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{  }\CommentTok{# now x becomes y}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"MS_Zoning"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Frequency"}\NormalTok{)  }\CommentTok{# better labels}

\CommentTok{# Bar chart of proportions}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(MS_Zoning) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# convert to proportions}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{reorder}\NormalTok{(MS_Zoning, pct), pct)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{  }\CommentTok{# now x becomes y}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"MS_Zoning"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Relative frequency"}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# better labels}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent)}

\CommentTok{# Dispay both plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-bar-chart-02-1}

\}

\textbackslash{}caption\{Bar chart and dot plots of \texttt{MS\_Zoning}.
\emph{Left}: Bar chart of frequencies. \emph{Right}: Bar chart of
relative frequencies (\%).\}\label{fig:04-bar-chart-02}
\textbackslash{}end\{figure\}

\BeginKnitrBlock{note}
Note how \texttt{dplyr} functions can be expressed sequentially using
the forward pipe operator \texttt{\%\textgreater{}\%}, whereas
\texttt{ggplot2} layers have to be added using \texttt{+}!
\EndKnitrBlock{note}

Now we can see that properties zoned as residential low density make up
nearly 80\% of all observations . We also see that properties zoned as
agricultural (\texttt{A\_agr}), industrial (\texttt{I\_all}), commercial
(\texttt{C\_all}), and residential high density make up a very small
amount of observations. In fact, below we see that these imbalanced
category levels each make up less than 1\% of all observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(MS_Zoning) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(pct)}
\NormalTok{## # A tibble: 7 x 3}
\NormalTok{##   MS_Zoning                        n      pct}
\NormalTok{##   <fct>                        <int>    <dbl>}
\NormalTok{## 1 A_agr                            2 0.000683}
\NormalTok{## 2 I_all                            2 0.000683}
\NormalTok{## 3 C_all                           25 0.00853 }
\NormalTok{## 4 Residential_High_Density        27 0.00922 }
\NormalTok{## 5 Floating_Village_Residential   139 0.0474  }
\NormalTok{## 6 Residential_Medium_Density     462 0.158   }
\NormalTok{## 7 Residential_Low_Density       2273 0.776}
\end{Highlighting}
\end{Shaded}

Severely imbalanced categories can cause problems in statistical
modelling, so it makes sense to sometimes combine the infrequent levels
into an \texttt{other} category. An easy way to accomplish this is to
use \texttt{fct\_lump()}\footnote{To learn more about managing factors,
  see \citet[ch.~15]{wickham-R-2017}}. Here we use \texttt{n\ =\ 2} to
retain the top two most frequent categories/levels, and condense the
remaining into an \texttt{other} category. You can see that
\texttt{other} still represents less than 10\% of all observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{MS_Zoning =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_lump}\NormalTok{(MS_Zoning, }\DataTypeTok{n =} \DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(MS_Zoning) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{reorder}\NormalTok{(MS_Zoning, pct), pct)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-bar-chart-03-1} 

}

\caption{Bar chart with collapsed categorical levels.}\label{fig:04-bar-chart-03}
\end{figure}

In some cases, the categories of a categorical variable have a natural
ordering (though, the difference between any two categories is not
meaningful)---these are called \emph{ordinal} variables. For example,
the \texttt{Kitchen\_Qual} variable in \texttt{ames} categorizes kitchen
quality into five buckets:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Kitchen_Qual)}
\NormalTok{## }
\NormalTok{## Excellent      Fair      Good      Poor   Typical }
\NormalTok{##       205        70      1160         1      1494}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Kitchen_Qual)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-kitchen-quality-02-1} 

}

\caption{Bar chart inadequately capturing ordinal levels.}\label{fig:04-kitchen-quality-02}
\end{figure}

Here, we might consider ordering the bars using their natural order:
\texttt{Poor} \textless{} \texttt{Fair} \textless{} \texttt{Typical}
\textless{} \texttt{Good} \textless{} \texttt{Excellent}. One way to
plot the categories in a particular order is to use
\texttt{forcats::fct\_relevel()}. From Figure
\ref{fig:04-kitchen-quality-03}, it is easier to see that most homes
have average to slightly above average quality kitchens.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Kitchen_Qual =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_relevel}\NormalTok{(}
\NormalTok{    Kitchen_Qual, }\StringTok{"Poor"}\NormalTok{, }\StringTok{"Fair"}\NormalTok{, }\StringTok{"Typical"}\NormalTok{, }\StringTok{"Good"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Kitchen_Qual)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-kitchen-quality-03-1} 

}

\caption{Bar chart adequately capturing ordinal levels.}\label{fig:04-kitchen-quality-03}
\end{figure}

Often our data will have categorical variables that are numerically
encoded (typically as integers). For example, in the \texttt{ames} data
set, the month each home was sold (\texttt{Mo\_Sold}) was encoded using
the integers 1--12. For visual (and in some cases, modelling), we will
want to make sure R treats such variables as categorical. This is
easiest to accomplish using \texttt{as.factor()}. The following example
demonstrates the difference; the results are displayed in Figure
\ref{fig:04-month-sold}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Keeping Mo_Sold as an integer}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(Mo_Sold)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Month sold (numeric)"}\NormalTok{)}

\CommentTok{# Converting Mo_Sold to a factor}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Mo_Sold))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Month sold (factor)"}\NormalTok{)}

\CommentTok{# Display both plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-month-sold-1}

\}

\textbackslash{}caption\{Bar chart of \texttt{Mo\_Sold}. \emph{Top}:
Numeric. \emph{Bottom}: Factor.\}\label{fig:04-month-sold}
\textbackslash{}end\{figure\}

\hypertarget{dot-plots}{%
\subsubsection{Dot plots}\label{dot-plots}}

Basic bar charts are great when the number of categories is small. As
the number of categories increases, the bars can become squished
together and distract attention from the main insights of the visual.
Cleveland dot plots (or just dot plots) and \emph{lollipop} charts, like
bar charts, are useful for visualizing discrete distributions (e.g.,
tables or the frequencies of different categories) while being more
economical in ink.

For example, if we can use \texttt{geom\_point()} to construct a dot
plot of the relative frequencies of home sales across the 28 within the
Ames housing data set. The result is displayed on the left side of
Figure \ref{fig:04-dot-plot-03}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\KeywordTok{count}\NormalTok{(Neighborhood) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(pct, }\KeywordTok{reorder}\NormalTok{(Neighborhood, pct))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Relative frequency"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Neighborhood"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Similar to a dot plot, a lollipop chart minimizes the visual ink but
uses a line to draw the readers attention to the specific \(x\)-axis
value of each category. To create a lollipop chart, we use
\texttt{geom\_segment()} to add lines to the previous plot; we
explicitly state that we want the lines to start at \texttt{x\ =\ 0} and
extend to the corresponding relative frequency with
\texttt{xend\ =\ pct}. We also need to include
\texttt{y\ =\ Neighborhood} and \texttt{yend\ =\ Neighborhood} so that
we get one line segment for each neighborhood. The result is displayed
on the right side of Figure \ref{fig:04-dot-plot-03}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 <-}\StringTok{ }\NormalTok{p }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\NormalTok{, }\DataTypeTok{xend =}\NormalTok{ pct, }\DataTypeTok{y =}\NormalTok{ Neighborhood, }\DataTypeTok{yend =}\NormalTok{ Neighborhood),}
               \DataTypeTok{size =} \FloatTok{0.15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/04-dot-plot-03-1} 

}

\caption{Relative frequency of home sales accross the different neighborhoods. *Left*: Dot plot. *Right*: Lollipop chart.}\label{fig:04-dot-plot-03}
\end{figure}

\hypertarget{pie-charts}{%
\subsubsection{Pie charts}\label{pie-charts}}

Don't use them.

\hypertarget{bivariate-data}{%
\section{Bivariate data}\label{bivariate-data}}

Beyond understanding the distribution of each individual variable, we
often want to investigate associations and relationships between
variables. When visualizing the relationship between two or more
variables we are generally interested in identifying potential
associations, outliers, clusters, gaps, barriers, and change points.

\hypertarget{scatter-plots}{%
\subsection{Scatter plots}\label{scatter-plots}}

The easiest way to assess the relationship between two continuous
variables is to use a scatter plot, one of the most important
statistical graphics. A scatter plot graphs two continuous variables
directly against each other (one variable on each axis).

In this section, we'll use the \texttt{geom\_point()}
function---\texttt{ggplot2}'s function for producing scatter plots. In
the code chunk below, we use \texttt{geom\_point()} to obtain a plot of
sale price (\texttt{Sale\_Price}) versus the square footage of above
ground living area (\texttt{Gr\_Liv\_Area}). To limit the effect of
\emph{over plotting}, a common issue with scatter plot of large data
sets, we lower the opacity of the individual points using
\texttt{alpha\ =\ 0.3}\footnote{This option can be specified for any of
  the \texttt{geom\_\textless{}geom-name\textgreater{}()} functions
  discussed in this chapter.}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{)  }\CommentTok{# lower opacity}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-scatter-01-1}

\}

\textbackslash{}caption\{Scatter plot of Sale\_Price and
Gr\_Liv\_Area.\}\label{fig:04-scatter-01} \textbackslash{}end\{figure\}

It is fairly evident from Figure \ref{fig:04-scatter-01} that there is a
generally positive association between \texttt{Gr\_Liv\_Area} and
\texttt{Sale\_Price} (this \textbf{does not} imply any \emph{causal
association} between them). Five potential outliers with
\texttt{Gr\_liv\_Area\ \textgreater{}\ 4000} are also apparent. We also
gain a general understanding of the \emph{joint density} of these two
variables (e.g., dense regions/clusters of data points indicate
reasonable combinations of \texttt{Gr\_liv\_Area} and
\texttt{Sale\_Price}.) For example, it is not very likely to find a home
that will sell for more than \$400K that less than 1.5K (sq. ft.) of
above ground living area.

The relationship between \texttt{Gr\_Liv\_Area} and \texttt{Sale\_Price}
appears to be fairly linear, but the variability in \texttt{Sale\_Price}
increases with \texttt{Gr\_Liv\_Area} (probably due to the positive
skewness associated with both variables); this non-constant variance is
called \emph{heteroscedasticiy} and we will revisit this topic briefly
in section \textbf{{[}REFERNCE LINEAR REGRESSION SECTION{]}}. To help
guide our eyes in interpreting trends between two variables, we can add
parametric and/or non-parametric trend lines. In Figure
\ref{fig:04-scatter-02}, we use \texttt{geom\_smooth()} to add two
different trend lines:

\begin{itemize}
\item
  \texttt{method\ =\ "lm"} draw a trend line of the form
  \(\beta_0 + \beta_1\)\texttt{Gr\_Liv\_Area}, where the \(y\)-intercept
  (\(\beta_0\)) and the slope (\(\beta_1\)) are estimated from the
  observed data;
\item
  \texttt{method\ =\ "auto"} use the number of observations to choose an
  appropriate non-parametric smoother (i.e., let the trend be dictated
  by the observed data).
\end{itemize}

Note that for both trend lines we used \texttt{se\ =\ FALSE} to suppress
plotting +/-2 standard error bands. Figure \ref{fig:04-scatter-02} shows
that for homes with less than 2,250 square feet the relationship is
fairly linear; however, beyond 2,250 square feet we see strong
deviations from linearity. For reference, we also included the same
plot, but with both axes on the \(log_{10}\) scale. Notice how this
transformation helps to alleviate, to some extent, the heteroskedacticy
noted earlier.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Better colors for the trend lines}
\NormalTok{set1 <-}\StringTok{ }\NormalTok{RColorBrewer}\OperatorTok{::}\KeywordTok{brewer.pal}\NormalTok{(}\DataTypeTok{n =} \DecValTok{9}\NormalTok{, }\DataTypeTok{name =} \StringTok{"Set1"}\NormalTok{)}

\CommentTok{# Scatter plot with trend lines}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ set1[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"auto"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ set1[}\DecValTok{2}\NormalTok{])}

\CommentTok{# Scatter plot with trend lines (log10 scale)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ set1[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"auto"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ set1[}\DecValTok{2}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{()}

\CommentTok{# Display plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-scatter-02-1} 

}

\caption{Illustrating linear versus nonlinear relationships in scatter plots.}\label{fig:04-scatter-02}
\end{figure}

Although we can overcome the issue of over plotting to some extent by
lowering the opacity of each individual point, this is often not enough.
We can go a step beyond by adding a 2-D kernel density estimate (KDE)
using \texttt{stat\_density\_2d()}, or grouping points into hexagonal
bins and displaying a heat map of the bin counts using
\texttt{geom\_hex()}. Each of these are illustrated in Figure
\ref{fig:04-scatter-03} which graphs \texttt{Sale\_Price} versus
\texttt{Garage\_Area}. By incorporating a 2-D KDE (middle) we draw
attention to the higher density areas which appear to be located at
homes with \texttt{Garage\_Area\ =\ 0}, and homes where
\texttt{250\ \textless{}\ Garage\_Area\ \textless{}\ 500}. Similar
observations can be drawn from the hexagonal heat map (right).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scatter plot}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Garage_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{)}

\CommentTok{# Scatter plot with 2-D KDE}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Garage_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_density2d}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =} \KeywordTok{stat}\NormalTok{(level)), }\DataTypeTok{geom =} \StringTok{"polygon"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\NormalTok{viridis}\OperatorTok{::}\KeywordTok{scale_fill_viridis}\NormalTok{(}\DataTypeTok{option =} \StringTok{"A"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\CommentTok{# heat map of hexagonal bin counts}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Garage_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{50}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\NormalTok{viridis}\OperatorTok{::}\KeywordTok{scale_fill_viridis}\NormalTok{(}\DataTypeTok{option =} \StringTok{"D"}\NormalTok{)  }\CommentTok{# "D" is the default}

\CommentTok{# Display plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-scatter-03-1} 

}

\caption{Avoiding over plotting in scatter plots. *Left*: scatter plot with transparent points. *Middle*: transparent points with 2-D KDE. *Right*: heat map of hexagonal bin counts.}\label{fig:04-scatter-03}
\end{figure}

A scatter plot of a continuous variable against a categorical variable
is referred to as a \emph{strip plot}. Strip plots can be useful in
practice, but it is generally advisable to use box plots (and there
extensions) instead. Below we plot \texttt{Sale\_Price} against the
number of above ground bedrooms (\texttt{Bedroom\_AbvGr}). Due to the
size of this data set, the strip plot (top left) suffers from over
plotting. We can use \texttt{geom\_jitter()} to add a some random
variation to points within each category (top right), which allows us to
see where heavier concentrations of points exist. Alternatively, we can
use box plots and violin plots to compare the distributions of
\texttt{Sale\_Price} to \texttt{Bedroom\_AbvGr} (bottom row).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Convert to an ordered factor}
\NormalTok{ames}\OperatorTok{$}\NormalTok{Bedroom_AbvGr <-}\StringTok{ }\KeywordTok{as.ordered}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Bedroom_AbvGr)}

\CommentTok{# Strip plot}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Bedroom_AbvGr, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{)}

\CommentTok{# Strip plot (with jittering)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Bedroom_AbvGr, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{width =} \FloatTok{0.2}\NormalTok{)}

\CommentTok{# Box plots}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Bedroom_AbvGr, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}

\CommentTok{# Violin plots}
\NormalTok{p4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Bedroom_AbvGr, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_violin}\NormalTok{()}

\CommentTok{# Display plots side by side}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/04-comparing-groups-1} 

}

\caption{Comparing a continuous variable accross groups. *Top left*: scatter plot. *Top right*: strip plots. *Bottom left*: box plots. *Bottom right*: violin plots.}\label{fig:04-comparing-groups}
\end{figure}

\BeginKnitrBlock{tip}
When constructing box plots of a continuous variable within different
groups of a factor, it is sometimes useful to use \emph{notched box
plots} (at least from a comparative statistical inference perspective).
Whenever \texttt{notch\ =\ TRUE}, a ``notch'' is drawn on both sides of
each box. If the notches of any two box plots do not overlap, this is
evidence that the medians differ; see \texttt{?boxplot} for details and
a reference. An example is displayed in Figure
\ref{fig:04-notched-box-plot} comparing \texttt{Sale\_Price} across
homes of varying quality (\texttt{Overall\_Qual}).
\EndKnitrBlock{tip}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall_Qual, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{notch =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/04-notched-box-plot-1}

\}

\textbackslash{}caption\{Notched box plots comparing
\texttt{Sale\_Price} across homes of varying quality
(\texttt{Overall\_Qual}).\}\label{fig:04-notched-box-plot}
\textbackslash{}end\{figure\}

Box plots and violin plots are an effective way to visualize
distributional differences in a continuous variable accross groups. A
popular alternative is to use what are called \emph{ridge plots}
(formerly known as \emph{joy plots}). These plots can be constructed
easily using the \texttt{ggridges::geom\_density\_ridges()} function, as
shown below; the results are displayed in Figure
\ref{fig:04-ridge-plot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Sale_Price, }\DataTypeTok{y =}\NormalTok{ Overall_Qual)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\NormalTok{ggridges}\OperatorTok{::}\KeywordTok{geom_density_ridges}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-ridge-plot-1}

\}

\textbackslash{}caption\{Ridge plot comparing \texttt{Sale\_Price}
across homes of varying quality
(\texttt{Overall\_Qual}).\}\label{fig:04-ridge-plot}
\textbackslash{}end\{figure\}

\hypertarget{multivariate-data}{%
\section{Multivariate data}\label{multivariate-data}}

\hypertarget{fixme-this-section-still-needs-cleaned-up-a-bit.}{%
\subsubsection{FIXME: This section still needs cleaned up a
bit.}\label{fixme-this-section-still-needs-cleaned-up-a-bit.}}

More often than not, we are interested in visualizing the relationship
between two or more variables. In the Ames housing data, for example, we
might be interested in visualizing the relationship between
\texttt{Sale\_Price} and \texttt{Overall\_Qual}. The nature of each
variable (i.e., continuous or categorical) will often dictate the
appropriate type of graphic to use. We start with the simplest case,
where we want to visualize the relationship (if any) between two
continuous variables.

\hypertarget{facetting}{%
\subsection{Facetting}\label{facetting}}

Data are usually multivariate by nature, and the many analytic
techniques are designed to capture multivariate relationships. Visual
exploration should therefore also incorporate this important aspect.
Although we have shown how to visualize one or two variables at a time,
we can go a step beyond by adding information about additional variables
using color, shape, size, etc.

In Figure \ref{fig:04-scatter-01}, we compared \texttt{Sale\_Price} with
\texttt{Gr\_Liv\_Area} using a simple scatter plot. We can also include
information on other variables by using color, shape, and point size.
For instance, we can use a different point shape and color to indicate
homes with and without central air conditioning (\texttt{Central\_Air}).
Figure \ref{fig:04-facet-01} illustrates that there are far more homes
with central air; furthermore, the homes without central air tend to
have less square footage and lower sale prices.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price, }
                 \DataTypeTok{color =}\NormalTok{ Central_Air, }\DataTypeTok{shape =}\NormalTok{ Central_Air)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# lower the opacity of each point}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-facet-01-1}

\}

\textbackslash{}caption\{Scatter plot of \texttt{Sale\_Price} and
\texttt{Gr\_Liv\_Area} colored and shaped by
\texttt{Central\_Air}.\}\label{fig:04-facet-01}
\textbackslash{}end\{figure\}

Although we lowered the opacity of each point (using
\texttt{alpha\ =\ 0.5}), the over crowded plot makes it difficult to
distinguish the relationships between homes with and without air
conditioning. Another approach is to use \emph{facetting} (i.e., plot
each category in its own panel). In \texttt{ggplot2}, we have two
options: \texttt{facet\_wrap()} for a 1-D ribbon of 2-D panels, and
\texttt{facet\_grid()} to form a particular matrix of panels. For this
problem, we'll use the former:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price, }
                 \DataTypeTok{color =}\NormalTok{ Central_Air, }\DataTypeTok{shape =}\NormalTok{ Central_Air)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# lower the opacity of each point}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{Central_Air)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics{abar_files/figure-latex/04-facet-02-1}

\}

\textbackslash{}caption\{Scatter plot of \texttt{Sale\_Price} and
\texttt{Gr\_Liv\_Area} colored and shaped by
\texttt{Central\_Air}.\}\label{fig:04-facet-02}
\textbackslash{}end\{figure\}

However, as before, when there are many levels in a categorical variable
it becomes hard to compare differences by only incorporating color or
shape features. An alternative is to create small multiples. Below we
compare the relationship between \texttt{Sale\_Price} and
\texttt{Gr\_Liv\_Area} and how this relationship differs across the
different house styles (\texttt{House\_Style}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{House_Style, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-facet-03-1} 

}

\caption{Using small multiples to assess three dimensions in a scatter plot.}\label{fig:04-facet-03}
\end{figure}

We can start to add several of the features discussed in this chapter to
highlight multivariate features. For example, here we assess the
relationship between sales price and above ground square footage for
homes with and without central air conditioning and across the different
housing styles. For each house style and central air category we can see
where the values are clustered and how the linear relationship changes.
For all home styles, houses with central air have a higher selling price
with a steeper slope than those without central air. Also, those plots
without density markings and linear lines for the no central air
category (red) tell us that there are no more than one observation in
these groups; so this identifies gaps across multivariate categories of
interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ames, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Gr_Liv_Area, }\DataTypeTok{y =}\NormalTok{ Sale_Price, }\DataTypeTok{color =}\NormalTok{ Central_Air, }\DataTypeTok{shape =}\NormalTok{ Central_Air)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density2d}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ dollar) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{House_Style, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Sale Price vs. Above Ground Sq.Ft"}\NormalTok{,}
          \DataTypeTok{subtitle =} \StringTok{"How does central air and house style influence this relationship?"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-facet-04-1} 

}

\caption{Combining multiple attributes.}\label{fig:04-facet-04}
\end{figure}

We can also use faceting (i.e., \texttt{facet\_wrap()} and
\texttt{facet\_grid()}) to understand how two or more categorical
variables are associated with each other. For example, below we assess
the quality of kitchens (\texttt{Kitchen\_Qual}) for homes that sold
above average (i.e.,
\texttt{Sale\_Price\ \textgreater{}\ mean(Sale\_Price)}) below average
(i.e., \texttt{Sale\_Price\ \textgreater{}\ mean(Sale\_Price)}). Not
surprisingly, we see that that sold for above average tended to have
higher quality kitchens (Figure \ref{fig:04-facet-05}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{Group =} \KeywordTok{ifelse}\NormalTok{(}
\NormalTok{      Sale_Price }\OperatorTok{>}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price), }\DataTypeTok{yes =} \StringTok{"Sold above avg."}\NormalTok{, }
      \DataTypeTok{no =} \StringTok{"Sold below avg."}\NormalTok{),}
    \DataTypeTok{Kitchen_Qual =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_relevel}\NormalTok{(}
\NormalTok{      Kitchen_Qual, }\StringTok{"Poor"}\NormalTok{, }\StringTok{"Fair"}\NormalTok{, }\StringTok{"Typical"}\NormalTok{, }\StringTok{"Good"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Kitchen_Qual)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{Group)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-facet-05-1} 

}

\caption{Kitchen quality for homes that sold below average (left panel) and above average (right panel).}\label{fig:04-facet-05}
\end{figure}

Figure \ref{fig:04-facet-06} builds onto Figure \ref{fig:04-facet-05}
using \texttt{facet\_grid()}. In this example, we assess kitchen quality
for homes that sold below average and above average across the different
neighborhoods.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{Group =} \KeywordTok{ifelse}\NormalTok{(}
\NormalTok{      Sale_Price }\OperatorTok{>}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price), }\DataTypeTok{yes =} \StringTok{"Sold above avg."}\NormalTok{, }
      \DataTypeTok{no =} \StringTok{"Sold below avg."}\NormalTok{),}
    \DataTypeTok{Kitchen_Qual =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_relevel}\NormalTok{(}
\NormalTok{      Kitchen_Qual, }\StringTok{"Poor"}\NormalTok{, }\StringTok{"Fair"}\NormalTok{, }\StringTok{"Typical"}\NormalTok{, }\StringTok{"Good"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Neighborhood, Group, Kitchen_Qual) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tally}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Kitchen_Qual, pct)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(Neighborhood }\OperatorTok{~}\StringTok{ }\NormalTok{Group) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{strip.text.y =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{0}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-facet-06-1} 

}

\caption{Kitchen quality for homes that sold below average (left panel) and above average (right panel) across the different neighborhoods.}\label{fig:04-facet-06}
\end{figure}

\hypertarget{parallel-coordinate-plots}{%
\subsubsection{Parallel coordinate
plots}\label{parallel-coordinate-plots}}

Parallel coordinate plots (PCPs) are also a great way to visualize
continuous variables across multiple variables. In PCPs, a vertical axis
is drawn for each variable. Then each observation is represented by
drawing a line that connects its values on the different axis, creating
a multivariate profile. To create a PCP, we can use the
\texttt{GGally::ggparcoord()} function. By default,
\texttt{GGally::ggparcoord()} will standardize the variables based on a
\(z\)-score distribution; however, there are many options for scaling
(see \texttt{?GGally::ggparcoord}). A benefit of using PCPs is that you
can visualize observations across both continuous and categorical
variables. In the example below we include \texttt{Overall\_Qual}, an
ordered factor with ten levels
\texttt{"Very\ Poor"\ \textless{}\ "Poor"\ \textless{}\ "Fair"\ \textless{}\ ...\ \textless{}\ "Excellent"\ \textless{}\ "Very\ Excellent"}
having integer values of 1--10. When including a factor variable,
\texttt{GGally::ggparcoord()} will use the factor integer levels as
their corresponding value, so it is important to appropriately order any
factors you want to include.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variables of interest}
\NormalTok{variables <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sale_Price"}\NormalTok{, }\StringTok{"Year_Built"}\NormalTok{, }\StringTok{"Year_Remod_Add"}\NormalTok{, }\StringTok{"Overall_Qual"}\NormalTok{)}

\CommentTok{# Parallel coordinate plot}
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(variables) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{GGally}\OperatorTok{::}\KeywordTok{ggparcoord}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{scale =} \StringTok{"center"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-pcp-01-1} 

}

\caption{Parallel coordinate plot.}\label{fig:04-pcp-01}
\end{figure}

The darker bands in the above plot illustrate several features. The
observations with higher sales prices tend to be built in more recent
years, be remodeled in recent years and be categorized in the top half
of the overall quality measures. In contracts, homes with lower sales
prices tend to be more out-dated (based on older built and remodel
dates) and have lower quality ratings. We also see some homes with
exceptionally old build dates that have much newer remodel dates but
still have just average quality ratings.

We can make this more explicit by adding a new variable to indicate if a
sale price is above average. We can then tell
\texttt{GGally::ggparcood()} to group by this new variable. Now we
clearly see that above average sale prices are related to much newer
homes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(variables) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Above_Avg =}\NormalTok{ Sale_Price }\OperatorTok{>}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price)) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{GGally}\OperatorTok{::}\KeywordTok{ggparcoord}\NormalTok{(}
    \DataTypeTok{alpha =} \FloatTok{0.05}\NormalTok{,}
    \DataTypeTok{scale =} \StringTok{"center"}\NormalTok{,}
    \DataTypeTok{columns =} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}
    \DataTypeTok{groupColumn =} \StringTok{"Above_Avg"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-pcp-02-1} 

}

\caption{Refined parallel coordinate plot.}\label{fig:04-pcp-02}
\end{figure}

\hypertarget{mosaic-plots}{%
\subsubsection{Mosaic plots}\label{mosaic-plots}}

Mosaic plots are a graphical method for visualizing data from two or
more qualitative variables. In this visual the graphics area is divided
up into rectangles proportional in size to the counts of the
combinations they represent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames2 <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{Above_Avg =}\NormalTok{ Sale_Price }\OperatorTok{>}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price),}
    \DataTypeTok{Garage_Type =} \KeywordTok{abbreviate}\NormalTok{(Garage_Type),}
    \DataTypeTok{Garage_Qual =} \KeywordTok{abbreviate}\NormalTok{(Garage_Qual)}
\NormalTok{  )}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{mosaicplot}\NormalTok{(Above_Avg }\OperatorTok{~}\StringTok{ }\NormalTok{Garage_Type, }\DataTypeTok{data =}\NormalTok{ ames2, }\DataTypeTok{las =} \DecValTok{1}\NormalTok{)}
\KeywordTok{mosaicplot}\NormalTok{(Above_Avg }\OperatorTok{~}\StringTok{ }\NormalTok{Garage_Type }\OperatorTok{+}\StringTok{ }\NormalTok{Garage_Cars, }\DataTypeTok{data =}\NormalTok{ ames2, }\DataTypeTok{las =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/mosaic-1} 

}

\caption{Mosaic plot.}\label{fig:mosaic}
\end{figure}

\hypertarget{tree-maps}{%
\subsubsection{Tree maps}\label{tree-maps}}

Tree maps are also a useful visualization aimed at assessing the
hierarchical structure of data. Tree maps are primarily used to assess a
numeric value across multiple categories. It can be useful to assess the
counts or proportions of a categorical variable nested within other
categorical variables. For example, we can use a tree map to visualize
the above right mosaic plot that illustrates the number of homes sold
above and below average sales price with different garage
characteristics. We can see in the tree map that houses with above
average prices tend to have attached 2 and 3-car garages. Houses sold
below average price have more attached 1-car garages and also have far
more detached garages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Above_Below =} \KeywordTok{ifelse}\NormalTok{(Sale_Price }\OperatorTok{>}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price), }\StringTok{"Above Avg"}\NormalTok{, }\StringTok{"Below Avg"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{count}\NormalTok{(Garage_Type, Garage_Cars, Above_Below) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{treemap}\OperatorTok{::}\KeywordTok{treemap}\NormalTok{(}
    \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"Above_Below"}\NormalTok{, }\StringTok{"Garage_Type"}\NormalTok{, }\StringTok{"Garage_Cars"}\NormalTok{),}
    \DataTypeTok{vSize =} \StringTok{"n"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-tree-map-01-1} 

}

\caption{Treemaps to illustrate hierarchical structures.}\label{fig:04-tree-map-01}
\end{figure}

\hypertarget{heat-maps}{%
\subsubsection{Heat maps}\label{heat-maps}}

A \emph{heat map} is essentially a false color image of a matrix or data
set. Heat maps can be extremely useful in identifying clusters of
values; a common use is in plotting a correlation matrix (Figure
\ref{fig:04-heat-map-01}). In the code chunk below we select all the
numeric variables in \texttt{ames}, compute the correlation matrix
between them, and visualize the results with a heat map. Bright/dark
spots represent clusters of observations with similar correlations. From
Figure \ref{fig:04-heat-map-01}, we can see that \texttt{Sale\_Price}
(3rd row from top) has a relatively weak linear association with
variables such as \texttt{BsmtFin\_Sf\_1}, \texttt{Bsmt\_Unf\_SF},
\texttt{Longitude}, and \texttt{Enclosed\_Porch}. The larger
correlations values for \texttt{Sale\_Price} align with variables such
as \texttt{Garage\_Cars}, \texttt{Garage\_Area}, and
\texttt{First\_Flr\_SF}, etc.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{          }\CommentTok{# select all the numeric columns}
\StringTok{  }\KeywordTok{cor}\NormalTok{() }\OperatorTok{%>%}\StringTok{                          }\CommentTok{# compute the correlation matrix}
\StringTok{  }\KeywordTok{heatmap}\NormalTok{(                           }
    \DataTypeTok{symm =} \OtherTok{TRUE}\NormalTok{,                     }\CommentTok{# since correlation matrices are symmetric!}
    \DataTypeTok{col =}\NormalTok{ viridis}\OperatorTok{::}\KeywordTok{inferno}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ames))}
\NormalTok{  )  }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-heat-map-01-1} 

}

\caption{heat map where yellow represents highly correlated values and red represents weakly correlated values.}\label{fig:04-heat-map-01}
\end{figure}

\hypertarget{generalized-pairs-plot}{%
\subsubsection{Generalized pairs plot}\label{generalized-pairs-plot}}

When dealing with a small data set (or subset of a large data set), it
can be useful to construct a matrix of plots comparing two-way
relationships across a number of variables. In the code chunk below we
(i) select \texttt{Sale\_Price} and all variables names that contain
\texttt{"sf"} (i.e., all square footage variables), (ii) scale all
variables, and (iii) display scatter plots and correlation values with
the \texttt{GGally::ggpairs()} function. The results are displayed in
Figure \ref{fig:04-ggpairs-01}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Sale_Price, }\KeywordTok{contains}\NormalTok{(}\StringTok{"sf"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# select column names containing "sf"}
\StringTok{  }\NormalTok{purrr}\OperatorTok{::}\KeywordTok{map_df}\NormalTok{(scale) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{GGally}\OperatorTok{::}\KeywordTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-ggpairs-01-1} 

}

\caption{Pairwise scatter plot.}\label{fig:04-ggpairs-01}
\end{figure}

\hypertarget{data-quality}{%
\section{Data quality}\label{data-quality}}

Data quality is an important issue for any project involving analyzing
data. Data quality issues deserves an entire book in its own right, and
a good reference is the The Quartz guide to bad data \citep{quartz}. In
this section, we discuss one topic of particular importance: visualizing
missing data.

It is important to understand the distribution of missing values (i.e.,
\texttt{NA}) is any data set. So far, we have been using a pre-processed
version of the Ames housing data set (via the
\texttt{AmesHousing::make\_ames()} function). However, if we use the raw
Ames housing data (via \texttt{AmesHousing::ames\_raw}), there are
actually 13,997 missing values---there is at least one missing values in
each row of the original data!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(AmesHousing}\OperatorTok{::}\NormalTok{ames_raw))}
\NormalTok{## [1] 13997}
\end{Highlighting}
\end{Shaded}

It is important to understand the distribution of missing values in a
data set in order to determine 1) if any variable(s) should be
eliminated prior to analysis, or 2) if any values need to be
\emph{imputed}\footnote{Imputation essentially just means filling in
  missing values with an \emph{intelligent guess}. We do not discuss
  missing value imputation in this book, but the interested reader is
  referred to \citet{van-flexible-2012} and the R package \texttt{mice}
  \citep{R-mice}.}.

Heat maps are an efficient way to visualize the distribution of missing
values for small- to medium-sized data sets. The code
\texttt{is.na(\textless{}data-frame-name\textgreater{})} will return a
matrix of the same dimension as the given dsta frame, but each cell will
contain either \texttt{TRUE} (if the corresponding value is missing) or
\texttt{FALSE} (if the corresponding value is not missing). To construct
such a plot, we can use R'2 built-in \texttt{heatmap()} or
\texttt{image()} functions, or \texttt{ggplot2}'s
\texttt{geom\_raster()} function, among others; we use
\texttt{geom\_raster()} below. This allows us to easily see where the
majority of missing values occur (i.e., in the variables \texttt{Alley},
\texttt{Fireplace\ Qual}, \texttt{Pool\ QC}, \texttt{Fence}, and
\texttt{Misc\ Feature}). Due to their high frequency of missingness,
these variables would likely need to be removed prior to statiscial
analysis, or imputed (i.e., filled in with intelligent guesses). We can
also spot obvious patterns of missingness. For example, missing values
appear to occur within the same observations across all garage
variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AmesHousing}\OperatorTok{::}\NormalTok{ames_raw }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{is.na}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\NormalTok{reshape2}\OperatorTok{::}\KeywordTok{melt}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Var2, Var1, }\DataTypeTok{fill=}\NormalTok{value)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_raster}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_grey}\NormalTok{(}\DataTypeTok{name =} \StringTok{""}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Present"}\NormalTok{, }\StringTok{"Missing"}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =} \StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.y  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/04-heat-map-02-1} 

}

\caption{Heat map of ,issing values in the raw Ames housing data.}\label{fig:04-heat-map-02}
\end{figure}

Digging a little deeper into these variables, we might notice that
\texttt{Garage\_Cars} and \texttt{Garage\_Area} contain the value
\texttt{0} whenever the other \texttt{Garage\_xx} variables have missing
values (i.e.~a value of \texttt{NA}). This might be because they did not
have a way to identify houses with no garages when the data were
originally collected; and therefore, all houses with no garage were
identified by including nothing. Since this missingness is informative,
it would be appropriate to impute \texttt{NA} with a new category level
(e.g., \texttt{"None"}) for these garage variables. Circumstances like
this tend to only arise upon careful descriptive and visual examination
of the data!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AmesHousing}\OperatorTok{::}\NormalTok{ames_raw }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(}\StringTok{`}\DataTypeTok{Garage Type}\StringTok{`}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{contains}\NormalTok{(}\StringTok{"garage"}\NormalTok{))}
\NormalTok{## # A tibble: 157 x 7}
\NormalTok{##    `Garage Type` `Garage Yr Blt` `Garage Finish`}
\NormalTok{##    <chr>                   <int> <chr>          }
\NormalTok{##  1 <NA>                       NA <NA>           }
\NormalTok{##  2 <NA>                       NA <NA>           }
\NormalTok{##  3 <NA>                       NA <NA>           }
\NormalTok{##  4 <NA>                       NA <NA>           }
\NormalTok{##  5 <NA>                       NA <NA>           }
\NormalTok{##  6 <NA>                       NA <NA>           }
\NormalTok{##  7 <NA>                       NA <NA>           }
\NormalTok{##  8 <NA>                       NA <NA>           }
\NormalTok{##  9 <NA>                       NA <NA>           }
\NormalTok{## 10 <NA>                       NA <NA>           }
\NormalTok{## # ... with 147 more rows, and 4 more variables:}
\NormalTok{## #   `Garage Cars` <int>, `Garage Area` <int>, `Garage}
\NormalTok{## #   Qual` <chr>, `Garage Cond` <chr>}
\end{Highlighting}
\end{Shaded}

The \texttt{visna()} function in R package \texttt{extracat}
\citep{R-extracat} allows for easy visualization of missing data
patterns. We illustrate this functionality below using the raw Ames
housing data (Figure @ref(fig: 04-missingness-02)). The columns of the
heat map represent the 82 variables of the raw data and the rows
represent the observations. By default, missing values (i.e.,
\texttt{NA}) are inidcated via a blue cell. The variables and patterns
have been ordered by the number of \texttt{NA}s on both rows and columns
(i.e., \texttt{sort\ =\ "b"}). The bars beneath the columns show the
proportion of \texttt{NA}s by variable and the bars on the right show
the relative frequency of \texttt{NA}s.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extracat}\OperatorTok{::}\KeywordTok{visna}\NormalTok{(AmesHousing}\OperatorTok{::}\NormalTok{ames_raw, }\DataTypeTok{sort =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{abar_files/figure-latex/04-missingness-02-1} 

}

\caption{Visualizing missing patterns}\label{fig:04-missingness-02}
\end{figure}

Data can be missing for different reasons. Perhaps the values was never
recroded (or lost in trabnslation), or it was recorded an error (a
common feature of data enetered by hand). Regardless, it is important to
identify and attempt to understand how missing values are distributed
across a data set as it can provide insight into how to deal with these
observations.

\hypertarget{further-reading}{%
\section{Further reading}\label{further-reading}}

List some good books, like \citet{cleveland-visualizing-1993}!

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

Coming soon!

Can use this to introduce additional plots/geoms!

\hypertarget{inference}{%
\chapter{Statistical Inference}\label{inference}}

\begin{quote}
``To consult the statistician after an experiment is finished is often
merely to ask him to conduct a post mortem examination. He can perhaps
say what the experiment died of.''

--- Sir Ronald Fisher
\end{quote}

Suppose you are moving to Ames, Iowa and are considering buying a home.
How would you know whether or not the house you are considering is
averagely priced, significantly more expensive, or significantly less
expensive? With all of the data available on the web, it is possible to
gather data selling prices of similar homes in the area. From this data,
which we call a \emph{reference distribution}, it can be determined
whether or not the price of a particular home is on par with similar
homes in the neighborhood.

Say, for example, the price of a new home for sale in Ames, Iowa is
\$610,000. Using historical data, we can compare this price against a
reference distribution. This is illustrated in the code chunk below
using the \texttt{ames} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sale_Price <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}\OperatorTok{$}\NormalTok{Sale_Price}
\NormalTok{(}\DecValTok{610000} \OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Sale_Price)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(Sale_Price)  }\CommentTok{# compute a z-score}
\NormalTok{## [1] 5.373}
\end{Highlighting}
\end{Shaded}

So a house costing \$610,000 is more than five standard deviations
beyond the mean of all the houses sold between the years 2006 and
2010---of course, a more fair comparison would only involve houses with
similar features (e.g., a fireplace, finished basement, same
neighborhood, etc.).

Classical statistical inference (e.g., \emph{significance testing}) is a
similar process. An investigator or analyst considers the result from a
particular experiment and wants to know whether or not the result is
\emph{statistically significant}\footnote{Keep in mind that a
  statistically significant result does not necessarilly imply a
  \emph{practically significant} result (recall the importance of
  specifying effect sizes).} (e.g., due to the varying experimental
conditions), or due to chance alone. In order to make this conclusion, a
relative reference set is required that characterizes the outcome if the
varying experimental factors truly had no impact on the result. The
observed outcome can then be compared to this reference distribution and
the statistical significance of the result can be quantified. This
approach to statistical inference is called the \emph{frequentist}
approach---in contrast to \emph{Bayesian inference} which is not
discussed in this book.

\hypertarget{the-frequentist-approach}{%
\section{The frequentist approach}\label{the-frequentist-approach}}

The most common methods in statistical inference are based on the
frequentist approach to probability. Many of the common statistical
tests, like the one-sample \(t\)-test, follow the same paradigm: compute
a test statistic associated with the population attribute of interest
(e.g., the mean), determine it's \emph{sampling distribution}, and use
the sampling distribution to compute a \(p\)-value, construct a
\emph{confidence interval}, etc.

The sampling distribution of a statistic (e.g., a test statistic), based
on a sample of size \(n\), is the distribution obtained after taking
every possible sample of size \(n\) from the population of interest and
computing the sample statistic for each; see, for example, Figure
\ref{fig:sampling-distribution}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/sampling-distribution} 

}

\caption{Frequentist approach to sampling and sampling distributions.}\label{fig:sampling-distribution}
\end{figure}

In some cases, the sampling distribution of the statistic is known,
provided certain assumptions are met (like independent observations and
normality). For example, consider a random sample
\(x_1, x_2, \dots, x_n\) from a normal distribution with mean \(\mu\)
and variance \(\sigma ^ 2\). As it turns out, the statistic
\begin{equation}
  z_{obs} = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}},
  \label{eq:zobs}
\end{equation} has a standard normal distribution. From this sampling
distribution, we can formulate a confidence for the true mean \(\mu\),
or test specific hypotheses. In practice, \(\sigma\) is unknown and is
estimated using the sample standard deviation, \(s\). Replacing
\(\sigma\) with \(s\) in Equation \eqref{eq:zobs} results in another
statistic, called the \(t\)-statistic, and is given by \begin{equation}
  t_{obs} = \frac{\bar{x} - \mu}{s / \sqrt{n}}.
  \label{eq:tobs}
\end{equation} \citet{student-probable-1908} showed that \(t_{obs}\)
follows a \(t\)-distribution with \(n - 1\) \emph{degrees of freedom}.

In more complicated examples, the sampling distribution of a statistic
is not known, or is rather complicated (e.g., the correlation
coefficient or the ratio of two means), but can be simulated through a
process called the \emph{booststrap}, which we discuss in
\ref{the-nonparametric-bootstrap}.

\hypertarget{the-central-limit-theorem}{%
\subsection{The central limit theorem}\label{the-central-limit-theorem}}

One of the most common goals in classical statistical inference is to
make inference regarding the mean of a population: \[
  H_0: \mu = \mu_0 \quad vs. \quad H_1: \mu \ne \mu_0,
\] where \(\mu\) is the true population mean and \(\mu_0\) is some
hypothetical value. Assume we have a random sample
\(x_1, x_2, \dots, x_n\) from the population of interest. If the data
are normally distributed, we can test this hypothesis using a standard
\(t\)-test (discussed later). If the data are not normally distributed,
then the \emph{central limit theorem} (CLM) tells us that the sampling
distribution of the sample mean (or more simply the sample total) will
be approximately normal for sufficiently large \(n\). How large does
\(n\) need to be? The answer depends on how far the true distribution
deviates from normality! A common rule of thumb, though not always
adequate, is that \(n > 30\) is sufficient to invoke the CLM. For
instance, as we saw in Chapters 2--3, the distribution of
\texttt{Sale\_Price} is quite skewed to the right. However, the sampling
distribution of the mean from this population will be approximately
normal provided \(n\) is sufficiently large. For example, we simulated
10\^{}\{4\} sample means from \texttt{Sale\_Price} based on sample of
various sizes. The resulting sampling distributions are displayed in
Figure \ref{fig:sale-price-clm}. Clearly, the sampling distribution
becomes more bell-shaped and normal looking as the sample size
increases; for this population, \(n = 30\) seems sufficient.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/sale-price-clm-1} 

}

\caption{Sampling distribution of mean sale price based on samples of size $n = 5$ (top left), $n = 10$ (top right), $n = 30$ (bottom left), and $n = 100$ (bottom right).}\label{fig:sale-price-clm}
\end{figure}

The classical tests and procedures discussed in this chapter assume that
we are sampling from populations that are infinitely large. In most
cases, however, the populations from which we obtain samples are finite.

\hypertarget{hypothesis-testing}{%
\subsection{Hypothesis testing}\label{hypothesis-testing}}

Univariate statistical tests of hypotheses usually concern a single
parameter, say \(\theta\). For instance, \(\theta\) could be the mean of
a single population (i.e., \(\theta = \mu\)), or the difference between
the means of two populations (i.e., \(\theta = \mu_1 - \mu_2\)). The
\emph{null hypothesis}, denoted \(H_0\), represents the status quo of
\(\theta\) and the alternative hypothesis, denoted \(H_1\) or \(H_a\),
represents the research hypothesis regarding \(\theta\). For instance,
in comparing the means of two populations, we may be interested in
testing \[
H_0: \mu_1 - \mu_2 = \delta_0 \quad \text{vs.} \quad H_1: \mu_1 - \mu_2 \ne \delta_0,
\] where \(\delta_0\) is some hypothetical value (usually \(0\)
signifying no difference in the means of the two populations). To carry
out such tests, we require an estimate of \(\theta\), say
\(\widehat{\theta}\), and its corresponding sampling distribution.

\hypertarget{one-sided-versus-two-sided-tests}{%
\subsection{One-sided versus two-sided
tests}\label{one-sided-versus-two-sided-tests}}

The previous hypothesis test involved a two-sided alternative (i.e.,
\(H_1: \mu_1 - \mu_2 \ne \delta_0\)). Such a test is called a
\emph{two-sided test}. It is possible, though less common, to use a
one-sided alternative of the form \[
H_1: \theta < \theta_0 \quad \text{or} \quad H_1: \theta > \theta_0
\] A word of caution regarding one-sided alternatives is to avoid them!
These are more common in experimental studies where \emph{a priori}
information is available suggesting that the population attribute of
interest is either less than or greater than some hypothetical value.
Although the proceeding discussions apply specifically to two-sided
tests, the methodology can easily be amended to accommodate one-sided
tests.

\hypertarget{type-i-and-type-ii-errors}{%
\subsection{Type I and type II errors}\label{type-i-and-type-ii-errors}}

Statistical significance testing relies on the \emph{presumption of
innocence}. That is, we fail to reject the null hypothesis unless the
data provide sufficient evidence to say otherwise. For example, in the
US criminal justice system, the defendant is assumed innocent until
proven guilty: \[
H_0: \text{Defendant is innocent} \quad \text{vs.} \quad H_1: \text{Defendant is guilty}
\] Whenever we conduct a statistical test of hypothesis, a decision is
made regarding the null hypothesis. Since this is a binary decision,
there are four possible outcomes, two of which are errors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Convict the defendant when the defendant is guilty (a good decision)
\item
  Convict the defendant when the defendant is innocent (a bad decision)
\item
  Fail to convict the defendant when the defendant is guilty (a bad
  decision)
\item
  Fail to convict the defendant when the defendant is innocent (a good
  decision)
\end{enumerate}

Which decision is worst? Naturally, it would be worse to convict an
innocent person than to let a guilty person go free. We call the first
type of error a \emph{type I error}, and the second a \emph{type II
error}. Furthermore, we denote the probability of making a type I error
as \(\alpha\) and the probability of making a type II error as
\(\beta\). The classic approach to statistical testing fixes the
probability of making a type I error ahead of time (e.g.,
\(\alpha = 0.05\)), we then do our best to reduce the risk of making a
type II error (e.g., collecting sufficient sample size). In order to
carry out a test with the goal of mitigating the probability of making a
type II error we would conduct a \emph{power analysis}, which we do not
discuss here---the classic reference is \citet{cohen-statistical-1988}.

\textbf{INSERT TABLE HERE}

\hypertarget{p-values}{%
\subsection{\texorpdfstring{\(p\)-values}{p-values}}\label{p-values}}

There are three equivalent approaches to conducting hypothesis tests:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \emph{rejection region} approach (the least informative).
\item
  The \(p\)-value approach.
\item
  The \emph{confidence interval} approach.
\end{enumerate}

The latter two are the most informative as they provide information
beyond our decision to simply reject or fail to reject a null
hypothesis. This section discusses \(p\)-values.

\(p\)-values provide a measure of evidence in favor of or against the
null hypothesis. The \(p\)-value, denoted \(p\), can be thought of as
the \emph{observed significance level}. We would reject the null
hypothesis at the \(\alpha\) level of significance whenever
\(p < \alpha\). Table \ref{tab:p-values} provides a general guideline
for interpreting \(p\)-values.

\begin{longtable}[]{@{}cl@{}}
\caption{\label{tab:p-values} p-values.}\tabularnewline
\toprule
Result & Interpretation\tabularnewline
\midrule
\endfirsthead
\toprule
Result & Interpretation\tabularnewline
\midrule
\endhead
\(p \le 0.01\) & Very strong evidence against the null\tabularnewline
\(0.01 < p \le 0.05\) & Strong evidence against the null\tabularnewline
\(0.05 < p \le 0.10\) & Moderate evidence against the
null\tabularnewline
\(0.10 < p \le 0.20\) & Weak evidence against the null\tabularnewline
\(p > 0.20\) & No evidence against the null\tabularnewline
\bottomrule
\end{longtable}

To put another way, \(p\)-values tell us the smallest value of
\(\alpha\) that would result in rejecting the null hypothesis. Keep in
mind, however, that it is highly unethical to change \(\alpha\) after
comparing it to the \(p\)-value in order to change the resulting
decision of the test---the significance level should be stated before
the data are inspected, or even collected, and never be changed
thereafter. To compute a \(p\)-value, we need to be able to compute
probabilities from the sampling distribution of the test statistic under
the assumption that the null hypothesis is true. Most statistical tests
built into R, however, compute \(p\)-values that are provided in the
output.

\hypertarget{confidence-intervals}{%
\subsection{Confidence intervals}\label{confidence-intervals}}

\emph{Confidence intervals} assign a range of plausible values to the
population attribute of interest. A traditional
\(100\left(1 - \alpha\right)\)\% confidence interval for a population
parameter \(\theta\) has the form \begin{equation}
  \widehat{\theta} \pm \gamma_{1 - \alpha / 2} \widehat{SE}\left(\widehat{\theta}\right),
  \label{eq:conf-int}
\end{equation} where \(\widehat{\theta}\) is an appropriate estimate of
\(\theta\), \(\gamma_{1 - \alpha / 2}\) is the \(1 - \alpha / 2\)
quantile from an appropriate reference distribution, and
\(\widehat{SE}\left(\widehat{\theta}\right)\) is the estimated standard
error of \(\widehat{\theta}\). Confidence intervals of the form
\eqref{eq:conf-int} are commonly used in practice, but are not always
accurate---they assume that the sampling distribution of
\(\widehat{\theta}\) is symmetric. Later in this chapter, we discuss the
\emph{nonparametric bootstrap}, a simulation-based approach to
estimating \(\widehat{SE}\left(\widehat{\theta}\right)\) and computing
confidence intervals that does not assume a theoretical sampling
distribution for \(\widehat{\theta}\).

\hypertarget{one--and-two-sample-t-tests}{%
\section{One- and two-sample
t-tests}\label{one--and-two-sample-t-tests}}

One of the most common goals in classical statistical inference is to
make inference regarding the mean of a single population
(\(\theta = \mu\)) or the difference in means between two populations
(\(\theta = \mu_1 - \mu_2\)). And the corresponding test has the form \[
  H_0: \theta = \theta_0 \quad \text{vs.} \quad H_1: \theta \ne \theta_0,
\] where \(\theta_0\) is the hypothesized value of the mean or
difference in means.

\hypertarget{one-sample-t-test}{%
\subsection{One-sample t-test}\label{one-sample-t-test}}

Assume we have a random sample \(x_1, x_2, \dots, x_n\) from the
population of interest with sample mean \(\bar{x}\) and sample standard
deviation \(s\). If the data are normally distributed, we can test this
hypothesis using a standard \(t\)-test. If the data are not normally
distributed, then the \emph{central limit theorem} (CLM) tells us that
the sampling distribution of the sample mean will be approximately
normal for sufficiently large \(n\). How large does \(n\) need to be?
The answer depends on how far the true distribution deviates from
normality! A common rule of thumb, though not always sufficient, is that
\(n > 30\) is required to invoke the CLM.

The test statistic for the one-sample \(t\)-test is \[
  t_{obs} = \frac{\bar{x} - \theta_0}{s / \sqrt{n}}.
\] If the null hypothesis is true, then \(t_{obs}\) comes from a
\(t\)-distribution with \(n - 1\) \emph{degrees of freedom}. We would
reject the null hypothesis if \(|t_{obs}| > t_{1 - \alpha / 2, n - 1}\),
where \(t_{1 - \alpha / 2, n - 1}\) is the \(1 - \alpha / 2\) quantile
from a \(t\)-distribution with \(n - 1\) degrees of freedom. A
\(100\left(1 - \alpha\right)\)\% confidence interval for true mean is
given by \begin{equation}
  \bar{x} \pm t_{1 - \alpha / 2, n - 1} \times \frac{s}{\sqrt{n}}
  \label{eq:one-sample-t-test-ci}
\end{equation} Correspondingly, we would reject the null hypothesis
whenever the hypothesized value \(\theta_0\) is not contained within
\eqref{eq:one-sample-t-test-ci}. A \(p\)-value for the test can also be
computes as \(p = 2 \times Pr\left(T_{n - 1} > |t_{obs}|\right)\), where
\(T_{n - 1}\) is a random variable following a \(t\)-distribution with
\(n - 1\) degrees of freedom. In other words, the \(p\)-value is the
area under the curve of a \(t\)-distribution with \(n - 1\) degrees of
freedom to the right \(t_{obs}\); see Figure \ref{fig:t-dist}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/t-dist-1} 

}

\caption{$t$-distribution with $n - 1$ degrees of freedom. The shaded area corresponds to the $p$-value of the test.}\label{fig:t-dist}
\end{figure}

To illustrate, the one-sample \(t\)-test, we'll use the \texttt{ames}
data set. In Chapters 2--3, we provided both numerical and visual
descriptions of \texttt{Sale\_Price}. Below, we use R's built-in
\texttt{t.test} function to obtain a 95\% confidence interval for the
true mean selling price based on a random sample of size \(n = 50\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1551}\NormalTok{)  }\CommentTok{# for reproducibility}
\NormalTok{sp50 <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(Sale_Price, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{t.test}\NormalTok{(sp50, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }\DataTypeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{## }
\NormalTok{##  One Sample t-test}
\NormalTok{## }
\NormalTok{## data:  sp50}
\NormalTok{## t = 21, df = 49, p-value <2e-16}
\NormalTok{## alternative hypothesis: true mean is not equal to 0}
\NormalTok{## 95 percent confidence interval:}
\NormalTok{##  158237 192246}
\NormalTok{## sample estimates:}
\NormalTok{## mean of x }
\NormalTok{##    175241}
\end{Highlighting}
\end{Shaded}

Based on the output, a 95\% confidence interval for the mean selling
price, based on a sample of size \(n = 50\) is
\(\left(1.5824\times 10^{5}, 1.9225\times 10^{5}\right)\). By default,
the \texttt{t.test} function uses \(\theta_0 = 0\). To specify a
different value, use the \texttt{mu} argument:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(sp50, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }\DataTypeTok{conf.level =} \FloatTok{0.95}\NormalTok{, }\DataTypeTok{mu =} \DecValTok{600000}\NormalTok{)}
\NormalTok{## }
\NormalTok{##  One Sample t-test}
\NormalTok{## }
\NormalTok{## data:  sp50}
\NormalTok{## t = -50, df = 49, p-value <2e-16}
\NormalTok{## alternative hypothesis: true mean is not equal to 6e+05}
\NormalTok{## 95 percent confidence interval:}
\NormalTok{##  158237 192246}
\NormalTok{## sample estimates:}
\NormalTok{## mean of x }
\NormalTok{##    175241}
\end{Highlighting}
\end{Shaded}

The resulting confidence interval is the same, but the corresponding
\(p\)-value now correspond to the testing whether or not the population
mean significantly differs from the value \$\(600,000\); in this case,
we would fail to reject the null hypothesis at the \(0.05\) level of
significance.

\hypertarget{two-sample-t-test}{%
\subsection{Two-sample t-test}\label{two-sample-t-test}}

Assume we have a random sample \(x_1, x_2, \dots, x_n\) from one
population of interest with sample mean \(\bar{x}\) and sample standard
deviation \(s_x\) and another random sample \(y_1, y_2, \dots, y_n\)
from a second population of interest with sample mean \(\bar{y}\) and
sample standard deviation \(s_y\). For the two-sample \(t\)-test,
\(\theta = \mu_x - \mu_y\) and \(\theta_0\) is often \(0\) (i.e., no
difference between the population means). Of course, no two means are
exactly equal! What we really care about is whether or not the true
difference is small enough to say that the two means are practically the
same. The more data we have, the smaller a true difference we are able
to detect.

In performing a two-sample \(t\)-test, we have to make an assumption
regarding the variance of the two populations. The assumption we make
here determines which two-sample \(t\)-test will be used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pooled variance \(t\)-test (\(\sigma_1 ^ 2 = \sigma_2 ^ 2\))
\item
  Welch's two-sample \(t\)-test (\(\sigma_1 ^ 2 \ne \sigma_2 ^ 2\)).
\end{enumerate}

Since the variances of two populations are not typically equal in
practice, we discuss Welch's two-sample \(t\)-test. (Even when the
population variance are equal, Welch's \(t\)-test can still provide
satisfactory results.) The test statistic corresponding to Welch's
\(t\)-test is given by \[
  t_{obs} = \frac{\bar{x} - \bar{y}}{\sqrt{s_1 ^ 2 / n_1 + s_2 ^ 2 / n_2}}
\] The trouble with Welch's \(t\)-test is that the \(t_obs\) does not
come from a \(t\)-distribution, but can be approximated by a
\(t\)-distribution with \(v\) degrees of freedom, where \(v\) is given
by the \textbf{Satterthwaite approximation}: \[
  v = TBD.
\] Fortunately, the degrees of freedom is computed automatically by the
\texttt{t.test} function.

Confidence intervals and \(p\)-values can be computed in a manner
analogous to the one-sample \(t\)-test. For instance, a
\(100\left(1 - \alpha\right)\)\% confidence interval for the true
difference \(\mu_1 - \mu_2\) is given by \[
  \bar{x}_1 + \bar{x}_2 \pm t_{1 - \alpha / 2, v} \times \sqrt{s_1 ^ 2 / n_1 + s_2 ^ 2 / n_2},
\] where \(t_{1 - \alpha / 2, v}\) is the \(1 - \alpha / 2\) quantile
from a \(t\)-distribution with \(v\) degrees of freedom.

\textbf{A/B test example}

\hypertarget{tests-involving-more-than-two-means-anova-models}{%
\section{Tests involving more than two means: ANOVA
models}\label{tests-involving-more-than-two-means-anova-models}}

TBD.

\hypertarget{testing-for-association-in-contingency-tables}{%
\section{Testing for association in contingency
tables}\label{testing-for-association-in-contingency-tables}}

TBD.

\hypertarget{nonparametric-tests}{%
\section{Nonparametric tests}\label{nonparametric-tests}}

TBD.

\hypertarget{bootstrap}{%
\section{The nonparametric bootstrap}\label{bootstrap}}

The statistical tests discussed so far in this chapter assume a
theoretical sampling distribution for the corresponding test statistic
\(\widehat{\theta}\), which requires certain assumptions like large
sample sizes (when appealing to the CLT) or normality of the population
from which the sample was obtained. These assumptions are often
difficult to meet in practice. The \emph{bootstrap} technique
\citep{efron-bootstrap-1979} estimates the sampling \(\widehat{\theta}\)
by direct simulation. In general, bootstrap methods fall into one of two
categories, \emph{parametric} and \emph{nonparametric}. In this section,
we briefly introduce the nonparametric bootstrap. A thorough
introduction to the bootstrap and its use in R is provided in
\citet{davison-bootstrap-1997}.

Suppose we have a sample of data
\(\boldsymbol{x} = \left\{x_1, x_2, \dots, x_n\right\}\) from some
population of interest. We can estimate a particular population
attribute \(\theta\) using a statistic that is a function of the sample,
say \(\widehat{\theta} = t\left(\boldsymbol{x}\right)\). In order to
make inference regarding \(\theta\), we need to know the complete
sampling distribution of \(\widehat{\theta}\).

The nonparametric bootstrap constructs the sampling distribution of
\(\widehat{\theta}\) by sampling \textbf{with replacement} from the
original sample; that is, treating the sample as if it were the
population and making repeated resamples from it, each time recomputing
the statistic of interest (see Figure \ref{fig:bootstrap-distribution}).
This is analogous to the frequentist approach displayed in Figure
\ref{fig:sampling-distribution}. A single \emph{bootstrap sample} \[
  \boldsymbol{x} ^ \star = \left\{x_1 ^ \star, x_2 ^ \star, \dots, x_n ^ \star\right\},
\] where \(x_i ^ \star\) \(\left(i = 1, 2, \dots, n\right)\) is drawn
from the original sample \(\boldsymbol{x}\) with replacement. Since
samples are drawn with replacement, each bootstrap sample will contain
duplicate values. In fact, on average, \(1 - e ^ {-1} \approx 63.21\)\%
of the original sample ends up in any particular bootstrap sample. The
original observations not contained in a particular bootstrap sample are
considered \emph{out-of-bag} and will have important implications when
discussing \emph{random forests} in Chapter ?.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{illustrations/bootstrap-distribution} 

}

\caption{Bootstrap distirbution.}\label{fig:bootstrap-distribution}
\end{figure}

With each bootstrap sample, we can compute a bootstrap replicate of the
statistic \(\widehat{\theta}\) \[
  \widehat{\theta} ^ \star = t\left(\boldsymbol{x} ^ \star\right).
\] Given a large number, say \(R\), of bootstrap replicates
\(\widehat{\theta}_1 ^ \star, \widehat{\theta}_2 ^ \star, \dots, \widehat{\theta}_R ^ \star\)
we can form an estimated sampling distribution for the original
statistic \(\widehat{\theta}\). For example, a useful estimate of the
standard error of \(\widehat{\theta}\) is given by \[
  \widehat{SE}\left(\widehat{\theta}\right) ^ \star = \sqrt{\frac{1}{R - 1}\sum_{i = 1} ^ R\left(\widehat{\theta}_i ^ \star - \bar{\widehat{\theta} ^ \star}\right)},
\] where \[
\bar{\widehat{\theta} ^ \star} = \frac{1}{R}\sum_{i = 1} ^ R \widehat{\theta}_i ^ \star
\] is the sample mean of the \(R\) bootstrap replicates of
\(\widehat{\theta}\).

Given a vector of values in R, a random sample with replacement can be
obtained using the \texttt{sample} function, for example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2233}\NormalTok{)  }\CommentTok{# for reproducibility}
\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{##  [1]  2  4  2  6  9 10  8  5 10  4}
\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{##  [1]  5  7 10  4  9 10  5  4  4  9}
\end{Highlighting}
\end{Shaded}

Notice how some values from the original sample get repeated in each
bootstrap sample. For example, \texttt{4} shows up three times in the
second bootstrap sample.

To illustrate, we can bootstrap our sample of \(n = 50\) values of
\texttt{Sale\_Price} to form a bootstrap estimate of the sampling
distribution for the mean sale price. This is shown in Figure
@ref\{fig:bootstrap-distribution-ames\} and was produced using the code
chunk below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1551}\NormalTok{)  }\CommentTok{# for reproducibility}
\NormalTok{x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(Sale_Price, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)  }\CommentTok{# an SRS of size n = 50}
\NormalTok{bootreps <-}\StringTok{ }\NormalTok{plyr}\OperatorTok{::}\KeywordTok{rdply}\NormalTok{(}\DataTypeTok{.n =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{.expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)))}
\KeywordTok{ggplot}\NormalTok{(bootreps, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ V1)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Bootstrap repliacte"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{abar_files/figure-latex/bootstrap-distribution-ames-1} 

}

\caption{Bootstrap distirbution of mean sale price based on a random sample of size 50 using $R = 10,000$ bootstrap samples.}\label{fig:bootstrap-distribution-ames}
\end{figure}

Compare this to the true sampling distributions of the mean selling
price based on various sample sizes illustrated in Figure ?.

\hypertarget{bootstrap-confidence-intervals}{%
\subsection{Bootstrap confidence
intervals}\label{bootstrap-confidence-intervals}}

Confidence intervals can be obtained directly from the bootstrap
distribution of \(\widehat{\theta}\). For example, to obtain an
approximate \(100\left(1 - \alpha\right)\) confidence interval for
\(\theta\), we can use the \(\alpha / 2\) and \(1 - \alpha / 2\)
quantiles from the bootstrap distribution. For the above example, an
approximate 95\% confidence interval for the mean sale price, we get

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(bootreps}\OperatorTok{$}\NormalTok{V1, }\DataTypeTok{probs =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{##   2.5%  97.5% }
\NormalTok{## 159295 191915}
\end{Highlighting}
\end{Shaded}

This is called the \emph{percentile bootstrap interval}. Compare this to
the results from the \(t\)-test procedure, which gave
\(\left(1.5824\times 10^{5}, 1.9225\times 10^{5}\right)\).

So how many bootstrap samples are sufficient? The answer, of course,
depends on the inferential objectives. Previous studies have shown that
far less bootstrap replicates are required when estimating standard
errors (e.g., 200) while more are required for computing confidence
intervals (e.g., \(\ge 1000\)). With the speed of modern computers,
however, the number of bootstrap replicates should be made as large
possible within reason!

\hypertarget{further-reading-1}{%
\section{Further reading}\label{further-reading-1}}

TBD.

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

TBD.

\hypertarget{unsupervised}{%
\chapter{Unsupervised learning}\label{unsupervised}}

\textbf{\emph{Unsupervised learning}}, includes a set of statistical
tools to better understand \emph{n} observations that contain a set of
features (\(x_1, x_2, \dots, x_p\)) but do not contain a response
variable (\emph{Y}). In essence, unsupervised learning is concerned with
identifying groups in a data set. The groups may be defined by the rows
(i.e., \emph{clustering}) or the columns (i.e., \emph{dimension
reduction}); however, the motive in each case is quite different. The
goal of \textbf{\emph{clustering}} is to segment observations into
similar groups based on the observed variables. For example, to divide
consumers into different homogeneous groups, a process known as market
segmentation. In dimension reduction, we are often concerned with
reducing the number of variables in a data set. For example, classical
regression models break down in the presence of highly correlated
features. \textbf{\emph{Principal components analysis}} is a technique
that reduces the feature set to a potentially smaller set of
uncorrelated variables. These variables are often used as the input
variables to simpler modelling techniques like \emph{multiple linear
regression} (Section \ref{multi-lm}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/clustering_vs_pca} 

}

\caption{Clustering identifies groupings among the observations (left). Dimension reduction identifies groupings among the features (right).}\label{fig:cluster-pca}
\end{figure}

Unsupervised learning is often performed as part of an exploratory data
analysis. However, the exercise tends to be more subjective, and there
is no simple goal for the analysis, such as prediction of a response.
Furthermore, it can be hard to assess the quality of results obtained
from unsupervised learning methods. The reason for this is simple. If we
fit a predictive model using a supervised learning technique
(i.e.~linear regression), then it is possible to check our work by
seeing how well our model predicts the response \emph{Y} on observations
not used in fitting the model. However, in unsupervised learning, there
is no way to check our work because we don't know the true answer---the
problem is unsupervised.

\begin{tip}
Examples of how unsupervised methods can be used:

\begin{itemize}
\tightlist
\item
  A marketing firm can divide consumers into different homogeneous
  groups so that tailored marketing strategies can be developed and
  deployed for each segment.
\item
  An online shopping site might try to identify groups of shoppers with
  similar browsing and purchase histories, as well as items that are of
  particular interest to the shoppers within each group. Then an
  individual shopper can be preferentially shown the items in which he
  or she is particularly likely to be interested, based on the purchase
  histories of similar shoppers.
\item
  A search engine might choose what search results to display to a
  particular individual based on the click histories of other
  individuals with similar search patterns.
\item
  A cancer researcher might assay gene expression levels in 100 patients
  with breast cancer. He or she might then look for subgroups among the
  breast cancer samples, or among the genes, in order to obtain a better
  understanding of the disease.
\end{itemize}
\end{tip}

These questions, and many more, can be addressed with unsupervised
learning. This chapter covers the unsupervised learning techniques more
commonly applied for clustering and dimension reduction purposes which
includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Principal components analysis
\item
  K-means cluster analysis
\item
  Hierarchical cluster analysis
\item
  Alternative approaches for mixed data
\end{enumerate}

\hypertarget{prerequisites-2}{%
\section{Prerequisites}\label{prerequisites-2}}

For this section we will use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# data manipulation}
\KeywordTok{library}\NormalTok{(cluster)    }\CommentTok{# clustering algorithms}
\KeywordTok{library}\NormalTok{(factoextra) }\CommentTok{# clustering algorithms & visualization}
\end{Highlighting}
\end{Shaded}

To perform these unsupervised techniques in R, generally, the data
should be prepared as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rows are observations (individuals) and columns are variables (also
  known as \textbf{\emph{tidy}} per \citet{wickham2014tidy}).
\item
  Any missing values in the data must be removed or estimated.
\item
  Typically, the data must all be numeric values; however, in section ??
  we discuss alternative approaches that can be applied to mixed
  (numeric and categorical) data.
\item
  Numeric data must be standardized (i.e.~centered and scaled) to make
  variables comparable. Recall that, standardization consists of
  transforming the variables such that they have mean zero and standard
  deviation one.
\end{enumerate}

We'll continue using the Ames housing data throughout this chapter;
however, for the initial sections we'll only use the numeric variables.
Furthermore, we'll remove the sales price variable which results in 34
of the original variables. What results are all numeric variables that
describe various features of 2930 homes. Our objective will be to
identify various groupings among these variables and observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Sale_Price)}

\KeywordTok{dim}\NormalTok{(ames)}
\NormalTok{## [1] 2930   34}

\CommentTok{# remaining numeric variables describing homes}
\KeywordTok{names}\NormalTok{(ames)}
\NormalTok{##  [1] "Lot_Frontage"       "Lot_Area"          }
\NormalTok{##  [3] "Year_Built"         "Year_Remod_Add"    }
\NormalTok{##  [5] "Mas_Vnr_Area"       "BsmtFin_SF_1"      }
\NormalTok{##  [7] "BsmtFin_SF_2"       "Bsmt_Unf_SF"       }
\NormalTok{##  [9] "Total_Bsmt_SF"      "First_Flr_SF"      }
\NormalTok{## [11] "Second_Flr_SF"      "Low_Qual_Fin_SF"   }
\NormalTok{## [13] "Gr_Liv_Area"        "Bsmt_Full_Bath"    }
\NormalTok{## [15] "Bsmt_Half_Bath"     "Full_Bath"         }
\NormalTok{## [17] "Half_Bath"          "Bedroom_AbvGr"     }
\NormalTok{## [19] "Kitchen_AbvGr"      "TotRms_AbvGrd"     }
\NormalTok{## [21] "Fireplaces"         "Garage_Cars"       }
\NormalTok{## [23] "Garage_Area"        "Wood_Deck_SF"      }
\NormalTok{## [25] "Open_Porch_SF"      "Enclosed_Porch"    }
\NormalTok{## [27] "Three_season_porch" "Screen_Porch"      }
\NormalTok{## [29] "Pool_Area"          "Misc_Val"          }
\NormalTok{## [31] "Mo_Sold"            "Year_Sold"         }
\NormalTok{## [33] "Longitude"          "Latitude"}
\end{Highlighting}
\end{Shaded}

To prepare our data for these techniques, let's make sure our data
complies with the 3 requirements mentioned above. Our data is already
set up in the proper \emph{tidy} fashion where each row is an individual
observation and each column is an individual variable. And as you can
see, there are no missing values in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many missing values are in the data}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(ames))}
\NormalTok{## [1] 0}
\end{Highlighting}
\end{Shaded}

It is usually beneficial for each variable to be centered at zero due to
the fact that it makes comparing each principal component to the mean or
the dissimilarity distances for cluster analysis straightforward. This
also eliminates potential problems with magnitude differences of each
variable. For example, the variance of \texttt{Year\_Built} is 914,
while the variance of \texttt{First\_Flr\_SF} is 1,535,789. The
\texttt{Year\_Built} variable isn't necessarily more variable, it's
simply on a different scale relative to \texttt{First\_Flr\_SF}.
However, due to the math behind PCA and clustering algorithms, the
larger magnitude variables will bias the results.

\begin{note}
However, keep in mind that there may be instances where scaling is not
desirable.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(ames[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{], }\DecValTok{2}\NormalTok{, var)}
\NormalTok{##   Lot_Frontage       Lot_Area     Year_Built }
\NormalTok{##      1.122e+03      6.209e+07      9.148e+02 }
\NormalTok{## Year_Remod_Add   Mas_Vnr_Area   BsmtFin_SF_1 }
\NormalTok{##      4.352e+02      3.191e+04      4.988e+00 }
\NormalTok{##   BsmtFin_SF_2    Bsmt_Unf_SF  Total_Bsmt_SF }
\NormalTok{##      2.861e+04      1.932e+05      1.945e+05 }
\NormalTok{##   First_Flr_SF }
\NormalTok{##      1.536e+05}
\end{Highlighting}
\end{Shaded}

As we don't want our unsupervised techniques to depend on an arbitrary
variable unit, we start by standardizing the data using the R function
\texttt{scale}. However, \texttt{scale} only works on variables coded as
doubles; hence, we need to coerce any integer variables to double.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames_scale <-}\StringTok{ }\NormalTok{ames }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_all}\NormalTok{(as.double) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{scale}\NormalTok{()}

\CommentTok{# check that the mean value for each variable is centered at zero}
\CommentTok{# I only show the first 4 for brevity}
\KeywordTok{summary}\NormalTok{(ames_scale)[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{##   Lot_Frontage       Lot_Area        Year_Built    }
\NormalTok{##  Min.   :-1.721   Min.   :-1.123   Min.   :-3.285  }
\NormalTok{##  1st Qu.:-0.437   1st Qu.:-0.344   1st Qu.:-0.574  }
\NormalTok{##  Median : 0.160   Median :-0.090   Median : 0.054  }
\NormalTok{##  Mean   : 0.000   Mean   : 0.000   Mean   : 0.000  }
\NormalTok{##  3rd Qu.: 0.608   3rd Qu.: 0.179   3rd Qu.: 0.980  }
\NormalTok{##  Max.   : 7.623   Max.   :26.027   Max.   : 1.278  }
\NormalTok{##  Year_Remod_Add  }
\NormalTok{##  Min.   :-1.643  }
\NormalTok{##  1st Qu.:-0.924  }
\NormalTok{##  Median : 0.419  }
\NormalTok{##  Mean   : 0.000  }
\NormalTok{##  3rd Qu.: 0.946  }
\NormalTok{##  Max.   : 1.234}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{note}
PCA and clustering algorithms are influenced by the magnitude of each
variable; therefore, the results obtained when we perform these
algorithms will also depend on whether the variables have been
individually scaled.
\EndKnitrBlock{note}

\hypertarget{pca}{%
\section{Principal Components Analysis}\label{pca}}

Principal components analysis (PCA) \textbf{\emph{reduces the
dimensionality of the feature set}}, allowing most of the variability to
be explained using fewer variables than the original data set. Among our
34 numeric variables within the Ames data set, 16 variables have
moderate correlation ( \(\geq 0.30\)) with at least one other variable.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/unsupervised-correlation-1} 

}

\caption{Top 10 variables containing the strongest correlation with at least one other variable.}\label{fig:unsupervised-correlation}
\end{figure}

Multicollinearity such as this can cause problems in some supervised
models. Moreover, often we want to simply explain common attributes of a
data set in a lower dimensionality than the original data. Within the
Ames data, total ground level square footage (\texttt{Gr\_Liv\_Area})
and total number of rooms above ground (\texttt{TotRms\_AbvGrd}) have a
correlation of 0.81. These two variables largely capture the same
information - living space - of a house. In fact, there are multiple
variables in the Ames data that represent living space and are highly
correlated. Consequently, it can be useful to represent these highly
correlated variables in a lower dimension such as ``living space''.

One option includes examining pairwise scatter plots for each variable
against every other variable and identifying co-variation.
Unfortunately, this is tedious and becomes excessive quickly even with a
small number of variables (given \(p\) variables there are \(p(p-1)/2\)
scatterplot combinations. For example, since our Ames data has 35
numeric variables, we would need to examine \(35(35-1)/2 = 595\)
scatterplots! Clearly, a better method must exist to represent our data
in a smaller dimension.

PCA provides a tool to do just this. It finds a low-dimensional
representation of a data set that contains as much of the variation as
possible. The idea is that each of the \emph{n} observations lives in
\emph{p}-dimensional space, but not all of these dimensions are equally
interesting. PCA seeks a small number of dimensions that are as
interesting as possible, where the concept of \emph{interesting} is
measured by the amount that the observations vary along each dimension.
Each of the dimensions found by PCA is a linear combination of the
\emph{p} features and we can take these linear combinations of the
measurements and reduce the number of plots necessary for visual
analysis while retaining most of the information present in the data.

\hypertarget{finding-principal-components}{%
\subsection{Finding principal
components}\label{finding-principal-components}}

The \emph{first principal component} of a data set \(X_1\), \(X_2\),
\ldots{}, \(X_p\) is the linear combination of the features

\begin{equation}
\label{eq:pca1}
Z_{1} = \phi_{11}X_{1} + \phi_{21}X_{2} + ... + \phi_{p1}X_{p},
\end{equation}

that has the largest variance and where \(\phi_1\) is the first
principal component loading vector, with elements
\(\phi_{12}, \phi_{22},\dots,\phi_{p2}\). The \(\phi\) are
\emph{normalized}, which means that
\(\sum_{j=1}^{p}{\phi_{j1}^{2}} = 1\). After the first principal
component \(Z_1\) of the features has been determined, we can find the
second principal component \(Z_2\). The second principal component is
the linear combination of \(X_1,\dots , X_p\) that has maximal variance
out of all linear combinations that are \textbf{\emph{uncorrelated}}
with \(Z_1\). The second principal component scores
\(z_{12}, z_{22}, \dots, z_{n2}\) take the form

\begin{equation}
\label{eq:pca2}
Z_{2} = \phi_{12}X_{1} + \phi_{22}X_{2} + ... + \phi_{p2}X_{p}
\end{equation}

This proceeds until all principal components are computed. The elements
\(\phi_{11}, ..., \phi_{p1}\) in Eq. 1 are the \emph{loadings} of the
first principal component. To calculate these loadings, we must find the
\(\phi\) vector that maximizes the variance. It can be shown using
techniques from linear algebra that the eigenvector corresponding to the
largest eigenvalue of the covariance matrix is the set of loadings that
explains the greatest proportion of the variability.

An illustration provides a more intuitive grasp of principal components.
Within our Ames housing data, first floor square footage and above
ground square footage have a 0.56 correlation, we can explain the
covariation of these variables in two dimensions (principal component 1
and principal component 2). We see that the greatest co-variation falls
along the first principal component, which is simply the line that
minimizes the total squared distance from each point to its orthogonal
projection onto the line. Consequently, we can explain the vast majority
(93\% to be exact) of variability among first floor square footage and
above ground square footage simply with the first principal component.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/create-pca-image-1} 

}

\caption{Principal components of two living area variables.}\label{fig:create-pca-image}
\end{figure}

We can extend this to three variables, assessing the relationship
between first floor square footage, above ground square footage, and
total number of rooms above ground. The first two principal component
directions span the plane that best fits the data. It minimizes the sum
of squared distances from each point to the plan. As more dimension are
added, these visuals are not as intuitive but we'll see shortly how we
can use PCA to extract and visualize informative information.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/3D-PCA} 

}

\caption{Principal components of three living area variables variables.}\label{fig:pca-3d-plot}
\end{figure}

\hypertarget{performing-pca-in-r}{%
\subsection{Performing PCA in R}\label{performing-pca-in-r}}

R has several built-in functions (along with numerous add-on packages)
that simplifies performing PCA. One of these built-in functions is
\texttt{prcomp}. With \texttt{prcomp} we can perform PCA calculations
quickly. By default, the \texttt{prcomp} function centers the variables
to have mean zero. By using the argument \texttt{scale\ =\ TRUE}, we can
scale the variables to have standard deviation one; however, since we
already standardized our data we'll remove this option. The output from
\texttt{prcomp} contains a number of items.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform PCA}
\NormalTok{pca_result <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(ames_scale, }\DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# various output provided by the model}
\KeywordTok{names}\NormalTok{(pca_result)}
\NormalTok{## [1] "sdev"     "rotation" "center"   "scale"   }
\NormalTok{## [5] "x"}
\end{Highlighting}
\end{Shaded}

The \emph{rotation} matrix provides the principal component loadings.
There are 35 distinct principal components for our data. This is to be
expected because you can have the same number of components as you have
variables. However, shortly I'll show you how to understand how much
each component explains our data.

As for the principal component loadings - remember, the loadings
represent \(\phi_{12}, \phi_{22},\dots,\phi_{p2}\) in Equation
\eqref{eq:eq:pca1}. Thus, these loadings represent coefficients in which
it illustrates each variables \textbf{\emph{influence}} on the principal
component. By default, loadings (aka eigenvectors) in R point in the
\emph{negative} direction. For this example, we'd prefer them to point
in the positive direction because it leads to more logical insights. To
use the positive-pointing vector, we multiply the default loadings by
-1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert loadings to positive}
\NormalTok{pca_result}\OperatorTok{$}\NormalTok{rotation <-}\StringTok{ }\OperatorTok{-}\NormalTok{pca_result}\OperatorTok{$}\NormalTok{rotation}

\CommentTok{# look at the first 5 principal component loadings and the first 5 rows}
\NormalTok{pca_result}\OperatorTok{$}\NormalTok{rotation[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{##                    PC1      PC2      PC3      PC4}
\NormalTok{## Lot_Frontage   0.09618 -0.01628  0.21698 -0.10323}
\NormalTok{## Lot_Area       0.13045  0.03500  0.25828  0.13446}
\NormalTok{## Year_Built     0.24156  0.21729 -0.32987 -0.12749}
\NormalTok{## Year_Remod_Add 0.22212  0.10844 -0.30413 -0.13136}
\NormalTok{## Mas_Vnr_Area   0.21338  0.07667  0.02667  0.02696}
\NormalTok{##                     PC5}
\NormalTok{## Lot_Frontage    0.01746}
\NormalTok{## Lot_Area       -0.01150}
\NormalTok{## Year_Built     -0.07034}
\NormalTok{## Year_Remod_Add -0.07856}
\NormalTok{## Mas_Vnr_Area    0.16102}
\end{Highlighting}
\end{Shaded}

We can visualize the level of contribution (relative size of the
loadings) each variable has on principal components 1 (left) and 2
(right). From the results below we can see that the first principal
component (PC1) roughly corresponds to the main living space and the
garage. The second component (PC2) is also affected by living space but
appears to consist of several secondary living areas (i.e.~second floor,
basement).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{fviz_contrib}\NormalTok{(pca_result, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{1}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{fviz_contrib}\NormalTok{(pca_result, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{2}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pca-contribution-plot-1} 

}

\caption{Level of contribution each variable has on principal components 1 (left) and 2 (right).}\label{fig:pca-contribution-plot}
\end{figure}

We can also obtain the principal components \emph{scores} from our
results as these are stored in the \emph{x} list item of our results.
However, we also want to make a sign adjustment to our scores to point
them in the positive direction.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sign adjustment}
\NormalTok{pca_result}\OperatorTok{$}\NormalTok{x <-}\StringTok{ }\OperatorTok{-}\NormalTok{pca_result}\OperatorTok{$}\NormalTok{x}

\CommentTok{# look at scores for the first five observations for PC1 and PC2}
\NormalTok{pca_result}\OperatorTok{$}\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{##          PC1     PC2}
\NormalTok{## [1,]  1.1839  1.4308}
\NormalTok{## [2,] -2.2839  0.8561}
\NormalTok{## [3,] -0.2968  0.6984}
\NormalTok{## [4,]  2.7066  0.8782}
\NormalTok{## [5,]  0.9716 -0.3285}
\end{Highlighting}
\end{Shaded}

The principal components \emph{scores} simply places a standardized
score for each observation for each principal component. Thus, above we
see that the first observation has a score of 1.18 for PC1. This just
states that based on this houses attributes (at least for the numeric
variables we are assessing), this house is about 1 standard deviation
above the average value for PC1 across all the homes. Since PC1 appears
to represent \emph{main living space} and \emph{garage space} it appears
that this house is about 1 standard deviation more than the average of
these attributes compared to all other homes.

\BeginKnitrBlock{note}
We can also visualize the contribution of each observation on a
particular PC with \texttt{fviz\_contrib} by changing
\texttt{choice\ =\ "ind"}. This basically takes the absoluate values of
the \emph{scores} and plots the percent of total \emph{scores} for each
state. However, this is only useful when we are dealing with a small
amount of observations (50 or less).
\EndKnitrBlock{note}

\hypertarget{selecting-the-number-of-principal-components}{%
\subsection{Selecting the Number of Principal
Components}\label{selecting-the-number-of-principal-components}}

So far we have computed principal component attributes and gained a
little understanding of what the results initially tell us. However, a
primary goal is to use PCA for data reduction. In essence, we want to
come out of PCA with less components than variables and with these
components telling us as much variation as possible about our data. But
how do we decide how many principal components to keep? Do we keep the
first four principal components or the first 16?

There are three primary approaches in helping to make this decision:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eigenvalue criterion
\item
  Proportion of variance explained criterion
\item
  Scree plot criterion
\end{enumerate}

\hypertarget{eigenvalue-criterion}{%
\subsubsection{Eigenvalue criterion}\label{eigenvalue-criterion}}

The sum of the eigenvalues is equal to the number of variables entered
into the PCA; however, the eigenvalues will range from greater than one
to near zero. An eigenvalue of 1 means that the principal component
would explain about one variable's worth of the variability. The
rationale for using the eigenvalue criterion is that each component
should explain at least one variable's worth of the variability, and
therefore, the eigenvalue criterion states that only components with
eigenvalues greater than 1 should be retained.

\texttt{prcomp} automatically computes the standard deviations of the
principal components, which is equal to the square roots of the
eigenvalues, and stores these values in the \texttt{pca\_result\$sdev}
list item. Therefore, we can compute the eigenvalues easily and identify
principal components where the sum of eigenvalues is greater than or
equal to 1. Consequently, using this criteria would have us retain the
first 11 principal components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute eigenvalues}
\NormalTok{eigen <-}\StringTok{ }\NormalTok{pca_result}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}

\CommentTok{# sum of all eigenvalues equals number of variables}
\KeywordTok{sum}\NormalTok{(eigen)}
\NormalTok{## [1] 34}

\CommentTok{# find all PCs where the sum of eigenvalues is greater than or equal to 1}
\KeywordTok{which}\NormalTok{(eigen }\OperatorTok{>=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{##  [1]  1  2  3  4  5  6  7  8  9 10 11}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/eigen-criterion-plot-1} 

}

\caption{Eigenvalue criterion keeps all principal components where the sum of the eigenvalues are above or equal to a value of one.}\label{fig:eigen-criterion-plot}
\end{figure}

\hypertarget{proportion-of-variance-explained-criterion}{%
\subsubsection{Proportion of variance explained
criterion}\label{proportion-of-variance-explained-criterion}}

The \emph{proportion of variance explained} (PVE) provides us a
technical way to identify the optimal number of principal components to
keep based on the total variability that we would like to account for.
Mathematically, the PVE for the \emph{m}th principal component is
calculated as:

\[PVE = \frac{{\sum_{i=1}^{n}(\sum_{j=1}^{p}{\phi_{jm}x_{ij}})^{2}}}{\sum_{j=1}^{p}\sum_{i=1}^{n}{x_{ij}^{2}}} \tag{3}\]

It can be shown that the PVE of the \emph{m}th principal component can
be more simply calculated by taking the \emph{m}th eigenvalue and
dividing it by the number of principal components (or, equivalently, the
sum of the eigenvalues). We can create a vector of PVEs for each
principal component:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the PVE of each principal component}
\NormalTok{PVE <-}\StringTok{ }\NormalTok{eigen }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(eigen)}

\KeywordTok{round}\NormalTok{(PVE, }\DecValTok{2}\NormalTok{)}
\NormalTok{##  [1] 0.18 0.09 0.06 0.06 0.04 0.04 0.03 0.03 0.03 0.03}
\NormalTok{## [11] 0.03 0.03 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.02}
\NormalTok{## [21] 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01}
\NormalTok{## [31] 0.00 0.00 0.00 0.00}
\end{Highlighting}
\end{Shaded}

The first principal component in our example therefore explains 18\% of
the variability, and the second principal component explains 9\%.
Together, the first two principal components explain 27\% of the
variability.

Thus, if an analyst desires to choose the number of principal components
that explains at least 75\% of the variability in our original data then
they would choose the first 16 components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many PCs required to explain at least 75% of total variability}
\KeywordTok{min}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{cumsum}\NormalTok{(PVE) }\OperatorTok{>=}\StringTok{ }\FloatTok{.75}\NormalTok{))}
\NormalTok{## [1] 16}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pve-criterion-plot-1} 

}

\caption{PVE criterion keeps all principal components that are above or equal to a pre-specified threshold of total variability explained.}\label{fig:pve-criterion-plot}
\end{figure}

What amount of variability is reasonable? This varies from the problem
being addressed and the data being used. However, when the principal
components are being used for descriptive purposes only, such as
customer profiling, then the proportion of variability explained may be
lower than otherwise. When the principal components are to be used as
replacements for the original variables, and used for further inference
in models downstream, then the PVE should be as much as can conveniently
be achieved, given any constraints.

\hypertarget{scree-plot-criterion}{%
\subsubsection{Scree plot criterion}\label{scree-plot-criterion}}

A scree plot shows the eigenvalues or PVE for each individual principal
component. Most scree plots look broadly similiar in shape, starting
high on the left, falling rather quickly, and then flattening out at
some point. This is because the first component usually explains much of
the variability, the next few components explain a moderate amount, and
the latter components only explain a small amount of the variability.
The scree plot criterion selects all components just before the line
flattens out, which is four in our example.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_screeplot}\NormalTok{(pca_result, }\DataTypeTok{ncp =} \DecValTok{34}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pca-scree-plot-criterion-1} 

}

\caption{Scree plot criterion keeps all principal components before the line flattens out.}\label{fig:pca-scree-plot-criterion}
\end{figure}

So how many principal components should we use in this example? The
frank answer is that there is no single method for determining how many
components to use. In this case, differing criteria suggest to retain 4,
11, and 16 (based on a 75\% requirement) components. The number you go
with depends on your end objective and analytic workflow. If I were
merely trying to profile houses I would probably use 4, if I were
performing dimension reduction to feed into a downstream predictive
model I would likely retain 11 or 16.

\hypertarget{extracting-additional-insights}{%
\subsection{Extracting additional
insights}\label{extracting-additional-insights}}

As previously identified, 27\% of the variation in our data can be
captured in the first two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  PC1 roughly corresponds to the main living space and the garage.
\item
  PC2 is also affected by living space but appears to consist of several
  secondary living areas (i.e.~second floor, basement).
\end{enumerate}

We can visualize this further with the following multivariate plot. This
plot provides the directional influence each variable has on the
principal components. The center point represents no influence on PC1
(x-axis) or PC2 (y-axis). Variables that are darker and further to the
right of the center vertical line have a strong position influence on
PC1 (i.e. \texttt{Gr\_Liv\_Area}, \texttt{TotRms\_Abv\_Grd}). Like-wise,
variables that are lighter and closer to the center horizontal line have
a small influence on PC2 (i.e. \texttt{Longitude}, \texttt{Porch\_SF}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_var}\NormalTok{(pca_result, }\DataTypeTok{alpha.var =} \StringTok{"contrib"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pca-var-contribution-1} 

}

\caption{Variable contributions to the first and second principal components.}\label{fig:pca-var-contribution}
\end{figure}

\BeginKnitrBlock{note}
Check out the \texttt{axes} argument (\texttt{?fviz\_pca\_var}) to
compare different principal components in a pairwise fashion.
\EndKnitrBlock{note}

Furthermore, we can see where each observation aligns along these
components. This allows us to identify observations that have high
values for one or more attributes that influence PC1 and PC2. For
example, observations 2181 and 1499 likely have higher values for main
living space and garage attributes. Whereas observation 2195 likely has
lower values of secondary living space (likely a single story home or
does not have a basement).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(pca_result, }\DataTypeTok{alpha.ind =} \FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pca-ind-contribution-1} 

}

\caption{Individual household observations along the first and second principal components.}\label{fig:pca-ind-contribution}
\end{figure}

\hypertarget{pca-with-mixed-data}{%
\subsection{PCA with mixed data}\label{pca-with-mixed-data}}

Typical textbook examples of PCA include only numeric data as
demonstrated above. However, most real life data sets contain a mixture
of numeric and categorical variables. The original Ames housing data set
contains 35 numeric variables and 46 categorical variables.
Consequently, only focusing on the numeric variables required us to
remove over half of our features. Rather, than remove this (likely
important) information, we can retain it and still perform PCA. However,
the approach we apply differs depending on if we are merely seeking
inference on housing attributes or if we plan to use the PCA output for
downstream modeling.

\hypertarget{pca-for-inference}{%
\subsubsection{PCA for inference}\label{pca-for-inference}}

When performing data mining where the principal components are being
used for descriptive purposes only, such as customer profiling, we can
convert our categorical variables to numeric information. First, any
ordinal variables can be numerically coded in an ordinal fashion. For
example, the \texttt{Overall\_Qual} variable measures the overall
quality of a home across 10 levels (very poor to very excellent). We can
recode these variables numerically from 1-10 which now puts them on a
continuous dimension.

Nominal categorical variables; however, do not contain any natural
ordering. One alternative is to one-hot encode these variables to
convert them to binary 0/1 values. This significantly expands the number
of variables in our data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# full ames data set --> recode ordinal variables to numeric}
\NormalTok{ames_full <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(}\KeywordTok{names}\NormalTok{(.), }\StringTok{"Qual|Cond|QC|Qu"}\NormalTok{), as.numeric)}

\CommentTok{# one-hot encode --> retain only the features and not sale price}
\NormalTok{full_rank  <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{dummyVars}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_full, }\DataTypeTok{fullRank =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{ames_1hot <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(full_rank, ames_full)}

\CommentTok{# new dimensions}
\KeywordTok{dim}\NormalTok{(ames_1hot)}
\NormalTok{## [1] 2930  240}
\end{Highlighting}
\end{Shaded}

Now that all our variables are represented numerically, we can perform
PCA as we did in the previous sections. Using the scree plot criterion
suggests to retain eight principal components, which explains 50\% of
the variability across all 240 variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply PCA to one-hot encoded data}
\NormalTok{pca_one_hot <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(ames_1hot, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# sign adjustment to loadings and scores}
\NormalTok{pca_one_hot}\OperatorTok{$}\NormalTok{rotation <-}\StringTok{ }\OperatorTok{-}\NormalTok{pca_one_hot}\OperatorTok{$}\NormalTok{rotation}
\NormalTok{pca_one_hot}\OperatorTok{$}\NormalTok{x <-}\StringTok{ }\OperatorTok{-}\NormalTok{pca_one_hot}\OperatorTok{$}\NormalTok{x}

\CommentTok{# scree plot}
\KeywordTok{fviz_screeplot}\NormalTok{(pca_result, }\DataTypeTok{ncp =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pca-apply-one-hot-1} 

}

\caption{Scree plot showing the amount of variance explained for each of the first 20 principal components.}\label{fig:pca-apply-one-hot}
\end{figure}

\hypertarget{pca-for-downstream-modeling}{%
\subsubsection{PCA for downstream
modeling}\label{pca-for-downstream-modeling}}

When the principal components are to be used as replacements for the
original variables, and used for further inference in models downstream,
then we want to be a little more particular about how we change the
data. Many models (i.e.~tree-based) perform quite well when the
categorical variables are untransformed. Consequently, often our motives
to perform PCA is to reduce the dimension of numerical variables and
minimize multicollinearity so that we can apply models that are
sensitive to multicollinearity (i.e.~linear regression models, neural
networks).

The \texttt{caret} package provides a function that allows you to
perform many preprocessing steps to a set of features. In the following
example, I center, scale, and apply PCA to the Ames data. The output of
\texttt{preProcess} lists the number of variables centered, scaled, and
PCA applied to (34 variables). These represent the 34 numeric variables.
It also states that 46 categorical variables were ignored since these
preprocessing steps cannot be applied to non-numeric variables. Lastly,
it states that 26 principal components were retained to capture the
variability explained threshold specified (95\%).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get feature set}
\NormalTok{ames_full <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}
\NormalTok{features <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(ames_full, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Sale_Price)}

\CommentTok{# preprocess data}
\NormalTok{preprocess <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{preProcess}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ features,}
  \DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"pca"}\NormalTok{),}
  \DataTypeTok{thresh =} \FloatTok{0.95}
\NormalTok{)}

\NormalTok{preprocess}
\NormalTok{## Created from 2930 samples and 80 variables}
\NormalTok{## }
\NormalTok{## Pre-processing:}
\NormalTok{##   - centered (34)}
\NormalTok{##   - ignored (46)}
\NormalTok{##   - principal component signal extraction (34)}
\NormalTok{##   - scaled (34)}
\NormalTok{## }
\NormalTok{## PCA needed 26 components to capture 95 percent of the variance}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{note}
You can adjust the \texttt{thresh} argument or also use \texttt{numComp}
to explicitly specify the number of varibales to retain.
\EndKnitrBlock{note}

Next, we apply the \texttt{preProcess} object to the training data to
return a transformed feature set with the specified preprocessing steps.
This new \texttt{transformed\_features} data frame contains the original
46 categorical variables and the 26 principal components retained (72
total variables). This preprocessed data can now be fed into any future
models, which you will learn about in the predictive analytics section.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create transformed feature set}
\NormalTok{transformed_features <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(preprocess, features)}
\KeywordTok{dim}\NormalTok{(transformed_features)}
\NormalTok{## [1] 2930   72}
\end{Highlighting}
\end{Shaded}

\hypertarget{cluster-analysis}{%
\section{Cluster Analysis}\label{cluster-analysis}}

Clustering is a broad set of techniques for \textbf{\emph{finding
subgroups of observations}} within a data set. When we cluster
observations, we want observations in the same group to be similar and
observations in different groups to be dissimilar. Because there isn't a
response variable, this is an unsupervised method, which implies that it
seeks to find relationships between the \emph{n} observations without
being trained by a response variable. Clustering allows us to identify
which observations are alike, and potentially categorize them therein.
Within the cluster analysis domain, there are several clustering
techniques we can apply - we will focus on the more common applications.

\hypertarget{clustering-distance-measures}{%
\subsection{Clustering distance
measures}\label{clustering-distance-measures}}

The classification of observations into groups requires some methods for
computing the distance of the (dis)similarity between each pair of
observations. The result of this computation is known as a dissimilarity
or distance matrix.

There are many methods to calculate this distance information; the
choice of distance measures is a critical step in clustering. It defines
how the similarity of two elements (x, y) is calculated and it will
influence the shape of the clusters.

The classical methods for distance measures are \emph{Euclidean} and
\emph{Manhattan distances}, which are defined as follow:

\textbf{Euclidean distance:}

\[ d_{euc}(x,y) = \sqrt{\sum^n_{i=1}(x_i - y_i)^2} \tag{1}\]

\textbf{Manhattan distance:}

\[ d_{man}(x,y) = \sum^n_{i=1}|(x_i - y_i)| \tag{2}\]

Where, \emph{x} and \emph{y} are two vectors of length \emph{n}.

Other dissimilarity measures exist such as correlation-based distances,
which is widely used for gene expression data analyses.
Correlation-based distance is defined by subtracting the correlation
coefficient from 1. Different types of correlation methods can be used
such as:

\textbf{Pearson correlation distance:}

\[d_{cor}(x, y) = 1 - \frac{\sum^n_{i=1}(x_i-\bar x)(y_i - \bar y)}{\sqrt{\sum^n_{i=1}(x_i-\bar x)^2\sum^n_{i=1}(y_i - \bar y)^2}} \tag{3}\]

\textbf{Spearman correlation distance:}

The spearman correlation method computes the correlation between the
rank of \emph{x} and the rank of \emph{y} variables.

\[d_{spear}(x, y) = 1 - \frac{\sum^n_{i=1}(x^\prime_i-\bar x^\prime)(y^\prime_i - \bar y^\prime)}{\sqrt{\sum^n_{i=1}(x^\prime_i-\bar x^\prime)^2\sum^n_{i=1}(y^\prime_i - \bar y^\prime)^2}} \tag{4}\]

Where \(x^\prime_i = rank(x_i)\) and \(y^\prime_i = rank(y_i)\).

\textbf{Kendall correlation distance:}

Kendall correlation method measures the correspondence between the
ranking of \emph{x} and \emph{y} variables. The total number of possible
pairings of \emph{x} with \emph{y} observations is \emph{n(n − 1)/2},
where \emph{n} is the size of \emph{x} and \emph{y}. Begin by ordering
the pairs by the \emph{x} values. If \emph{x} and \emph{y} are
correlated, then they would have the same relative rank orders. Now, for
each \(y_i\), count the number of \(y_j > y_i\) (concordant pairs (c))
and the number of \(y_j < y_i\) (discordant pairs (d)).

Kendall correlation distance is defined as follow:

\[d_{kend}(x,y) = 1 - \frac{n_c - n_d}{\frac{1}{2}n(n - 1)} \tag{5}\]

The choice of distance measures is very important, as it has a strong
influence on the clustering results. For most common clustering
software, the default distance measure is the Euclidean distance.
However, depending on the type of the data and the research questions,
other dissimilarity measures might be preferred and you should be aware
of the options.

Within R it is simple to compute and visualize the distance matrix using
the functions \texttt{get\_dist} and \texttt{fviz\_dist} from the
\texttt{factoextra} R package. The following figure plots the Euclidean
distances between the first 50 homes. This starts to illustrate which
observations have large dissimilarities (red) versus those that appear
to be fairly similar (blue).

\begin{itemize}
\tightlist
\item
  \texttt{get\_dist}: for computing a distance matrix between the rows
  of a data matrix. The default distance computed is the Euclidean;
  however, \texttt{get\_dist} also supports distanced described in
  equations 2-5 above plus others.
\item
  \texttt{fviz\_dist}: for visualizing a distance matrix
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we continue using our scaled ames data set}
\NormalTok{distance <-}\StringTok{ }\KeywordTok{get_dist}\NormalTok{(ames_scale[}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{, ], }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\KeywordTok{fviz_dist}\NormalTok{(}
\NormalTok{  distance,}
  \DataTypeTok{gradient =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{low =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{mid =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/distance-1} 

}

\caption{Distance matrix plot for first 50 observations.}\label{fig:distance}
\end{figure}

\BeginKnitrBlock{tip}
Check out the different distance measures that \texttt{get\_dist}
accepts with \texttt{?get\_dist}. Use \texttt{method\ =\ "pearson"} and
see how the distance matrix visualization changes.
\EndKnitrBlock{tip}

\hypertarget{k-means-clustering}{%
\subsection{K-means clustering}\label{k-means-clustering}}

K-means clustering is the most commonly used clustering algorithm for
partitioning observations into a set of \emph{k} groups (i.e. \emph{k}
clusters), where \emph{k} represents the number of groups pre-specified
by the analyst. It classifies objects in multiple groups (i.e.,
clusters), such that objects within the same cluster are as similar as
possible (i.e., high intra-class similarity), whereas objects from
different clusters are as dissimilar as possible (i.e., low inter-class
similarity). In k-means clustering, each cluster is represented by its
center (i.e, centroid) which corresponds to the mean of points assigned
to the cluster.

\hypertarget{defining-clusters}{%
\subsubsection{Defining clusters}\label{defining-clusters}}

The basic idea behind k-means clustering consists of defining clusters
so that the total intra-cluster variation (known as total within-cluster
variation) is minimized. There are several k-means algorithms available.
The standard algorithm is the Hartigan-Wong algorithm (1979), which
defines the total within-cluster variation as the sum of squared
distances Euclidean distances between items and the corresponding
centroid:

\[W(C_k) = \sum_{x_i \in C_k}(x_i - \mu_k)^2 \tag{6}\]

where:

\begin{itemize}
\tightlist
\item
  \(x_i\) is a data point belonging to the cluster \(C_k\)
\item
  \(\mu_k\) is the mean value of the points assigned to the cluster
  \(C_k\)
\end{itemize}

Each observation (\(x_i\)) is assigned to a given cluster such that the
sum of squares (SS) distance of the observation to their assigned
cluster centers (\(\mu_k\)) is minimized.

We define the total within-cluster variation as follows:

\[tot.withiness = \sum^k_{k=1}W(C_k) = \sum^k_{k=1}\sum_{x_i \in C_k}(x_i - \mu_k)^2 \tag{7} \]

The \emph{total within-cluster sum of square} measures the compactness
(i.e goodness) of the clustering and we want it to be as small as
possible.

\hypertarget{k-means-algorithm}{%
\subsubsection{K-means algorithm}\label{k-means-algorithm}}

The first step when using k-means clustering is to indicate the number
of clusters (k) that will be generated in the final solution. The
algorithm starts by randomly selecting k objects from the data set to
serve as the initial centers for the clusters. The selected objects are
also known as cluster means or centroids. Next, each of the remaining
objects is assigned to it's closest centroid, where closest is defined
using the Euclidean distance between the object and the cluster mean.
This step is called ``cluster assignment step''. After the assignment
step, the algorithm computes the new mean value of each cluster. The
term cluster ``centroid update'' is used to design this step. Now that
the centers have been recalculated, every observation is checked again
to see if it might be closer to a different cluster. All the objects are
reassigned again using the updated cluster means. The cluster assignment
and centroid update steps are iteratively repeated until the cluster
assignments stop changing (i.e until \emph{convergence} is achieved).
That is, the clusters formed in the current iteration are the same as
those obtained in the previous iteration.

K-means algorithm can be summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the number of clusters (K) to be created (by the analyst)
\item
  Select randomly k objects from the data set as the initial cluster
  centers or means
\item
  Assigns each observation to their closest centroid, based on the
  Euclidean distance between the object and the centroid
\item
  For each of the k clusters update the cluster centroid by calculating
  the new mean values of all the data points in the cluster. The
  centroid of a Kth cluster is a vector of length \emph{p} containing
  the means of all variables for the observations in the kth cluster;
  \emph{p} is the number of variables.
\item
  Iteratively minimize the total within sum of square. That is, iterate
  steps 3 and 4 until the cluster assignments stop changing or the
  maximum number of iterations is reached. By default, the R software
  uses 10 as the default value for the maximum number of iterations.
\end{enumerate}

\hypertarget{computing-k-means-clustering-in-r}{%
\subsubsection{Computing k-means clustering in
R}\label{computing-k-means-clustering-in-r}}

We can compute k-means in R with the \texttt{kmeans} function. Here will
group the data into two clusters (\texttt{centers\ =\ 2}). The
\texttt{kmeans} function also has an \texttt{nstart} option that
attempts multiple initial configurations and reports on the best one.
For example, adding \texttt{nstart\ =\ 25} will generate 25 initial
configurations. This approach is often recommended.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k2 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(ames_scale, }\DataTypeTok{centers =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output of \texttt{kmeans} is a list with several bits of
information. The most important being:

\begin{itemize}
\tightlist
\item
  \texttt{cluster}: A vector of integers (from 1:k) indicating the
  cluster to which each point is allocated.
\item
  \texttt{centers}: A matrix of cluster centers.
\item
  \texttt{totss}: The total sum of squares.
\item
  \texttt{withinss}: Vector of within-cluster sum of squares, one
  component per cluster.
\item
  \texttt{tot.withinss}: Total within-cluster sum of squares,
  i.e.~sum(withinss).
\item
  \texttt{betweenss}: The between-cluster sum of squares, i.e.
  \(totss-tot.withinss\).
\item
  \texttt{size}: The number of points in each cluster.
\end{itemize}

If we print the results we'll see that our groupings resulted in 2
cluster sizes of 1397 and 1533. We can also extract the cluster centers
for each variable and the cluster assignment for each observation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(k2)}
\NormalTok{## List of 9}
\NormalTok{##  $ cluster     : int [1:2930] 1 2 2 1 1 1 1 1 1 1 ...}
\NormalTok{##  $ centers     : num [1:2, 1:34] 0.118 -0.108 0.201 -0.183 0.665 ...}
\NormalTok{##   ..- attr(*, "dimnames")=List of 2}
\NormalTok{##   .. ..$ : chr [1:2] "1" "2"}
\NormalTok{##   .. ..$ : chr [1:34] "Lot_Frontage" "Lot_Area" "Year_Built" "Year_Remod_Add" ...}
\NormalTok{##  $ totss       : num 99586}
\NormalTok{##  $ withinss    : num [1:2] 47995 39746}
\NormalTok{##  $ tot.withinss: num 87741}
\NormalTok{##  $ betweenss   : num 11845}
\NormalTok{##  $ size        : int [1:2] 1397 1533}
\NormalTok{##  $ iter        : int 1}
\NormalTok{##  $ ifault      : int 0}
\NormalTok{##  - attr(*, "class")= chr "kmeans"}
\end{Highlighting}
\end{Shaded}

We can also view our results by using \texttt{fviz\_cluster}. This
provides a nice illustration of the clusters. If there are more than two
dimensions (variables) \texttt{fviz\_cluster} will perform principal
component analysis (PCA) and plot the data points according to the first
two principal components that explain the largest amount of variance. So
this chart shows that our observations are being clustered primarily
based on having above or below average values of dimension 2 (recall
from the PCA section that the second principal component - or x axis -
largely represented \emph{secondary living spaces}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_cluster}\NormalTok{(k2, }\DataTypeTok{data =}\NormalTok{ ames_scale, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/k2-viz-1} \end{center}

\BeginKnitrBlock{tip}
You can also visualize clusters against specific variables by assigning
\texttt{choose.vars} within \texttt{fviz\_cluster}.
\EndKnitrBlock{tip}

Because the number of clusters (k) must be set before we start the
algorithm, it is often advantageous to use several different values of k
and examine the differences in the results. We can execute the same
process for 3, 4, and 5 clusters, and the results are shown in the
figure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k3 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(ames_scale, }\DataTypeTok{centers =} \DecValTok{3}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}
\NormalTok{k4 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(ames_scale, }\DataTypeTok{centers =} \DecValTok{4}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}
\NormalTok{k5 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(ames_scale, }\DataTypeTok{centers =} \DecValTok{5}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}

\CommentTok{# plots to compare}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{fviz_cluster}\NormalTok{(k2, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ ames_scale, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"k = 2"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{fviz_cluster}\NormalTok{(k3, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{,  }\DataTypeTok{data =}\NormalTok{ ames_scale, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"k = 3"}\NormalTok{)}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{fviz_cluster}\NormalTok{(k4, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{,  }\DataTypeTok{data =}\NormalTok{ ames_scale, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"k = 4"}\NormalTok{)}
\NormalTok{p4 <-}\StringTok{ }\KeywordTok{fviz_cluster}\NormalTok{(k5, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{,  }\DataTypeTok{data =}\NormalTok{ ames_scale, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"k = 5"}\NormalTok{)}

\KeywordTok{library}\NormalTok{(gridExtra)}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/kmeans-which-k-1} \end{center}

Although visually assessing the different k cluster outputs tells us
where true dilineations occur (or do not occur) between clusters, it
does not tell us what the optimal number of clusters is.

\hypertarget{determining-optimal-clusters}{%
\subsubsection{Determining optimal
clusters}\label{determining-optimal-clusters}}

As you may recall the analyst specifies the number of clusters to use;
preferably the analyst would like to use the optimal number of clusters.
To aid the analyst, the following explains the three most popular
methods for determining the optimal clusters, which includes:

\begin{itemize}
\tightlist
\item
  Elbow method
\item
  Silhouette method
\item
  Gap statistic
\end{itemize}

\hypertarget{elbow}{%
\paragraph{Elbow method}\label{elbow}}

Recall that, the basic idea behind cluster partitioning methods, such as
k-means clustering, is to define clusters such that the total
within-cluster variation is minimized:

\[ minimize\Bigg(\sum^k_{k=1}W(C_k)\Bigg) \tag{8}\]

where \(C_k\) is the \(k^{th}\) cluster and \(W(C_k)\) is the
within-cluster variation. The total within-cluster sum of square (wss)
measures the compactness of the clustering and we want it to be as small
as possible. Thus, we can use the following algorithm to define the
optimal clusters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute clustering algorithm (e.g., k-means clustering) for different
  values of \emph{k}. For instance, by varying \emph{k} from 1 to 20
  clusters.
\item
  For each \emph{k}, calculate the total within-cluster sum of square
  (wss).
\item
  Plot the curve of wss according to the number of clusters \emph{k}.
\item
  The location of a bend (knee) in the plot is generally considered as
  an indicator of the appropriate number of clusters.
\end{enumerate}

Fortunately, this process to compute the ``Elbow method'' has been
wrapped up in a single function (\texttt{fviz\_nbclust}). Unfortunately,
the results are unclear where the elbow actually is? Do we select 3
clusters? 7 clusters? 18?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\KeywordTok{fviz_nbclust}\NormalTok{(ames_scale, kmeans, }\DataTypeTok{method =} \StringTok{"wss"}\NormalTok{, }\DataTypeTok{k.max =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/kmeans-elbow-method2-1} \end{center}

\hypertarget{silo}{%
\paragraph{Average silhouette method}\label{silo}}

In short, the average silhouette approach measures the quality of a
clustering. That is, it determines how well each object lies within its
cluster. A high average silhouette width indicates a good clustering.
The average silhouette method computes the average silhouette of
observations for different values of \emph{k}. The optimal number of
clusters \emph{k} is the one that maximizes the average silhouette over
a range of possible values for \emph{k}.\footnote{\href{http://onlinelibrary.wiley.com/book/10.1002/9780470316801}{Kaufman
  and Rousseeuw, 1990}}

Similar to the elbow method, this process to compute the ``average
silhoutte method'' has been wrapped up in a single function
(\texttt{fviz\_nbclust}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\KeywordTok{fviz_nbclust}\NormalTok{(ames_scale, kmeans, }\DataTypeTok{method =} \StringTok{"silhouette"}\NormalTok{, }\DataTypeTok{k.max =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/kmeans-silhouette-1} \end{center}

\hypertarget{gap}{%
\subsubsection{Gap statistic method}\label{gap}}

The gap statistic has been published by
\href{http://web.stanford.edu/~hastie/Papers/gap.pdf}{R. Tibshirani, G.
Walther, and T. Hastie (Standford University, 2001)}. The approach can
be applied to any clustering method (i.e.~K-means clustering,
hierarchical clustering). The gap statistic compares the total
intracluster variation for different values of \emph{k} with their
expected values under null reference distribution of the data (i.e.~a
distribution with no obvious clustering). The reference dataset is
generated using Monte Carlo simulations of the sampling process. That
is, for each variable (\(x_i\)) in the data set we compute its range
\([min(x_i), max(x_j)]\) and generate values for the n points uniformly
from the interval min to max.

For the observed data and the the reference data, the total intracluster
variation is computed using different values of \emph{k}. The \emph{gap
statistic} for a given \emph{k} is defined as follow:

\[ Gap_n(k) = E^*_n{log(W_k)} - log(W_k) \tag{9}\]

Where \(E^*_n\) denotes the expectation under a sample size \emph{n}
from the reference distribution. \(E^*_n\) is defined via bootstrapping
(B) by generating B copies of the reference datasets and, by computing
the average \(log(W^*_k)\). The gap statistic measures the deviation of
the observed \(W_k\) value from its expected value under the null
hypothesis. The estimate of the optimal clusters (\(\hat k\)) will be
the value that maximizes \(Gap_n(k)\). This means that the clustering
structure is far away from the uniform distribution of points.

In short, the algorithm involves the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cluster the observed data, varying the number of clusters from
  \(k=1, \dots, k_{max}\), and compute the corresponding \(W_k\).
\item
  Generate B reference data sets and cluster each of them with varying
  number of clusters \(k=1, \dots, k_{max}\). Compute the estimated gap
  statistics presented in eq. 9.
\item
  Let \(\bar w = (1/B) \sum_b log(W^*_{kb})\), compute the standard
  deviation \(sd(k) = \sqrt{(1/b)\sum_b(log(W^*_{kb})- \bar w)^2}\) and
  define \(s_k = sd_k \times \sqrt{1 + 1/B}\).
\item
  Choose the number of clusters as the smallest k such that
  \(Gap(k) \geq Gap(k+1) - s_{k+1}\).
\end{enumerate}

We can visualize the results with \texttt{fviz\_gap\_stat} which
suggests four clusters as the optimal number of clusters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\KeywordTok{fviz_nbclust}\NormalTok{(ames_scale, kmeans, }\DataTypeTok{method =} \StringTok{"gap_stat"}\NormalTok{, }\DataTypeTok{k.max =} \DecValTok{20}\NormalTok{, }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/kmeans-gap-1} \end{center}

\BeginKnitrBlock{comment}
It is often the case, as above, where each metric suggests a different
number of preferred clusters. This is part of the challege of clustering
and, often, the decision about the number of clusters is partly driven
by qualitative assessment and domain knowledge.
\EndKnitrBlock{comment}

In addition to these commonly used approaches, the \texttt{NbClust}
package, published by
\href{http://www.jstatsoft.org/v61/i06/paper}{Charrad et al., 2014},
provides 30 indices for determining the relevant number of clusters and
proposes to users the best clustering scheme from the different results
obtained by varying all combinations of number of clusters, distance
measures, and clustering methods.

\hypertarget{extracting-results}{%
\subsubsection{Extracting results}\label{extracting-results}}

Once you've identified the preferred number of clusters, we rerun the
analysis with the selected \emph{k} (8 in this example). We can extract
the clusters and add to our initial data to do some descriptive
statistics at the cluster level. Here, we can see that homes assigned to
cluster 2 tend to be older, one story homes (or with a minimal second
floor) whereas homes in cluster 7 tend to be newer (relatively
speaking), two story homes with lots of space.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# re-run kmeans}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{final <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(ames_scale, }\DecValTok{8}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}

\CommentTok{# perform descriptive analysis at the cluster level}
\NormalTok{ames }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Cluster =}\NormalTok{ final}\OperatorTok{$}\NormalTok{cluster) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Cluster) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(Year_Built, Second_Flr_SF, TotRms_AbvGrd, Garage_Area), }\StringTok{"mean"}\NormalTok{)}
\NormalTok{## # A tibble: 8 x 5}
\NormalTok{##   Cluster Year_Built Second_Flr_SF TotRms_AbvGrd}
\NormalTok{##     <int>      <dbl>         <dbl>         <dbl>}
\NormalTok{## 1       1      1928.        402.            8.38}
\NormalTok{## 2       2      1969.         40.3           5.36}
\NormalTok{## 3       3      1997.          5.95          6.48}
\NormalTok{## 4       4      1939.        310.            5.84}
\NormalTok{## 5       5      1950.        441.            8.61}
\NormalTok{## 6       6      1992.        821.            7.05}
\NormalTok{## 7       7      1990.        994.            8.99}
\NormalTok{## 8       8      1971         691.            8.36}
\NormalTok{## # ... with 1 more variable: Garage_Area <dbl>}
\end{Highlighting}
\end{Shaded}

\hypertarget{additional-comments}{%
\subsubsection{Additional comments}\label{additional-comments}}

K-means clustering is a very simple and fast algorithm. Furthermore, it
can efficiently deal with very large data sets. However, there are some
weaknesses of the k-means approach.

One potential disadvantage of K-means clustering is that it requires us
to pre-specify the number of clusters. Hierarchical clustering is an
alternative approach which does not require that we commit to a
particular choice of clusters. Hierarchical clustering has an added
advantage over K-means clustering in that it results in an attractive
tree-based representation of the observations, called a dendrogram. An
additional disadvantage of K-means is that it's sensitive to outliers
and different results can occur if you change the ordering of your data.
The Partitioning Around Medoids (PAM) clustering approach is less
sensititive to outliers and provides a robust alternative to k-means to
deal with these situations. The next two sections illustrate these
clustering approaches.

\hypertarget{hierarchical-clustering}{%
\subsection{Hierarchical clustering}\label{hierarchical-clustering}}

Hierarchical clustering is an alternative approach to k-means clustering
for identifying groups in the dataset. It does not require us to
pre-specify the number of clusters to be generated as is required by the
k-means approach. Furthermore, hierarchical clustering has an added
advantage over K-means clustering in that it results in an attractive
tree-based representation of the observations, called a dendrogram.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/dendrogram} 

}

\caption{Illustrative dendrogram.}\label{fig:dendrogram}
\end{figure}

\hypertarget{hierarchical-clustering-algorithms}{%
\subsubsection{Hierarchical Clustering
Algorithms}\label{hierarchical-clustering-algorithms}}

Hierarchical clustering can be divided into two main types:
\emph{agglomerative} and \emph{divisive}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Agglomerative clustering:} Also known as AGNES (Agglomerative
  Nesting). It works in a bottom-up manner. That is, each object is
  initially considered as a single-element cluster (leaf). At each step
  of the algorithm, the two clusters that are the most similar are
  combined into a new bigger cluster (nodes). This procedure is iterated
  until all points are a member of just one single big cluster (root)
  (see figure below). The result is a tree which can be plotted as a
  dendrogram.
\item
  \textbf{Divisive hierarchical clustering:} It's also known as DIANA
  (Divise Analysis) and it works in a top-down manner. The algorithm is
  an inverse order of AGNES. It begins with the root, in which all
  objects are included in a single cluster. At each step of iteration,
  the most heterogeneous cluster is divided into two. The process is
  iterated until all objects are in their own cluster (see figure
  below).
\end{enumerate}

\BeginKnitrBlock{tip}
Note that agglomerative clustering is good at identifying small
clusters. Divisive hierarchical clustering is good at identifying large
clusters.
\EndKnitrBlock{tip}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/dendrogram2} 

}

\caption{AGNES (bottom-up) versus DIANA (top-down) clustering.}\label{fig:dendrogram2}
\end{figure}

As we learned in the last section, we measure the (dis)similarity of
observations using distance measures (i.e.~Euclidean distance, Manhattan
distance, etc.) In R, the Euclidean distance is used by default to
measure the dissimilarity between each pair of observations.

However, a bigger question is: \emph{How do we measure the dissimilarity
between two clusters of observations?} A number of different cluster
agglomeration methods (i.e, linkage methods) have been developed to
answer to this question. The most common types methods are:

\begin{itemize}
\tightlist
\item
  \textbf{Maximum or complete linkage clustering:} It computes all
  pairwise dissimilarities between the elements in cluster 1 and the
  elements in cluster 2, and considers the largest value (i.e., maximum
  value) of these dissimilarities as the distance between the two
  clusters. It tends to produce more compact clusters.
\item
  \textbf{Minimum or single linkage clustering:} It computes all
  pairwise dissimilarities between the elements in cluster 1 and the
  elements in cluster 2, and considers the smallest of these
  dissimilarities as a linkage criterion. It tends to produce long,
  ``loose'' clusters.
\item
  \textbf{Mean or average linkage clustering:} It computes all pairwise
  dissimilarities between the elements in cluster 1 and the elements in
  cluster 2, and considers the average of these dissimilarities as the
  distance between the two clusters.
\item
  \textbf{Centroid linkage clustering:} It computes the dissimilarity
  between the centroid for cluster 1 (a mean vector of length p
  variables) and the centroid for cluster 2.
\item
  \textbf{Ward's minimum variance method:} It minimizes the total
  within-cluster variance. At each step the pair of clusters with
  minimum between-cluster distance are merged.
\end{itemize}

We can see the differences these approaches in the following
dendrograms:

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/dendrogram3} 

}

\caption{Differing hierarchical clustering outputs based on similarity measures.}\label{fig:dendrogram3}
\end{figure}

\BeginKnitrBlock{comment}
The important thing to remember is there are multiple ways to define
clusters when performing hierarchical cluster analysis.
\EndKnitrBlock{comment}

\hypertarget{hierarchical-clustering-with-r}{%
\subsubsection{Hierarchical Clustering with
R}\label{hierarchical-clustering-with-r}}

There are different functions available in R for computing hierarchical
clustering. The commonly used functions are:

\begin{itemize}
\tightlist
\item
  \texttt{hclust} {[}in stats package{]} and \texttt{agnes} {[}in
  cluster package{]} for agglomerative hierarchical clustering (HC)
\item
  \texttt{diana} {[}in cluster package{]} for divisive HC
\end{itemize}

\hypertarget{agglomerative-hierarchical-clustering}{%
\paragraph{Agglomerative Hierarchical
Clustering}\label{agglomerative-hierarchical-clustering}}

We can perform agglomerative HC with \texttt{hclust}. First we compute
the dissimilarity values with \texttt{dist} and then feed these values
into \texttt{hclust} and specify the agglomeration method to be used
(i.e. ``complete'', ``average'', ``single'', ``ward.D'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# Dissimilarity matrix}
\NormalTok{d <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(ames_scale, }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{)}

\CommentTok{# Hierarchical clustering using Complete Linkage}
\NormalTok{hc1 <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(d, }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{tip}
You could plot the dendrogram with
\texttt{plot(hc1,\ cex\ =\ 0.6,\ hang\ =\ -1)}; however, due to the
number of observations the output is not discernable.
\EndKnitrBlock{tip}

Alternatively, we can use the \texttt{agnes} function. This function
behaves similar to \texttt{hclust}; however, with the \texttt{agnes}
function you can also get the agglomerative coefficient, which measures
the amount of clustering structure found (values closer to 1 suggest
strong clustering structure).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# Compute maximum or "complete linkage clustering with agnes}
\NormalTok{hc2 <-}\StringTok{ }\KeywordTok{agnes}\NormalTok{(ames_scale, }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{)}

\CommentTok{# Agglomerative coefficient}
\NormalTok{hc2}\OperatorTok{$}\NormalTok{ac}
\NormalTok{## [1] 0.9268}
\end{Highlighting}
\end{Shaded}

This allows us to find certain hierarchical clustering methods that can
identify stronger clustering structures. Here we see that Ward's method
identifies the strongest clustering structure of the four methods
assessed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# methods to assess}
\NormalTok{m <-}\StringTok{ }\KeywordTok{c}\NormalTok{( }\StringTok{"average"}\NormalTok{, }\StringTok{"single"}\NormalTok{, }\StringTok{"complete"}\NormalTok{, }\StringTok{"ward"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(m) <-}\StringTok{ }\KeywordTok{c}\NormalTok{( }\StringTok{"average"}\NormalTok{, }\StringTok{"single"}\NormalTok{, }\StringTok{"complete"}\NormalTok{, }\StringTok{"ward"}\NormalTok{)}

\CommentTok{# function to compute coefficient}
\NormalTok{ac <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{agnes}\NormalTok{(ames_scale, }\DataTypeTok{method =}\NormalTok{ x)}\OperatorTok{$}\NormalTok{ac}
\NormalTok{\}}

\CommentTok{# get agglomerative coefficient for each linkage method}
\KeywordTok{map_dbl}\NormalTok{(m, ac)}
\NormalTok{##  average   single complete     ward }
\NormalTok{##   0.9139   0.8713   0.9268   0.9767}
\end{Highlighting}
\end{Shaded}

\hypertarget{divisive-hierarchical-clustering}{%
\paragraph{Divisive Hierarchical
Clustering}\label{divisive-hierarchical-clustering}}

The R function \texttt{diana} provided by the cluster package allows us
to perform divisive hierarchical clustering. \texttt{diana} works
similar to \texttt{agnes}; however, there is no method to provide. As
before, a divisive coefficient closer to one suggests stronger group
distinctions. Consequently, it appears that an agglomerative approach
with Ward's linkage provides the optimal results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute divisive hierarchical clustering}
\NormalTok{hc4 <-}\StringTok{ }\KeywordTok{diana}\NormalTok{(ames_scale)}

\CommentTok{# Divise coefficient; amount of clustering structure found}
\NormalTok{hc4}\OperatorTok{$}\NormalTok{dc}
\NormalTok{## [1] 0.9191}
\end{Highlighting}
\end{Shaded}

\hypertarget{determining-optimal-clusters-1}{%
\subsubsection{Determining optimal
clusters}\label{determining-optimal-clusters-1}}

Similar to how we determined optimal clusters with k-means clustering,
we can execute similar approaches for hierarchical clustering:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc_ward <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(d, }\DataTypeTok{method =} \StringTok{"ward.D2"}\NormalTok{ )}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{fviz_nbclust}\NormalTok{(ames_scale, }\DataTypeTok{FUN =}\NormalTok{ hcut, }\DataTypeTok{method =} \StringTok{"wss"}\NormalTok{, }\DataTypeTok{k.max =} \DecValTok{10}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{fviz_nbclust}\NormalTok{(ames_scale, }\DataTypeTok{FUN =}\NormalTok{ hcut, }\DataTypeTok{method =} \StringTok{"silhouette"}\NormalTok{, }\DataTypeTok{k.max =} \DecValTok{10}\NormalTok{)}

\NormalTok{gap_stat <-}\StringTok{ }\KeywordTok{clusGap}\NormalTok{(ames_scale, }\DataTypeTok{FUN =}\NormalTok{ hcut, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{, }\DataTypeTok{K.max =} \DecValTok{10}\NormalTok{, }\DataTypeTok{B =} \DecValTok{20}\NormalTok{)}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{fviz_gap_stat}\NormalTok{(gap_stat)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{abar_files/figure-latex/hclust-optimal-clusters-compare-1} \end{center}

\hypertarget{working-with-dendrograms}{%
\subsubsection{Working with
Dendrograms}\label{working-with-dendrograms}}

The nice thing about hierarchical clustering is that it provides a
complete dendrogram illustrating the relationships between groupings in
our data. In the dendrogram displayed below, each leaf corresponds to
one observation (aka an individual house). As we move up the tree,
observations that are similar to each other are combined into branches,
which are themselves fused at a higher height.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc5 <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(d, }\DataTypeTok{method =} \StringTok{"ward.D2"}\NormalTok{ )}
\NormalTok{dend_plot <-}\StringTok{ }\KeywordTok{fviz_dend}\NormalTok{(hc5)}
\NormalTok{dend_data <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(dend_plot, }\StringTok{"dendrogram"}\NormalTok{)}
\NormalTok{dend_cuts <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(dend_data, }\DataTypeTok{h =} \DecValTok{8}\NormalTok{)}
\KeywordTok{fviz_dend}\NormalTok{(dend_cuts}\OperatorTok{$}\NormalTok{lower[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/illustrative_sub_dendrogram} 

}

\caption{A subsection of the dendrogram for illustrative purposes.}\label{fig:illustrative-dendrogram-plot}
\end{figure}

The height of the fusion, provided on the vertical axis, indicates the
(dis)similarity between two observations. The higher the height of the
fusion, the less similar the observations are.

\BeginKnitrBlock{warning}
Conclusions about the proximity of two observations can be drawn only
based on the height where branches containing those two observations
first are fused. We cannot use the proximity of two observations along
the horizontal axis as a criteria of their similarity.
\EndKnitrBlock{warning}

The height of the cut to the dendrogram controls the number of clusters
obtained. It plays the same role as the \emph{k} in k-means clustering.
In order to identify sub-groups (i.e.~clusters), we can cut the
dendrogram with cutree. Here, we cut our agglomerative hierarchical
clustering model into 8 clusters based on the silhouette results in the
previous section. We can see that the concentration of observations are
in clusters 1-3.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ward's method}
\NormalTok{hc5 <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(d, }\DataTypeTok{method =} \StringTok{"ward.D2"}\NormalTok{ )}

\CommentTok{# Cut tree into 4 groups}
\NormalTok{sub_grp <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(hc5, }\DataTypeTok{k =} \DecValTok{8}\NormalTok{)}

\CommentTok{# Number of members in each cluster}
\KeywordTok{table}\NormalTok{(sub_grp)}
\NormalTok{## sub_grp}
\NormalTok{##    1    2    3    4    5    6    7    8 }
\NormalTok{## 1363  567  650   36  123  156   24   11}
\end{Highlighting}
\end{Shaded}

We can plot the entire dendrogram with \texttt{fviz\_dend} and highlight
the 8 clusters with \texttt{k\ =\ 8}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot full dendogram}
\KeywordTok{fviz_dend}\NormalTok{(}
\NormalTok{  hc5,}
  \DataTypeTok{k =} \DecValTok{8}\NormalTok{,}
  \DataTypeTok{horiz =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{rect =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{rect_fill =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{rect_border =} \StringTok{"jco"}\NormalTok{,}
  \DataTypeTok{k_colors =} \StringTok{"jco"}\NormalTok{,}
  \DataTypeTok{cex =} \FloatTok{0.1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/full_dendrogram} 

}

\caption{The complete dendogram highlighting all 8 clusters.}\label{fig:working-with-dends-2-plot}
\end{figure}

However, due to the size of our data, the dendrogram is not legible.
Consequently, we may want to zoom into one particular cluster. This
allows us to see which observations are most similiar within a
particular cluster.

\BeginKnitrBlock{comment}
There is no easy way to get the exact height required to capture all 8
clusters. This is largely trial and error by using different heights
until the output of \texttt{dend\_cuts} matches the cluster totals
identified previously.
\EndKnitrBlock{comment}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dend_plot <-}\StringTok{ }\KeywordTok{fviz_dend}\NormalTok{(hc5)                  }\CommentTok{# create full dendogram}
\NormalTok{dend_data <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(dend_plot, }\StringTok{"dendrogram"}\NormalTok{)   }\CommentTok{# extract plot info}
\NormalTok{dend_cuts <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(dend_data, }\DataTypeTok{h =} \FloatTok{70.5}\NormalTok{)        }\CommentTok{# cut the dendogram at designated height}

\CommentTok{# create sub dendrogram plots}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{fviz_dend}\NormalTok{(dend_cuts}\OperatorTok{$}\NormalTok{lower[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{fviz_dend}\NormalTok{(dend_cuts}\OperatorTok{$}\NormalTok{lower[[}\DecValTok{1}\NormalTok{]])}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/cluster7_sub_dendrogram} 

}

\caption{A subsection of the dendrogram highlighting cluster 7.}\label{fig:zoom-into-dendrogram-plot}
\end{figure}

\hypertarget{clustering-with-mixed-data}{%
\subsection{Clustering with mixed
data}\label{clustering-with-mixed-data}}

As with PCA, typical textbook examples of clustering include only
numeric data as demonstrated above. However, most real life data sets
contain a mixture of numeric and categorical variables and whether an
observation is similar to another observation should depend on both
types of variables. There are a few options for performing clustering
with mixed data and we'll demonstrate on the full ames data set (minus
the response variable \texttt{Sale\_Price}). To perform k-means
clustering on mixed data we can convert any ordinal categorical
variables to numeric and one-hot encode the remaining nominal
categorical variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# full ames data set --> recode ordinal variables to numeric}
\NormalTok{ames_full <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(}\KeywordTok{names}\NormalTok{(.), }\StringTok{"Qual|Cond|QC|Qu"}\NormalTok{), as.numeric)}

\CommentTok{# one-hot encode --> retain only the features and not sale price}
\NormalTok{full_rank  <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{dummyVars}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames_full, }\DataTypeTok{fullRank =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{ames_1hot   <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(full_rank, ames_full)}

\CommentTok{# scale data}
\NormalTok{ames_1hot_scaled <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(ames_1hot)}

\CommentTok{# new dimensions}
\KeywordTok{dim}\NormalTok{(ames_1hot_scaled)}
\NormalTok{## [1] 2930  240}
\end{Highlighting}
\end{Shaded}

Now that all our variables are represented numerically, we can perform
k-means or hierarchical clustering as we did in the previous sections.
Using the Silhouette statistic criterion, it appears the optimal number
of clusters across all variables are 2.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\KeywordTok{fviz_nbclust}\NormalTok{(}
\NormalTok{  ames_1hot_scaled, }
\NormalTok{  kmeans, }
  \DataTypeTok{method =} \StringTok{"silhouette"}\NormalTok{, }
  \DataTypeTok{k.max =} \DecValTok{20}\NormalTok{, }
  \DataTypeTok{verbose =} \OtherTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/kmeans-silhouette-mixed-1} 

}

\caption{Suggested number of clusters for one-hot encoded Ames data using k-means clustering and the Silhouette criterion.}\label{fig:kmeans-silhouette-mixed}
\end{figure}

Unfortunately, as the number of features expand, performance of k-means
tends to break down and both k-means and hierarchical approaches become
slow. It starts to break down typically because your data becomes very
sparse (lots of 0s and 1s from one-hot encoding; however, standardizing
your data resolves this). Also, as you add more information you are
likely to introduce more outliers and since k-means uses the mean, it is
not robust to outliers. An alternative to this is to use
\emph{partitioning around mediods} (PAM), which has the same algorithmic
steps as k-means but uses the median rather than the mean; making it
more robust to outliers.

\BeginKnitrBlock{tip}
To perform PAM clustering use \texttt{pam()} instead of
\texttt{kmeans()}.
\EndKnitrBlock{tip}

If you compare k-means and PAM clustering results for a given criterion
and experience common results then that is good indication that outliers
are not skewing your k-mean results (as is the case here).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_nbclust}\NormalTok{(}
\NormalTok{  ames_1hot_scaled, }
\NormalTok{  pam, }
  \DataTypeTok{method =} \StringTok{"silhouette"}\NormalTok{, }
  \DataTypeTok{k.max =} \DecValTok{20}\NormalTok{, }
  \DataTypeTok{verbose =} \OtherTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pam-1} 

}

\caption{Suggested number of clusters for one-hot encoded Ames data using PAM clustering and the Silhouette criterion.}\label{fig:pam}
\end{figure}

As your data set becomes larger both hierarchical, k-means, and PAM
clustering become slower. An alternative is \emph{clustering large
applications} (CLARA), which performs the same algorithmic process as
PAM; however, instead of finding the medioids for the entire data set it
considers a small sample size and applies k-means or PAM. CLARA performs
the following algorithmic steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Randomly split the data set into multiple subsets with fixed size.
\item
  Compute PAM algorithm on each subset and choose the corresponding k
  mediods. Assign each observation of the entire data set to the closest
  mediod.
\item
  Calculate the mean (or sum) of the dissimilarities of the observations
  to their closest mediod. This is used as a measure of the goodness of
  fit of the clustering.
\item
  Retain the sub-data set for which the mean (or sum) is minimal.
\end{enumerate}

\BeginKnitrBlock{tip}
To perform CLARA clustering use \texttt{clara()} instead of
\texttt{pam()} and \texttt{kmeans()}.
\EndKnitrBlock{tip}

We see that our results are very similiar using CLARA as with PAM or
k-means but using clara took 1/10 of the time!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_nbclust}\NormalTok{(}
\NormalTok{  ames_1hot_scaled, }
\NormalTok{  clara, }
  \DataTypeTok{method =} \StringTok{"silhouette"}\NormalTok{, }
  \DataTypeTok{k.max =} \DecValTok{20}\NormalTok{, }
  \DataTypeTok{verbose =} \OtherTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/clara-1} 

}

\caption{Suggested number of clusters for one-hot encoded Ames data using CLARA clustering and the Silhouette criterion.}\label{fig:clara}
\end{figure}

An additional option is to use the \emph{Gower distance}. So far, all
our examples have used the Euclidean distance, the most popular distance
metric used. The Euclidean distance and all the other distance metrics
discussed at the beginning of the clustering chapter require numeric
data to perform the calculations; this is why clustering typically
requires all inputs to be numeric. However, the Gower distance metric
allows for mixed data types and, for each variable type, a particular
distance calculation that works well for that type is used and scaled to
fall between 0 and 1. The metrics used for each data type include:

\begin{itemize}
\tightlist
\item
  \textbf{quantitative (interval)}: range-normalized Manhattan distance,
\item
  \textbf{ordinal}: variable is first ranked, then Manhattan distance is
  used with a special adjustment for ties,
\item
  \textbf{nominal}: variables with k categories are first converted into
  k binary columns (one-hot encoded) and then the Dice coefficient is
  used.
\end{itemize}

To compute the dice metric, the algorithm looks across all one-hot
encoded categorical variables and scores them as:

\begin{itemize}
\tightlist
\item
  \textbf{a} - number of dummies 1 for both individuals
\item
  \textbf{b} - number of dummies 1 for this and 0 for that
\item
  \textbf{c} - number of dummies 0 for this and 1 for that
\item
  \textbf{d} - number of dummies 0 for both
\end{itemize}

and then uses the following formula:

\[ D = \frac{2a}{2a + b + c}  \]

We can use the \texttt{daisy} function to create a Gower distance matrix
of our data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# original data minus Sale_Price}
\NormalTok{ames_full <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Sale_Price)}

\CommentTok{# compute Gower distance for original data}
\NormalTok{gower_dst <-}\StringTok{ }\KeywordTok{daisy}\NormalTok{(ames_full, }\DataTypeTok{metric =} \StringTok{"gower"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now use the resulting distance matrix and feed it into any
clustering algorithm that accepts a distance matrix. This primarily
includes \texttt{pam()}, \texttt{diana()}, and \texttt{agnes()}
(\texttt{kmeans()} and \texttt{clara()} do not accept distance matrices
as inputs).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pam_gower <-}\StringTok{ }\KeywordTok{pam}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gower_dst, }\DataTypeTok{k =} \DecValTok{8}\NormalTok{, }\DataTypeTok{diss =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{diana_gower <-}\StringTok{ }\KeywordTok{diana}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gower_dst, }\DataTypeTok{diss =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{agnes_gower <-}\StringTok{ }\KeywordTok{agnes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gower_dst, }\DataTypeTok{diss =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{tip}
Another technique that you can apply is Non-Negative Matrix
Factorization; however, we do not cover this algorithm in this book.
\EndKnitrBlock{tip}

\hypertarget{part-predictive-analytics}{%
\part{Predictive Analytics}\label{part-predictive-analytics}}

\hypertarget{fundamentalconcepts}{%
\chapter{Fundamental concepts}\label{fundamentalconcepts}}

Predictive analytics continues to grow in importance for many
organizations across nearly all domains. A \textbf{\emph{predictive
model}} is used for tasks that involve the prediction of a given output
using other variables and their values (\emph{features}) in the data
set. Or as stated by \citet{apm}, predictive modeling is \emph{``the
process of developing a mathematical tool or model that generates an
accurate prediction''} (p.~2). The learning algorithm in a predictive
attempts to discover and model the relationship among the
\textbf{\emph{target}} response (the variable being predicted) and the
other features (aka predictor variables). Examples of predictive
modeling include:

\begin{itemize}
\tightlist
\item
  using customer attributes to predict the probability of the customer
  churning in the next 6 weeks,
\item
  using home attributes to predict the sales price,
\item
  using employee attributes to predict the likelihood of attrition,
\item
  using patient attributes and symptoms to predict the risk of
  readmission,
\item
  using production attributes to predict time to market.
\end{itemize}

Each of these examples have a defined learning task. They each intend to
use attributes (\(X\)) to predict an outcome measurement (\(Y\)).

\begin{note}
Throughout this section we will use various terms interchangeably for:

\begin{itemize}
\tightlist
\item
  \(X\): ``predictor variables'', ``independent variables'',
  ``attributes'', ``features'', ``predictors''
\item
  \(Y\): ``target variable'', ``dependent variable'', ``response'',
  ``outcome measurement''
\end{itemize}
\end{note}

The predictive modeling examples above describe what is known as
\emph{supervised learning}. The supervision refers to the fact that the
target values provide a supervisory role, which indicates to the learner
the task it needs to learn. Specifically, given a set of data, the
learning algorithm attempts to optimize a function (the algorithmic
steps) to find the combination of feature values that results in a
predicted value that is as close to the actual target output as
possible.

\begin{note}
In supervised learning, the training data you feed the algorithm
includes the desired solutions. Consequently, the solutions can be used
to help \emph{supervise} the training process to find the optimal
algorithm parameters.
\end{note}

Supervised learning problems revolve around two primary themes:
regression and classification.

\hypertarget{regression-problems}{%
\section{Regression problems}\label{regression-problems}}

When the objective of our supervised learning is to predict a numeric
outcome, we refer to this as a \textbf{\emph{regression problem}} (not
to be confused with linear regression modeling). Regression problems
revolve around predicting output that falls on a continuous numeric
spectrum. In the examples above predicting home sales prices and time to
market reflect a regression problem because the output is numeric and
continuous. This means, given the combination of predictor values, the
response value could fall anywhere along the continuous spectrum. Figure
\ref{fig:regression-problem} illustrates average home sales prices as a
function of two home features: year built and total square footage.
Depending on the combination of these two features, the expected home
sales price could fall anywhere along the plane.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/regression-problem-1} 

}

\caption{Average home sales price as a function of year built and total square footage.}\label{fig:regression-problem}
\end{figure}

\hypertarget{classification-problems}{%
\section{Classification problems}\label{classification-problems}}

When the objective of our supervised learning is to predict a
categorical response, we refer to this as a \textbf{\emph{classification
problem}}. Classification problems most commonly revolve around
predicting a binary or multinomial response measure such as:

\begin{itemize}
\tightlist
\item
  did a customer redeem a coupon (yes/no, 1/0),
\item
  did a customer churn (yes/no, 1/0),
\item
  did a customer click on our online ad (yes/no, 1/0),
\item
  classifying customer reviews:

  \begin{itemize}
  \tightlist
  \item
    binary: positive vs negative
  \item
    multinomial: extremely negative to extremely positive on a 0-5
    Likert scale
  \end{itemize}
\end{itemize}

However, when we apply predictive models for classification problems,
rather than predict a particular class (i.e. ``yes'' or ``no''), we
often predict the \emph{probability} of a particular class (i.e.~yes:
.65, no: .35). Then the class with the highest probability becomes the
predicted class. Consequently, even though we are performing a
classification problem, we are still predicting a numeric output
(probability). However, the essence of the problem still makes it a
classification problem.

\hypertarget{algorithm-comparison-guide}{%
\section{Algorithm Comparison Guide}\label{algorithm-comparison-guide}}

\textbf{TODO: do we want something along these lines here?}

Although there are supervised learning algorithms that can be applied to
regression problems but not classification and vice versa, the
supervised predictive models we cover in this book can be applied to
both.\footnote{For brevity, in each chapter we demonstrate the
  particular model on a single regression or classification problem.
  However, we provide example code of each algorithm applied to a
  regression and classification problem at
  \url{https://github.com/koalaverse/abar}.} These algorithms have
become the most popular predictive analytic techniques in recent years.

Although the chapters that follow will go into detail on each algorithm,
the following provides a quick reference guide that compares and
contrasts some of their features.

Characteristics

Generalized Linear Models (GLM)

Regularized GLM

Multivariate Adaptive Regression Splines

Random Forest

Gradient Boosting Machine

Deep Learning

Captures non-linear relationships

Allows n \textless{} p

Provides automatic feature selection

Handles missing values

No feature pre-processing required

Robust to outliers

Easy to tune

NA

Computational speed

Predictive power

\hypertarget{general-modeling-process}{%
\section{General modeling process}\label{general-modeling-process}}

Predictive modeling is a very iterative process. If performed and
interpreted correctly, we can have great confidence in our outcomes. If
not, the results will be useless. Approaching predictive modeling
correctly means approaching it strategically by spending our data wisely
on learning and validation procedures, properly pre-processing
variables, minimizing data leakage, tuning hyperparameters, and
assessing model performance (Figure \ref{fig:06-modeling-process}).
Before introducing specific algorithms, this section introduces concepts
that are commonly required in the supervised predictive modeling process
and that you'll see briskly covered in each chapter.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{illustrations/modeling_process2} 

}

\caption{General predictive modeling process.}\label{fig:06-modeling-process}
\end{figure}

\hypertarget{reg_perf_prereq}{%
\subsection{Prerequisites}\label{reg_perf_prereq}}

This section leverages the following packages.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rsample)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

To illustrate some of the concepts, we will use the Ames Housing data
and employee attrition data introduced in Section \ref{data}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ames data}
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}

\CommentTok{# attrition data}
\NormalTok{churn <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition}
\end{Highlighting}
\end{Shaded}

\hypertarget{reg-perf-split}{%
\subsection{Data splitting}\label{reg-perf-split}}

\hypertarget{spending-our-data-wisely}{%
\subsubsection{Spending our data
wisely}\label{spending-our-data-wisely}}

A major goal of the predictive modeling process is to find an algorithm
\(f(x)\) that most accurately predicts future values (\(y\)) based on a
set of inputs (\(x\)). In other words, we want an algorithm that not
only fits well to our past data, but more importantly, one that predicts
a future outcome accurately. This is called the
\textbf{\emph{generalizability}} of our algorithm. How we
\emph{``spend''} our data will help us understand how well our algorithm
generalizes to unseen data.

To provide an accurate understanding of the generalizability of our
final optimal model, we split our data into training and test data sets:

\begin{itemize}
\tightlist
\item
  \textbf{Training Set}: these data are used to train our algorithms and
  tune hyper-parameters.
\item
  \textbf{Test Set}: having chosen a final model, these data are used to
  estimate its prediction error (generalization error). These data
  should \emph{not be used during model training!}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=175]{illustrations/data_split} 

}

\caption{Splitting data into training and test sets.}\label{fig:unnamed-chunk-59}
\end{figure}

Given a fixed amount of data, typical recommendations for splitting your
data into training-testing splits include 60\% (training) - 40\%
(testing), 70\%-30\%, or 80\%-20\%. Generally speaking, these are
appropriate guidelines to follow; however, it is good to keep in mind
that as your overall data set gets smaller,

\begin{itemize}
\tightlist
\item
  spending too much in training (\(>80\%\)) won't allow us to get a good
  assessment of predictive performance. We may find a model that fits
  the training data very well, but is not generalizable (overfitting),
\item
  sometimes too much spent in testing (\(>40\%\)) won't allow us to get
  a good assessment of model parameters
\end{itemize}

In today's data-rich environment, typically, we are not lacking in the
quantity of observations, so a 70-30 split is often sufficient. The two
most common ways of splitting data include \textbf{\emph{simple random
sampling}} and \textbf{\emph{stratified sampling}}.

\hypertarget{simple-random-sampling}{%
\subsubsection{Simple random sampling}\label{simple-random-sampling}}

The simplest way to split the data into training and test sets is to
take a simple random sample. This does not control for any data
attributes, such as the percentage of data represented in your response
variable (\(y\)). There are multiple ways to split our data. Here we
show four options to produce a 70-30 split (note that setting the seed
value allows you to reproduce your randomized splits):

\begin{note}
Sampling is a random process so setting the random number generator with
a common seed allows for reproducible results. Throughout this book we
will use the number \emph{123} often for reproducibility but the number
itself has no special meaning.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# base R}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(ames), }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ames) }\OperatorTok{*}\StringTok{ }\FloatTok{0.7}\NormalTok{))}
\NormalTok{train_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{1}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{1}\NormalTok{, ]}

\CommentTok{# caret package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{index_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(ames}\OperatorTok{$}\NormalTok{Sale_Price, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{ames[index_}\DecValTok{2}\NormalTok{, ]}
\NormalTok{test_}\DecValTok{2}\NormalTok{  <-}\StringTok{ }\NormalTok{ames[}\OperatorTok{-}\NormalTok{index_}\DecValTok{2}\NormalTok{, ]}

\CommentTok{# rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(ames, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{train_}\DecValTok{3}\NormalTok{  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}
\NormalTok{test_}\DecValTok{3}\NormalTok{   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since this sampling approach will randomly sample across the
distribution of \(y\) (\texttt{Sale\_Price} in our example), you will
typically result in a similar distribution between your training and
test sets as illustrated below.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/06-distributions-1} 

}

\caption{Distribution comparison between the training (black) test (red) sets.}\label{fig:06-distributions}
\end{figure}

\hypertarget{stratified-sampling}{%
\subsubsection{Stratified sampling}\label{stratified-sampling}}

However, if we want to explicitly control our sampling so that our
training and test sets have similar \(y\) distributions, we can use
stratified sampling. This is more common with classification problems
where the reponse variable may be imbalanced (90\% of observations with
response ``Yes'' and 10\% with response ``No''). However, we can also
apply to regression problems for data sets that have a small sample size
and where the response variable deviates strongly from normality. With a
continuous response variable, stratified sampling will break \(y\) down
into quantiles and randomly sample from each quantile. Consequently,
this will help ensure a balanced representation of the response
distribution in both the training and test sets.

The easiest way to perform stratified sampling on a response variable is
to use the \textbf{rsample} package, where you specify the response
variable to \texttt{strata}fy. The following illustrates that in our
original employee attrition data we have an imbalanced response (No:
84\%, Yes: 16\%). By enforcing stratified sampling both our training and
testing sets have approximately equal response distributions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# orginal response distribution}
\KeywordTok{table}\NormalTok{(churn}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\NormalTok{## }
\NormalTok{##     No    Yes }
\NormalTok{## 0.8388 0.1612}

\CommentTok{# stratified sampling with the rsample package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split_strat  <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(churn, }\DataTypeTok{prop =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train_strat  <-}\StringTok{ }\KeywordTok{training}\NormalTok{(split_strat)}
\NormalTok{test_strat   <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(split_strat)}

\CommentTok{# consistent response ratio between train & test}
\KeywordTok{table}\NormalTok{(train_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\NormalTok{## }
\NormalTok{##     No    Yes }
\NormalTok{## 0.8388 0.1612}
\KeywordTok{table}\NormalTok{(test_strat}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{prop.table}\NormalTok{()}
\NormalTok{## }
\NormalTok{##     No    Yes }
\NormalTok{## 0.8386 0.1614}
\end{Highlighting}
\end{Shaded}

\hypertarget{reg_perf_feat}{%
\subsection{Feature engineering}\label{reg_perf_feat}}

\textbf{\emph{Feature engineering}} generally refers to the process of
adding, deleting, and transforming the variables to be applied to your
predictive modeling algorithms. Feature engineering is a significant
process and requires you to spend substantial time understanding your
data\ldots{}or as Leo Breiman said \emph{``live with your data before
you plunge into modeling''} \citep{breiman2001statistical}.

Although this section primarily focuses on applying predictive modeling
algorithms, feature engineering can make or break an algorithm's
predictive ability. We will not cover all the potential ways of
implementing feature engineering; however, we will cover a few
fundamental pre-processing tasks that can significantly improve modeling
performance. To learn more about feature engineering check out
\href{http://shop.oreilly.com/product/0636920049081.do}{Feature
Engineering for Machine Learning} by \citet{zheng2018feature} and Max
Kuhn's upcoming book \href{http://www.feat.engineering/}{Feature
Engineering and Selection: A Practical Approach for Predictive Models}.

\hypertarget{response-transformation}{%
\subsubsection{Response Transformation}\label{response-transformation}}

Although not a requirement, normalizing the distribution of the response
variable by using a \emph{transformation} can lead to a big improvement,
especially for parametric models. As we saw in Figure
\ref{fig:06-distributions}, our response variable \texttt{Sale\_Price}
is right skewed. To normalize, we have a few options:

\textbf{Option 1}: normalize with a log transformation as discussed in
\ref{empirical-rule}. This will transform most right skewed
distributions to be approximately normal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transformation}
\NormalTok{train_log_y <-}\StringTok{ }\KeywordTok{log}\NormalTok{(train_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price)}
\NormalTok{test_log_y  <-}\StringTok{ }\KeywordTok{log}\NormalTok{(test_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

If your reponse has negative values then a log transformation will
produce \texttt{NaN}s. If these negative values are small (between -0.99
and 0) then you can apply \texttt{log1p}, which adds 1 to the value
prior to applying a log transformation. If your data consists of
negative equal to or less than -1, use the Yeo Johnson transformation
mentioned next.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\OperatorTok{-}\NormalTok{.}\DecValTok{5}\NormalTok{)}
\NormalTok{## [1] NaN}
\KeywordTok{log1p}\NormalTok{(}\OperatorTok{-}\NormalTok{.}\DecValTok{5}\NormalTok{)}
\NormalTok{## [1] -0.6931}
\end{Highlighting}
\end{Shaded}

\textbf{Option 2}: use a Box Cox transformation. A Box Cox
transformation is more flexible than a log transformation and will find
the transformation from a family of
\href{https://en.wikipedia.org/wiki/Power_transform\#Box\%E2\%80\%93Cox_transformation}{power
transforms} that will transform the variable as close as possible to a
normal distribution. At the core of the Box Cox transformation is an
exponent, lambda (\(\lambda\)), which varies from -5 to 5. All values of
\(\lambda\) are considered and the optimal value for the given data is
selected; The ``optimal value'' is the one which results in the best
approximation of a normal distribution curve. The transformation of Y
has the form:

\[
 \begin{equation} 
 y(\lambda) =
\begin{cases}
   \frac{y^\lambda-1}{\lambda}, & \text{if}\ \lambda \neq 0 \\
   \log y, & \text{if}\ \lambda = 0.
\end{cases}
\end{equation}
\]

\begin{rmdwarning}
Be sure to compute the \texttt{lambda} on the training set and apply
that same \texttt{lambda} to both the training and test set to minimize
data leakage.
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Box Cox transformation}
\NormalTok{lambda  <-}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{BoxCox.lambda}\NormalTok{(train_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price)}
\NormalTok{train_bc_y <-}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{BoxCox}\NormalTok{(train_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price, lambda)}
\NormalTok{test_bc_y  <-}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{BoxCox}\NormalTok{(test_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price, lambda)}
\end{Highlighting}
\end{Shaded}

We can see that in this example, the log transformation and Box Cox
transformation both do about equally well in transforming our reponse
variable to be normally distributed.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/unnamed-chunk-62-1} 

}

\caption{Response variable transformations.}\label{fig:unnamed-chunk-62}
\end{figure}

Note that when you model with a transformed response variable, your
predictions will also be in the transformed value. You will likely want
to re-transform your predicted values back to their normal state so that
decision-makers can interpret the results. The following code can do
this for you:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transform a value}
\NormalTok{y <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{10}\NormalTok{)}

\CommentTok{# re-transforming the log-transformed value}
\KeywordTok{exp}\NormalTok{(y)}
\NormalTok{## [1] 10}

\CommentTok{# Box Cox transform a value}
\NormalTok{y <-}\StringTok{ }\NormalTok{forecast}\OperatorTok{::}\KeywordTok{BoxCox}\NormalTok{(}\DecValTok{10}\NormalTok{, lambda)}

\CommentTok{# Inverse Box Cox function}
\NormalTok{inv_box_cox <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, lambda) \{}
  \ControlFlowTok{if}\NormalTok{ (lambda }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\KeywordTok{exp}\NormalTok{(x) }\ControlFlowTok{else}\NormalTok{ (lambda}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{^}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{lambda) }
\NormalTok{\}}

\CommentTok{# re-transforming the Box Cox-transformed value}
\KeywordTok{inv_box_cox}\NormalTok{(y, lambda)}
\NormalTok{## [1] 10}
\NormalTok{## attr(,"lambda")}
\NormalTok{## [1] -0.3068}
\end{Highlighting}
\end{Shaded}

\begin{tip}
If your response has negative values, you can use the Yeo-Johnson
transformation. To apply, use \texttt{car::powerTransform} to identify
the lambda, \texttt{car::yjPower} to apply the transformation, and
\texttt{VGAM::yeo.johnson} to apply the transformation and/or the
inverse transformation.
\end{tip}

\hypertarget{predictor-transformation}{%
\subsubsection{Predictor
Transformation}\label{predictor-transformation}}

\hypertarget{one-hot-encoding}{%
\paragraph{One-hot encoding}\label{one-hot-encoding}}

Many models require all predictor variables to be numeric. Consequently,
we need to transform any categorical variables into numeric
representations so that these algorithms can compute. Some packages
automate this process (i.e. \texttt{h2o}, \texttt{glm}, \texttt{caret})
while others do not (i.e. \texttt{glmnet}, \texttt{keras}). Furthermore,
there are many ways to encode categorical variables as numeric
representations (i.e.~one-hot, ordinal, binary, sum, Helmert).

The most common is referred to as one-hot encoding, where we transpose
our categorical variables so that each level of the feature is
represented as a boolean value. For example, one-hot encoding variable
\texttt{x} in the following:

\begin{tabular}{r|l}
\hline
id & x\\
\hline
1 & a\\
\hline
2 & c\\
\hline
3 & b\\
\hline
4 & c\\
\hline
5 & c\\
\hline
6 & a\\
\hline
7 & b\\
\hline
8 & c\\
\hline
\end{tabular}

results in the following representation:

\begin{tabular}{r|r|r|r}
\hline
id & x.a & x.b & x.c\\
\hline
1 & 1 & 0 & 0\\
\hline
2 & 0 & 0 & 1\\
\hline
3 & 0 & 1 & 0\\
\hline
4 & 0 & 0 & 1\\
\hline
5 & 0 & 0 & 1\\
\hline
6 & 1 & 0 & 0\\
\hline
7 & 0 & 1 & 0\\
\hline
8 & 0 & 0 & 1\\
\hline
\end{tabular}

This is called less than \emph{full rank} encoding where we retain all
variables for each level of \texttt{x}. However, this creates perfect
collinearity which causes problems with some predictive modeling
algorithms (i.e.~generalized regression models, neural networks).
Alternatively, we can create full-rank one-hot encoding by dropping one
of the levels (level \texttt{a} has been dropped):

\begin{tabular}{r|r|r}
\hline
id & x.b & x.c\\
\hline
1 & 0 & 0\\
\hline
2 & 0 & 1\\
\hline
3 & 1 & 0\\
\hline
4 & 0 & 1\\
\hline
5 & 0 & 1\\
\hline
6 & 0 & 0\\
\hline
7 & 1 & 0\\
\hline
8 & 0 & 1\\
\hline
\end{tabular}

If you needed to manually implement one-hot encoding yourself you can
with \texttt{caret::dummyVars}. Sometimes you may have a feature level
with very few observations and all these observations show up in the
test set but not the training set. The benefit of using
\texttt{dummyVars} on the full data set and then applying the result to
both the train and test data sets is that it will guarantee that the
same features are represented in both the train and test data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# full rank one-hot encode - recommended for generalized linear models and}
\CommentTok{# neural networks}
\NormalTok{full_rank  <-}\StringTok{ }\KeywordTok{dummyVars}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{fullRank =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{train_oh   <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(full_rank, train_}\DecValTok{1}\NormalTok{)}
\NormalTok{test_oh    <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(full_rank, test_}\DecValTok{1}\NormalTok{)}

\CommentTok{# less than full rank --> dummy encoding}
\NormalTok{dummy    <-}\StringTok{ }\KeywordTok{dummyVars}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ames, }\DataTypeTok{fullRank =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_oh <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(dummy, train_}\DecValTok{1}\NormalTok{)}
\NormalTok{test_oh  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(dummy, test_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tip}
Since one-hot encoding adds new features it can significantly increase
the dimensionality of our data. If you have a data set with many
categorical variables and those categorical variables in turn have many
unique levels, the number of features can explode. In these cases you
may want to explore ordinal encoding of your data.
\end{tip}

\hypertarget{standardizing}{%
\subsubsection{Standardizing}\label{standardizing}}

Some models (i.e.~generalized linear models, regularized models, neural
networks) require that the predictor variables have the same units.
\textbf{Centering} and \textbf{scaling} can be used for this purpose and
is often referred to as \textbf{\emph{standardizing}} the features.
Standardizing numeric variables results in zero mean and unit variance,
which provides a common comparable unit of measure across all the
variables.

Some packages have built-in arguments (i.e. \texttt{glmnet},
\texttt{caret}) to standardize and some do not (i.e. \texttt{glm},
\texttt{keras}). If you need to manually standardize your variables you
can use the \texttt{preProcess} function provided by the \texttt{caret}
package. For example, here we center and scale our Ames predictor
variables.

\begin{warning}
It is important that you standardize the test data based on the training
mean and variance values of each feature. This minimizes data leakage.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify only the predictor variables}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(train_}\DecValTok{1}\NormalTok{), }\StringTok{"Sale_Price"}\NormalTok{)}

\CommentTok{# pre-process estimation based on training features}
\NormalTok{pre_process <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(}
  \DataTypeTok{x      =}\NormalTok{ train_}\DecValTok{1}\NormalTok{[, features],}
  \DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{)    }
\NormalTok{  )}

\CommentTok{# apply to both training & test}
\NormalTok{train_x <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(pre_process, train_}\DecValTok{1}\NormalTok{[, features])}
\NormalTok{test_x  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(pre_process, test_}\DecValTok{1}\NormalTok{[, features])}
\end{Highlighting}
\end{Shaded}

\hypertarget{alternative-feature-transformation}{%
\subsubsection{Alternative Feature
Transformation}\label{alternative-feature-transformation}}

There are some alternative transformations that you can perform:

\begin{itemize}
\item
  Normalizing the predictor variables with a Box Cox transformation can
  improve parametric model performance.
\item
  Collapsing highly correlated variables with PCA can reduce the number
  of features and increase the stability of generalize linear models.
  However, this reduces the amount of information at your disposal and
  we show you how to use regularization as a better alternative to PCA.
\item
  Removing near-zero or zero variance variables. Variables with vary
  little variance tend to not improve model performance and can be
  removed.
\end{itemize}

\begin{tip}
\texttt{preProcess} provides many other transformation options which you
can read more about
\href{https://topepo.github.io/caret/pre-processing.html}{here}.
\end{tip}

For example, the following normalizes predictors with a Box Cox
transformation, center and scales continuous variables, performs
principal component analysis to reduce the predictor dimensions, and
removes predictors with near zero variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify only the predictor variables}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(train_}\DecValTok{1}\NormalTok{), }\StringTok{"Sale_Price"}\NormalTok{)}

\CommentTok{# pre-process estimation based on training features}
\NormalTok{pre_process <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(}
  \DataTypeTok{x      =}\NormalTok{ train_}\DecValTok{1}\NormalTok{[, features],}
  \DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"pca"}\NormalTok{, }\StringTok{"nzv"}\NormalTok{)    }
\NormalTok{  )}

\CommentTok{# apply to both training & test}
\NormalTok{train_x <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(pre_process, train_}\DecValTok{1}\NormalTok{[, features])}
\NormalTok{test_x  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(pre_process, test_}\DecValTok{1}\NormalTok{[, features])}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-form}{%
\subsection{Basic model formulation}\label{model-form}}

There are \textbf{\emph{many}} packages to perform predictive modeling
and there are almost always more than one to perform each algorithm
(i.e.~there are over 20 packages to perform random forests). There are
pros and cons to each package; some may be more computationally
efficient while others may have more hyperparameter tuning options.
Future chapters will expose you to many of the packages and algorithms
that perform and scale best to most organization's problems and data
sets. Just realize there are \emph{more ways than one to skin a} 🙀.

For example, these three functions will all produce the same linear
regression model output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.lm    <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_}\DecValTok{1}\NormalTok{)}
\NormalTok{lm.glm   <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_}\DecValTok{1}\NormalTok{, }\DataTypeTok{family =}\NormalTok{ gaussian)}
\NormalTok{lm.caret <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_}\DecValTok{1}\NormalTok{, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

One thing you will notice throughout this section is that we can specify
our model formulation in different ways. In the above examples we use
the \emph{model formulation} (\texttt{Sale\_Price\ \textasciitilde{}\ .}
which says explain \texttt{Sale\_Price} based on all features) approach.
An alternative approach you will see throughout this section is the
matrix formulation approach.

\emph{Matrix formulation} requires that we separate our response
variable from our features. For example, in the regularization chaper
we'll use \texttt{glmnet} which requires our features (\texttt{x}) and
response (\texttt{y}) variable to be specified separately:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get feature names}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(train_}\DecValTok{1}\NormalTok{), }\StringTok{"Sale_Price"}\NormalTok{)}

\CommentTok{# create feature and response set}
\NormalTok{train_x <-}\StringTok{ }\NormalTok{train_}\DecValTok{1}\NormalTok{[, features]}
\NormalTok{train_y <-}\StringTok{ }\NormalTok{train_}\DecValTok{1}\OperatorTok{$}\NormalTok{Sale_Price}

\CommentTok{# example of matrix formulation}
\NormalTok{glmnet.m1 <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ train_x, }\DataTypeTok{y =}\NormalTok{ train_y)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tune}{%
\subsection{Model tuning}\label{tune}}

Hyperparameters control the level of model complexity. Some algorithms
have many tuning parameters while others have only one or two. Tuning
can be a good thing as it allows us to transform our model to better
align with patterns within our data. For example, Figure
\ref{fig:less-flexible} shows how the more flexible model aligns more
closely to the data than the fixed linear model.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/less-flexible-1} 

}

\caption{Tuning allows for more flexible patterns to be fit.}\label{fig:less-flexible}
\end{figure}

However, highly tunable models can also be dangerous because they allow
us to overfit our model to the training data, which will not generalize
well to future unseen data.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/over-flexible-1} 

}

\caption{Highly tunable models can overfit if we are not careful.}\label{fig:over-flexible}
\end{figure}

Throughout this section we will demonstrate how to tune the different
parameters for each model. One way to perform hyperparameter tuning is
to fiddle with hyperparameters manually until you find a great
combination of hyperparameter values that result in high predictive
accuracy. However, this would be very tedious work. An alternative
approach is to perform a \textbf{\emph{grid search}}. A grid search is
an automated approach to searching across many combinations of
hyperparameter values. Throughout this guide you will be exposed to
different approaches to performing grid searches.

\hypertarget{cv}{%
\subsection{Cross Validation for Generalization}\label{cv}}

Our goal is to not only find a model that performs well on training data
but to find one that performs well on \emph{future unseen data}. So
although we can tune our model to reduce some error metric to near zero
on our training data, this may not generalize well to future unseen
data. Consequently, our goal is to find a model and its hyperparameters
that will minimize error on held-out data.

Let's go back to this image\ldots{}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/bias-var-1} 

}

\caption{Bias versus variance.}\label{fig:bias-var}
\end{figure}

The model on the left is considered rigid and consistent. If we provided
it a new training sample with slightly different values, the model would
not change much, if at all. Although it is consistent, the model does
not accurately capture the underlying relationship. This is considered a
model with high \textbf{\emph{bias}}.

The model on the right is far more inconsistent. Even with small changes
to our training sample, this model would likely change significantly.
This is considered a model with high \textbf{\emph{variance}}.

The model in the middle balances the two and, likely, will minimize the
error on future unseen data compared to the high bias and high variance
models. This is our goal.

\textbf{TODO}: Create our own illustration

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth,height=0.8\textheight]{illustrations/bias_var} 

}

\caption{Bias-variance tradeoff.}\label{fig:bias-variance-tradeoff}
\end{figure}

To find the model that balances the \textbf{\emph{bias-variance
tradeoff}}, we search for a model that minimizes a \emph{k}-fold
cross-validation error metric. Figure \ref{fig:06-cv} illustrates
\emph{k}-fold cross-validation, which is a resampling method that
randomly divides the training data into \emph{k} groups (aka folds) of
approximately equal size. The model is fit on \(k-1\) folds and then the
held-out validation fold is used to compute the error. This procedure is
repeated \emph{k} times; each time, a different group of observations is
treated as the validation set. This process results in \emph{k}
estimates of the test error
(\(\epsilon_1, \epsilon_2, \dots, \epsilon_k\)). Thus, the \emph{k}-fold
CV estimate is computed by averaging these values, which provides us
with an approximation of the error to expect on unseen data.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{illustrations/cv} 

}

\caption{Illustration of the k-fold cross validation process.}\label{fig:06-cv}
\end{figure}

Many of the algorithms we cover in this guide have built-in cross
validation capabilities. One typically uses a 5 or 10 fold CV (\(k = 5\)
or \(k = 10\)). For example, \texttt{glmnet} implements CV with the
\texttt{nfolds} argument:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example of 10 fold CV in h2o}
\NormalTok{example.cv <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{nfolds =} \DecValTok{10}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{reg-perf-eval}{%
\subsection{Model evaluation}\label{reg-perf-eval}}

This leads us to our next topic, evaluating performance. Historically,
the performance of a predictive model was largely based on
goodness-of-fit tests and assessment of residuals. Unfortunately,
misleading conclusions may follow from predictive models that pass these
kind of assessments \citep{breiman2001statistical}. Today, it has become
widely accepted that a more sound approach to assessing model
performance is to assess the predictive accuracy via \textbf{\emph{loss
functions}}. Loss functions are metrics that compare the predicted
values to the actual value (often referred to as the error or residual).
There are many loss functions to choose when assessing the performance
of a predictive model; each providing a unique understanding of the
predictive accuracy and differing between regression and classification
models. The most common include:

\hypertarget{regression-models}{%
\subsubsection{Regression models}\label{regression-models}}

\begin{itemize}
\item
  \textbf{MSE}: Mean squared error is the average of the squared error
  (\(MSE = \frac{1}{n} \sum^n_{i=1}(y_i - \hat y_i)^2\)). The squared
  component results in larger errors having larger penalties. This
  (along with RMSE) is the most common error metric to use.
  \textbf{Objective: minimize}
\item
  \textbf{RMSE}: Root mean squared error. This simply takes the square
  root of the MSE metric
  (\(RMSE = \sqrt{\frac{1}{n} \sum^n_{i=1}(y_i - \hat y_i)^2}\)) so that
  your error is in the same units as your response variable. If your
  response variable units are dollars, the units of MSE are
  dollars-squared, but the RMSE will be in dollars. \textbf{Objective:
  minimize}
\item
  \textbf{Deviance}: Short for mean residual deviance. In essence, it
  provides a measure of \emph{goodness-of-fit} of the model being
  evaluated when compared to the null model (intercept only). If the
  response variable distribution is gaussian, then it is equal to MSE.
  When not, it usually gives a more useful estimate of error.
  \textbf{Objective: minimize}
\item
  \textbf{MAE}: Mean absolute error. Similar to MSE but rather than
  squaring, it just takes the mean absolute difference between the
  actual and predicted values
  (\(MAE = \frac{1}{n} \sum^n_{i=1}(\vert y_i - \hat y_i \vert)\)).
  \textbf{Objective: minimize}
\item
  \textbf{RMSLE}: Root mean squared logarithmic error. Similiar to RMSE
  but it performs a log() on the actual and predicted values prior to
  computing the difference
  (\(RMSLE = \sqrt{\frac{1}{n} \sum^n_{i=1}(log(y_i + 1) - log(\hat y_i + 1))^2}\)).
  When your response variable has a wide range of values, large repsonse
  values with large errors can dominate the MSE/RMSE metric. RMSLE
  minimizes this impact so that small response values with large errors
  can have just as meaningful of an impact as large response values with
  large errors. \textbf{Objective: minimize}
\item
  \textbf{\(R^2\)}: This is a popular metric that represents the
  proportion of the variance in the dependent variable that is
  predictable from the independent variable. Unfortunately, it has
  several limitations. For example, two models built from two different
  data sets could have the exact same RMSE but if one has less
  variability in the response variable then it would have a lower
  \(R^2\) than the other. You should not place too much emphasis on this
  metric. \textbf{Objective: maximize}
\end{itemize}

Most models we assess in this guide will report most, if not all, of
these metrics. We will emphasize MSE and RMSE but its good to realize
that certain situations warrant emphasis on some more than others.

\hypertarget{classification-models}{%
\subsubsection{Classification models}\label{classification-models}}

\begin{itemize}
\item
  \textbf{Misclassification}: This is the overall error. For example,
  say you are predicting 3 classes ( \emph{high}, \emph{medium},
  \emph{low} ) and each class has 25, 30, 35 observations respectively
  (90 observations total). If you misclassify 3 observations of class
  \emph{high}, 6 of class \emph{medium}, and 4 of class \emph{low}, then
  you misclassified 13 out of 90 observations resulting in a 14\%
  misclassification rate. \textbf{Objective: minimize}
\item
  \textbf{Mean per class error}: This is the average error rate for each
  class. For the above example, this would be the mean of
  \(\frac{3}{25}, \frac{6}{30}, \frac{4}{35}\), which is 12\%. If your
  classes are balanced this will be identical to misclassification.
  \textbf{Objective: minimize}
\item
  \textbf{MSE}: Mean squared error. Computes the distance from 1.0 to
  the probability suggested. So, say we have three classes, A, B, and C,
  and your model predicts a probabilty of 0.91 for A, 0.07 for B, and
  0.02 for C. If the correct answer was A the \(MSE = 0.09^2 = 0.0081\),
  if it is B \(MSE = 0.93^2 = 0.8649\), if it is C
  \(MSE = 0.98^2 = 0.9604\). The squared component results in large
  differences in probabilities for the true class having larger
  penalties. \textbf{Objective: minimize}
\item
  \textbf{Cross-entropy (aka Log Loss or Deviance)}: Similar to MSE but
  it incorporates a log of the predicted probability multiplied by the
  true class. Consequently, this metric disproportionately punishes
  predictions where we predict a small probability for the true class,
  which is another way of saying having high confidence in the wrong
  answer is really bad. \textbf{Objective: minimize}
\item
  \textbf{Gini index}: Mainly used with tree-based methods and commonly
  referred to as a measure of \emph{purity} where a small value
  indicates that a node contains predominantly observations from a
  single class. \textbf{Objective: minimize}
\end{itemize}

When applying classification models, we often use a \emph{confusion
matrix} to evaluate certain performance measures. A confusion matrix is
simply a matrix that compares actual categorical levels (or events) to
the predicted categorical levels. When we predict the right level, we
refer to this as a \emph{true positive}. However, if we predict a level
or event that did not happen this is called a \emph{false positive}
(i.e.~we predicted a customer would redeem a coupon and they did not).
Alternatively, when we do not predict a level or event and it does
happen that this is called a \emph{false negative} (i.e.~a customer that
we did not predict to redeem a coupon does).

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{illustrations/confusion-matrix} 

}

\caption{Confusion matrix.}\label{fig:confusion-matrix}
\end{figure}

We can extract different levels of performance from these measures. For
example, given the classification matrix below we can assess the
following:

\begin{itemize}
\item
  \textbf{Accuracy}: Overall, how often is the classifier correct?
  Opposite of misclassification above. Example:
  \(\frac{TP + TN}{total} = \frac{100+50}{165} = 0.91\).
  \textbf{Objective: maximize}
\item
  \textbf{Precision}: How accurately does the classifier predict events?
  This metric is concerned with maximizing the true positives to false
  positive ratio. In other words, for the number of predictions that we
  made, how many were correct? Example:
  \(\frac{TP}{TP + FP} = \frac{100}{100+10} = 0.91\). \textbf{Objective:
  maximize}
\item
  \textbf{Sensitivity (aka recall)}: How accurately does the classifier
  classify actual events? This metric is concerned with maximizing the
  true positives to false negatives ratio. In other words, for the
  events that occurred, how many did we predict? Example:
  \(\frac{TP}{TP + FN} = \frac{100}{100+5} = 0.95\). \textbf{Objective:
  maximize}
\item
  \textbf{Specificity}: How accurately does the classifier classify
  actual non-events? Example:
  \(\frac{TN}{TN + FP} = \frac{50}{50+10} = 0.83\). \textbf{Objective:
  maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{illustrations/confusion-matrix2} 

}

\caption{Example confusion matrix.}\label{fig:confusion-matrix2}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{AUC}: Area under the curve. A good classifier will have high
  precision and sensitivity. This means the classifier does well when it
  predicts an event will and will not occur, which minimizes false
  positives and false negatives. To capture this balance, we often use a
  ROC curve that plots the false positive rate along the x-axis and the
  true positive rate along the y-axis. A line that is diagonal from the
  lower left corner to the upper right corner represents a random guess.
  The higher the line is in the upper left-hand corner, the better. AUC
  computes the area under this curve. \textbf{Objective: maximize}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{illustrations/roc} 

}

\caption{ROC curve.}\label{fig:roc}
\end{figure}

\hypertarget{interpreting-predictive-models}{%
\subsection{Interpreting predictive
models}\label{interpreting-predictive-models}}

In his seminal 2001 paper \citep{breiman2001statistical}, Leo Breiman
popularized the phrase: \emph{``the multiplicity of good models.''} The
phrase means that for the same set of input variables and prediction
targets, complex predictive modeling algorithms can produce multiple
accurate models with very similar, but not the exact same, internal
architectures.

Figure \ref{fig:error-surface} is a depiction of a non-convex error
surface that is representative of the error function for a predictive
model with two inputs --- say, a customer's income and a customer's age,
and an output, such as the same customer's probability of redeeming a
coupon. This non-convex error surface with no obvious global minimum
implies there are many different ways complex predictive models could
learn to weigh a customer's income and age to make a good decision about
if they are likely to redeem a coupon. Each of these different
weightings would create a different function for making coupon
redemption (and therefore marketing) decisions, and each of these
different functions would have different explanations.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/error-surface-1} 

}

\caption{Non-convex error surface with many local minimas.}\label{fig:error-surface}
\end{figure}

All of this is an obstacle to analysts, as they can experience very
similar predictions from different models based on the same feature set.
However, these models will have very different logic and structure
leading to different interpretations. Consequently, practitioners should
understand how to interpret different types of models. Throughout this
section we will provide you with the a variety of ways to interpret your
predictive models so that you understand what is driving model and
prediction performance. This will allow you to be more effective and
efficient in applying and understanding mutliple good models.

\hypertarget{linear-regression}{%
\chapter{Linear regression}\label{linear-regression}}

Linear regression is a very simple approach for supervised learning, has
been around for a long time, and is the topic of innumerable textbooks.
Though it may seem somewhat dull compared to some of the more modern
statistical learning approaches described in later chapters, linear
regression is still a useful and widely applied statistical learning
method. Moreover, it serves as a good jumping-off point for newer
approaches: as we will see in later chapters, many fancy statistical
learning approaches can be seen as generalizations or extensions of
linear regression. Consequently, the importance of having a good
understanding of linear regression before studying more complex learning
methods cannot be overstated. This chapter introduces linear regression
with an emphasis on predictive purposes rather than inferential purposes
(see \citet{faraway2016linear} for discussion of linear regression in R
with an inferential emphasis).

\hypertarget{prerequisites-3}{%
\section{Prerequisites}\label{prerequisites-3}}

For this section we will use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# data manipulation & visualization}
\KeywordTok{library}\NormalTok{(rsample)    }\CommentTok{# data splitting}
\KeywordTok{library}\NormalTok{(caret)      }\CommentTok{# regression modeling}
\KeywordTok{library}\NormalTok{(modelr)     }\CommentTok{# provides easy pipeline modeling functions}
\KeywordTok{library}\NormalTok{(broom)      }\CommentTok{# helps to tidy up model outputs}
\KeywordTok{library}\NormalTok{(vip)        }\CommentTok{# variable importance}
\end{Highlighting}
\end{Shaded}

To illustrate linear regression concepts we will use the Ames, IA
housing data, where our intent is to predict \texttt{Sale\_Price}. As
discussed in the \emph{Data splitting} section \ref{reg-perf-split},
we'll set aside 30\% of our data as a test set to assess our
generalizability error.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.}
\CommentTok{# Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ames_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{(), }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Sale_Price"}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(ames_split)}
\NormalTok{test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(ames_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

\emph{Simple linear regression} lives up to its name: it is a very
straightforward approach for predicting a continuous quantitative
response \(Y\) on the basis of a single predictor variable \(X\). It
assumes that there is approximately a linear relationship between \(X\)
and \(Y\).

Consider our housing data, suppose we wish to model the linear
relationship between the year the house was built (\texttt{Year\_Built})
and sale price (\texttt{Sale\_Price}). We can write this as Equation
\eqref{eq:lm} and we often state this as \emph{regressing Y onto X}.

\begin{equation}
\label{eq:lm}
  Y = \beta_0 + \beta_1X + \epsilon,
\end{equation}

where \(Y\) represents \texttt{Sale\_Price}, \(X\) represents
\texttt{Year\_Built}, \(\beta_0\) and \(\beta_1\) represent two unknown
constants (commonly referred to as coefficients or parameters) that
represent the intercept and slope terms in the linear model, and
\(\epsilon\) is a mean-zero random error term. To estimate the
coefficient parameters (\(\beta_i\)), the linear regression algorithm
will identify the best-fit linear relationship that fits the data well.
There are multiple ways to measure ``best-fit'' but the most common
involves minimizing the \emph{least squares} criterion also referred to
as \emph{ordinary least squares} (OLS). The OLS criterion identifies the
``best-fit'' line by minimizing the error between the best-fit line (the
predicted sales price) and the actual sale price values.

To perform an OLS regression model in R we can use \texttt{lm}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area, }\DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:07-visualize-model1} illustrates this linear model with
the best fit linear line. The individual dots represent the actual sales
price, and the vertical grey lines represent the individual errors for
each observation. The OLS criterion identifies the best-fit line that
minimizes the residual sum of squared errors (RSS), which follows
Equation \eqref{eq:rss}

\begin{equation}
\label{eq:rss}
  RSS = \sum^n_{i=1}(y_i - f(x_i))^2,
\end{equation}

where \(y_i\) is the i\(^{th}\) value of the explanatory variable, and
\(f(x_i)\) is the predicted value of \(y_i\) (also termed
(\(\hat y_i\))). The left plot of \ref{fig:07-visualize-model1}
illustrates this model's fit across homes that have between 1000-2000
square feet of living above ground whereas the right plot shows the
model's fit across the entire training set.

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/07-visualize-model1-1}

\}

\textbackslash{}caption\{The least squares fit for regressing sale price
onto above ground square footage for the the Ames housing data. The
least squares criterion finds the `best-fit' line that minimizes the sum
of squared errors between the actual sales price (individual dots) and
the predicted sales price (blue line). \emph{Left}: model fit for homes
with 1000-2000 square feet. \emph{Right}: model fit across all
observations.\}\label{fig:07-visualize-model1} \textbackslash{}end\{figure\}

To identify the coefficients that minimize the RSS, OLS selects the
optimal estimated coefficients (\(\hat \beta_0\) and \(\hat \beta_1\)).
Its been proven that the coefficients that minimize the RSS are:

\begin{align}
\label{eq:coefficients}
  \hat \beta_1 = \frac{\sum^n_{i=1}(x_i - \bar x)(y_i - \bar y)}{\sum^n_{i=1}(x_i - \bar x)^2},
  \hat \beta_0 = \bar y - \hat \beta_1 \bar x
\end{align}

where \(\bar y = \frac{1}{n}\sum^n_{i=1}y_i\) and
\(\bar x = \frac{1}{n}\sum^n_{i=1}x_i\) are the sample means.
\texttt{summary} allows us to access the coeffients for our model along
with other model results. For this simple model, our coefficients are
\(\hat \beta_0 = 17797.0728\) and \(\hat \beta_1 = 108.0344\). In other
words, according to this approximation, an additional one square foot of
above ground living space of a house is associated with approximately an
additional \$108 in selling price. This ease in interpreting the
relationship between the sale price and square footage with a single
number is what makes linear regression such a intuitive and popular
modeling tool.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(model1)}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = Sale_Price ~ Gr_Liv_Area, data = train)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##     Min      1Q  Median      3Q     Max }
\NormalTok{## -467327  -30799   -1432   22339  338467 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##             Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept) 17797.07    3916.11    4.54  5.8e-06 ***}
\NormalTok{## Gr_Liv_Area   108.03       2.47   43.74  < 2e-16 ***}
\NormalTok{## ---}
\NormalTok{## Signif. codes:  }
\NormalTok{## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 56800 on 2052 degrees of freedom}
\NormalTok{## Multiple R-squared:  0.483,  Adjusted R-squared:  0.482 }
\NormalTok{## F-statistic: 1.91e+03 on 1 and 2052 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

It's also important to understand if these coefficients are
statistically significant. In other words, can we state these
coefficients are statistically different then 0? To do that we can start
by assessing the standard error (SE). The SE for \(\beta_0\) and
\(\beta_1\) are computed with:

\begin{align}
\label{eq:coef-se}
SE(\beta_0)^2 = \sigma^2\bigg[\frac{1}{n}+\frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2} \bigg],
\quad SE(\beta_1)^2 = \frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
\end{align}

where \(\sigma^2 = Var(\epsilon)\). We see that our model results
provide the SE (\(SE_{\beta_1}=2.47\)). We can use the SE to compute the
95\% confidence interval for the coefficients:

\begin{equation}
\label{eq:coef-interval}
 \beta_1 \pm 2 \cdot SE(\beta_1)
\end{equation}

To get this information in R we can simply use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model1)}
\NormalTok{##               2.5 %  97.5 %}
\NormalTok{## (Intercept) 10117.1 25477.0}
\NormalTok{## Gr_Liv_Area   103.2   112.9}
\end{Highlighting}
\end{Shaded}

Our results show us that our 95\% confidence interval for \(\beta_1\)
(Gr\_Liv\_Area) is {[}103.191, 112.8779{]}. Thus, since zero is not in
this interval we can conclude that as the \texttt{Gr\_Liv\_Area}
increases by one square foot we can expect \texttt{Sale\_Price} to
increase by approximately \$103--\$113. This is also supported by the
\emph{t-statistic} provided by our results, which are computed by

\begin{equation}
\label{eq:tstat}
  t=\frac{\beta_1 - 0}{SE(\beta_1)}
\end{equation}

which measures the number of standard deviations that \(\beta_1\) is
away from 0. Thus a large \emph{t-statistic} such as ours will produce a
small \emph{p-value} (a small p-value indicates that it is unlikely to
observe such a substantial association between the predictor variable
and the response due to chance). Thus, we can conclude that a
relationship between \texttt{Gr\_Liv\_Area} and \texttt{Sale\_Price}
exists.

\hypertarget{multi-lm}{%
\section{Multiple linear regression}\label{multi-lm}}

However, in practice we often have more than one predictor. For example,
in the Ames housing data, we may wish to understand if above ground
square footage (\texttt{Gr\_Liv\_Area}) \emph{and} the year the house
was built (\texttt{Year\_Built}) are related to sales price
(\texttt{Sale\_Price}). We can extend the simple linear regression model
so that it can directly accommodate multiple predictors; this is
referred to as \emph{multiple linear regression} and is represented by
Equation \eqref{eq:mlm} and illustrated in Figure
\ref{fig:07-visualize-model2}.

\begin{equation}
\label{eq:mlm}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,
\end{equation}

where now \(X_1\) represents \texttt{Gr\_Liv\_Area} and \(X_2\)
represents \texttt{Year\_Built}.

\hypertarget{htmlwidget-03b1e4a11bebac26bef1}{}

\label{fig:07-visualize-model2}In a three-dimensional setting, with two
predictors and one response, the least squares regression line becomes a
plane. The `best-fit' plane minimizes the sum of squared errors between
the actual sales price (individual dots) and the predicted sales price
(plane).

This model in R is built by simply adding the \texttt{Year\_Built}
variable to our original model. The below results illustrate that the
best-fit plane identified in Figure \ref{fig:07-visualize-model2}
resulted in \(\hat \beta_1 = 91.73\) and \(\hat \beta_2 = 1098\). In
other words, according to this approximation, an additional one square
foot of above ground square footage is now associated with approximately
an additional \$92 in selling price when holding the year the house was
built constant. Likewise, for every year newer a home is there is
approximately an increase of \$1,098 in selling price when holding the
main floor square footage constant.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{summary}\NormalTok{(model2)}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = train)}
\NormalTok{## }
\NormalTok{## Residuals:}
\NormalTok{##     Min      1Q  Median      3Q     Max }
\NormalTok{## -440347  -26137   -2859   18098  310897 }
\NormalTok{## }
\NormalTok{## Coefficients:}
\NormalTok{##              Estimate Std. Error t value Pr(>|t|)    }
\NormalTok{## (Intercept) -2.12e+06   6.91e+04   -30.7   <2e-16 ***}
\NormalTok{## Gr_Liv_Area  9.17e+01   2.11e+00    43.6   <2e-16 ***}
\NormalTok{## Year_Built   1.10e+03   3.54e+01    31.0   <2e-16 ***}
\NormalTok{## ---}
\NormalTok{## Signif. codes:  }
\NormalTok{## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\NormalTok{## }
\NormalTok{## Residual standard error: 46900 on 2051 degrees of freedom}
\NormalTok{## Multiple R-squared:  0.648,  Adjusted R-squared:  0.647 }
\NormalTok{## F-statistic: 1.88e+03 on 2 and 2051 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

We can continue to add predictors to our multiple regression and
generalize the multiple regression model to Equation \eqref{eq:generalmlm}
where we have \emph{p} distinct predictors.

\begin{equation}
\label{eq:generalmlm}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon,
\end{equation}

Unfortunately, visualizing beyond three dimensions is not practical as
our best-fit plane becomes a hyperplane. However, the motivation remains
the same where the best-fit hyperplane is identified by minimizing the
RSS. The below creates a third model where we use all features in our
data set to predict \texttt{Sale\_Price}.

\begin{tip}
The dot in \texttt{Sale\_Price\ \textasciitilde{}\ .} signals to regress
\texttt{Sale\_Price} onto all other variables in your data set.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{glance}\NormalTok{(model3)}
\NormalTok{## # A tibble: 1 x 11}
\NormalTok{##   r.squared adj.r.squared  sigma statistic p.value}
\NormalTok{## *     <dbl>         <dbl>  <dbl>     <dbl>   <dbl>}
\NormalTok{## 1     0.941         0.931 20719.      96.9       0}
\NormalTok{## # ... with 6 more variables: df <int>, logLik <dbl>,}
\NormalTok{## #   AIC <dbl>, BIC <dbl>, deviance <dbl>,}
\NormalTok{## #   df.residual <int>}
\end{Highlighting}
\end{Shaded}

\hypertarget{assessing-model-accuracy}{%
\section{Assessing Model Accuracy}\label{assessing-model-accuracy}}

We've fit three models, but the question remains, which model is
``best''. To get a good assessment of this accuracy, we want to use
cross-validation as discussed in Section \ref{cv}. We can use the
\texttt{caret::train} function to apply a linear model
(\texttt{method\ =\ "lm"}). The benefit of \textbf{caret} is that it
provides built-in cross validation capabilities whereas the \texttt{lm}
function does not. The following shows an average root mean square error
(RMSE) of 56873 across our 10 cross validation folds. How should we
interpret this? When applied to unseen data, the predictions this model
makes are, on average, about \$56,873 off from the actual sale price.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproducible CV results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# use caret package to train 10-fold cross-validated model}
\NormalTok{cv_model1 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area, }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\NormalTok{cv_model1}
\NormalTok{## Linear Regression }
\NormalTok{## }
\NormalTok{## 2054 samples}
\NormalTok{##    1 predictor}
\NormalTok{## }
\NormalTok{## No pre-processing}
\NormalTok{## Resampling: Cross-Validated (10 fold) }
\NormalTok{## Summary of sample sizes: 1849, 1849, 1849, 1848, 1849, 1849, ... }
\NormalTok{## Resampling results:}
\NormalTok{## }
\NormalTok{##   RMSE   Rsquared  MAE  }
\NormalTok{##   56873  0.4884    38678}
\NormalTok{## }
\NormalTok{## Tuning parameter 'intercept' was held constant at}
\NormalTok{##  a value of TRUE}
\end{Highlighting}
\end{Shaded}

We can perform cross validation on the other two models. Extracting the
results for each model we see that by adding more information via more
predictors, we are able to improve the out-of-sample cross validation
performance metrics. Specifically, our average prediction RMSE reduces
from \textbf{\$56,872} down to \textbf{\$41,438} for our full model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{Gr_Liv_Area }\OperatorTok{+}\StringTok{ }\NormalTok{Year_Built, }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model3 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{model1 =}\NormalTok{ cv_model1, }
  \DataTypeTok{model2 =}\NormalTok{ cv_model2, }
  \DataTypeTok{model3 =}\NormalTok{ cv_model3}
\NormalTok{  )))}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{## summary.resamples(object = resamples(list(model1}
\NormalTok{##  = cv_model1, model2 = cv_model2, model3 = cv_model3)))}
\NormalTok{## }
\NormalTok{## Models: model1, model2, model3 }
\NormalTok{## Number of resamples: 10 }
\NormalTok{## }
\NormalTok{## MAE }
\NormalTok{##         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\NormalTok{## model1 35890   36624  38445 38678   40566 41819    0}
\NormalTok{## model2 29243   30263  31202 31401   31895 34505    0}
\NormalTok{## model3 14944   15852  17626 17701   18883 21980    0}
\NormalTok{## }
\NormalTok{## RMSE }
\NormalTok{##         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\NormalTok{## model1 49534   53294  56878 56873   58485 66579    0}
\NormalTok{## model2 39812   43132  46900 46886   49341 55657    0}
\NormalTok{## model3 21305   24403  46475 41438   53958 63247    0}
\NormalTok{## }
\NormalTok{## Rsquared }
\NormalTok{##          Min. 1st Qu. Median   Mean 3rd Qu.   Max.}
\NormalTok{## model1 0.3126  0.3966 0.5180 0.4884  0.5607 0.6351}
\NormalTok{## model2 0.5060  0.5996 0.6664 0.6492  0.7090 0.7416}
\NormalTok{## model3 0.5456  0.6429 0.7124 0.7524  0.9058 0.9250}
\NormalTok{##        NA's}
\NormalTok{## model1    0}
\NormalTok{## model2    0}
\NormalTok{## model3    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-concerns}{%
\section{Model concerns}\label{model-concerns}}

As previously stated, linear regression has been a popular modeling tool
due to the ease of interpreting the coefficients. However, linear
regression makes several strong assumptions that are often violated as
we include more predictors in our model. Violation of these assumptions
can lead to flawed interpretation of the coefficients and prediction
results.

\textbf{1. Linear relationship:} Linear regression assumes a linear
relationship between the predictor and the response variable. When a
linear relationship does not hold then the coefficient estimate makes a
flawed assumption that a constant relationship holds. However, as
discussed in Chapter \ref{descriptive}, non-linear relationships can be
made linear (or near-linear) by applying power transformations to the
response and/or predictor. For example, Figure
\ref{fig:07-linear-relationship} illustrates the relationship between
sale price and the year a home was built. The left plot illustrates the
non-linear relationship that exists. However, we can achieve a
near-linear relationship by log transforming sale price; although some
non-linearity still exists for older homes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(train, }\KeywordTok{aes}\NormalTok{(Year_Built, Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\StringTok{"Sale price"}\NormalTok{, }\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Year built"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Non-transformed variables with a }\CharTok{\textbackslash{}n}\StringTok{non-linear relationship."}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(train, }\KeywordTok{aes}\NormalTok{(Year_Built, Sale_Price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\StringTok{"Sale price"}\NormalTok{, }\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{400000}\NormalTok{, }\DataTypeTok{by =} \DecValTok{100000}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Year built"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Transforming variables can provide a }\CharTok{\textbackslash{}n}\StringTok{near-linear relationship."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/07-linear-relationship-1} 

}

\caption{Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s).}\label{fig:07-linear-relationship}
\end{figure}

\textbf{2. Constant variance among residuals:} Linear regression assumes
the variance among error terms
(\(\epsilon_1, \epsilon_2, ..., \epsilon_p\)) are constant (also
referred to as homoscedasticity). When residuals are not constant, the
\emph{p}-values and confidence intervals of the coefficients are invalid
resulting in invalid prediction estimates and confidence intervals.
Similar to the linear relationships assumption, non-constant variance
can often be resolved with variable transformations or by including
additional predictors. For example, Figure \ref{fig:07-homoskedasticity}
shows residuals across predicted values for our linear regression models
\texttt{model1} and \texttt{model3}. \texttt{model1} displays a classic
violation of constant variance with cone-shaped residuals. However,
\texttt{model3} appears to have near-constant variance.

\begin{tip}
The \texttt{broom::augment} function is an easy way to add model results
to each observation (i.e.~predicted values, residuals).
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{df1 <-}\StringTok{ }\KeywordTok{augment}\NormalTok{(cv_model1}\OperatorTok{$}\NormalTok{finalModel, train)}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(.fitted, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Predicted values"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 1"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Sale_Price ~ Gr_Liv_Area"}\NormalTok{)}

\NormalTok{df2 <-}\StringTok{ }\KeywordTok{augment}\NormalTok{(cv_model3}\OperatorTok{$}\NormalTok{finalModel, train)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df2, }\KeywordTok{aes}\NormalTok{(.fitted, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Predicted values"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 3"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Sale_Price ~ ."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/07-homoskedasticity-1} 

}

\caption{Linear regression assumes constant variance among the residuals. `model1` (left) shows definitive signs of heteroskedasticity whereas `model3` (right) appears to have constant variance.}\label{fig:07-homoskedasticity}
\end{figure}

\textbf{3. No autocorrelation:} Linear regression assumes the error
terms are also independent and uncorrelated. If in fact, there is
correlation among the error terms, then the estimated standard errors of
the coefficients will be biased leading to prediction intervals being
narrower than they should be. For example, the left plot in Figure
\ref{fig:07-autocorrelation} displays the residuals (y-axis) to the
observation ID (x-axis) for \texttt{model1}. A clear pattern exists
suggesting that information about \(\epsilon_1\) provides information
about \(\epsilon_2\).

This pattern is a result of the data being ordered by neighborhood,
which we have not accounted for in this model. Consequently, the
residuals for homes in the same neighborhood are correlated (homes
within a neighborhood are typically the same size and can often contain
similar features). Since the \texttt{Neighborhood} predictor is included
in \texttt{model3} (right plot), our errors are no longer correlated.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{df1 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(df1, }\DataTypeTok{id =} \KeywordTok{row_number}\NormalTok{())}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(df2, }\DataTypeTok{id =} \KeywordTok{row_number}\NormalTok{())}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(id, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Row ID"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 1"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Correlated residuals."}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(df2, }\KeywordTok{aes}\NormalTok{(id, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Row ID"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Residuals"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Model 3"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Uncorrelated residuals."}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/07-autocorrelation-1} 

}

\caption{Linear regression assumes uncorrelated errors. The residuals in `model1` (left) have a distinct pattern suggesting that information about $\epsilon_1$ provides information about $\epsilon_2$.  Whereas residuals in `model3` have no signs of autocorrelation.}\label{fig:07-autocorrelation}
\end{figure}

\textbf{4. More observations than predictors:} Although not an issue
with the Ames housing data, when the number of features exceed the
number of observations (\(p > n\)), the OLS solution matrix is not
invertible. This causes significant issues because it means the
least-squares estimates are not unique. In fact, there are an infinite
set of solutions available so we lose our ability to meaningfully
interpret coefficients. Consequently, to resolve this issue an analyst
can remove variables until \(p < n\) and then fit an OLS regression
model. Although an analyst can use pre-processing tools to guide this
manual approach \citep[43-47]{apm}, it can be cumbersome and prone to
errors. Alternatively, we will introduce regularized regression in
Chapter \ref{regularize} which provides you an alternative linear
regression technique when \(p > n\).

\textbf{5. No or little multicollinearity:} \emph{Collinearity} refers
to the situation in which two or more predictor variables are closely
related to one another. The presence of collinearity can pose problems
in the regression context, since it can be difficult to separate out the
individual effects of collinear variables on the response. In fact,
collinearity can cause predictor variables to appear as statistically
insignificant when in fact they are significant. This, obviously, leads
to inaccurate interpretation of coefficients and identifying influential
predictors.

For example, in our data, \texttt{Garage\_Area} and
\texttt{Garage\_Cars} are two variables that have a correlation of 0.89
and both variables are strongly correlated to our response variable
(\texttt{Sale\_Price}). Looking at our full model where both of these
variables are included, we see that \texttt{Garage\_Area} is found to be
statistically significant but \texttt{Garage\_Cars} is not.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit with two strongly correlated variables}
\KeywordTok{summary}\NormalTok{(cv_model3) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Garage_Area"}\NormalTok{, }\StringTok{"Garage_Cars"}\NormalTok{))}
\NormalTok{## # A tibble: 2 x 5}
\NormalTok{##   term        estimate std.error statistic  p.value}
\NormalTok{##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>}
\NormalTok{## 1 Garage_Cars   2131.    1761.        1.21 0.226   }
\NormalTok{## 2 Garage_Area     19.5      5.88      3.31 0.000939}
\end{Highlighting}
\end{Shaded}

However, if we refit the full model without \texttt{Garage\_Area}, the
coefficient estimate for \texttt{Garage\_Cars} increases three fold and
becomes statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# model without Garage_Area}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{mod_wo_Garage_Area <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =} \KeywordTok{select}\NormalTok{(train, }\OperatorTok{-}\NormalTok{Garage_Area), }
  \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{summary}\NormalTok{(mod_wo_Garage_Area) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{==}\StringTok{ "Garage_Cars"}\NormalTok{)}
\NormalTok{## # A tibble: 1 x 5}
\NormalTok{##   term        estimate std.error statistic     p.value}
\NormalTok{##   <chr>          <dbl>     <dbl>     <dbl>       <dbl>}
\NormalTok{## 1 Garage_Cars    6342.     1222.      5.19 0.000000236}
\end{Highlighting}
\end{Shaded}

This reflects the instability in the linear regression model caused by
the between-predictor relationships and this instability gets propagated
directly to the model predictions. Considering 16 of our 34 numeric
predictors have medium to strong correlation (Section \ref{pca}), the
biased coefficients of these predictors are likely restricting the
predictive accuracy of our model. How can we control for this problem?
One option is to manually remove one of the offending predictors.
However, when the number of predictors is large such as in our case,
this becomes difficult. Moreover, relationships between predictors can
become complex and involve many predictors. In these cases, manual
removal of specific predictors may not be possible. Consequently, the
following sections offers two simple extensions of linear regression
where dimension reduction is applied prior to performing linear
regression. Chapter \ref{regularize} offers a modified regression
approach that helps to deal with the problem. And future chapters
provide alternative methods that are not effected by multicollinearity.

\hypertarget{principal-component-regression}{%
\section{Principal component
regression}\label{principal-component-regression}}

As discussed in Chapter \ref{unsupervised}, principal components
analysis can be used to represent correlated variables in a lower
dimension and the resulting components can be used as predictors in the
linear regression model. This two-step process is known as
\textbf{principal component regression} (PCR)
\citep{massy1965principal}.

Performing PCR with \textbf{caret} is an easy extension from our
previous model. We simply change the \texttt{method} to ``pcr'' within
\texttt{train} to perform PCA on all our numeric predictors prior to
applying the multiple regression. Often, we can greatly improve
performance by only using a small subset of all principal components as
predictors. Consequently, you can think of the number of principal
components as a tuning parameter (see Section \ref{tune}). The following
performs cross validated PCR with \(1, 2, \dots, 20\) principal
components, and Figure \ref{fig:pcr-regression} illustrates the
cross-validated RMSE. You can see a significant drop in prediction error
using just five principal components followed by a gradual decrease.
Using 17 principal components provided the lowest RMSE of \$35,769.99
(see \texttt{cv\_model\_pcr} for a comparison of the cross-validated
results).

\begin{warning}
Per Section \ref{pca}, don't forget to center and scale your predictors,
which you can do by incorporating the \texttt{preProcess} argument.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{# perform 10-fold cross validation on a PCR model tuning the number of}
\CommentTok{# principal components to use as predictors from 1-20}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pcr <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"pcr"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{20}
\NormalTok{  )}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_model_pcr}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##    ncomp}
\NormalTok{## 17    17}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{plot}\NormalTok{(cv_model_pcr)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pcr-regression-1} 

}

\caption{The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components.}\label{fig:pcr-regression}
\end{figure}

By controlling for multicollinearity with PCR, we saw significant
improvement in our predictive accuary (reducing out-of-sample RMSE from
41438 down to 35770). However, since PCR is a two step process, the PCA
step does not consider any aspects of the response when it selects the
components. Consequently, the new predictors produced by the PCA step
are not designed to maximize the relationship with the response.
Instead, it simply seeks to reduce the variability present throughout
the predictor space. If that variability happens to be related to the
response variability, then PCR has a good chance to identify a
predictive relationship, as in our case. If, however, the variability in
the predictor space is not related to the variability of the response,
then PCR can have difficulty identifying a predictive relationship when
one might actually exists (i.e.~we may actually experience a decrease in
our predictive accuracy). Thus, an alternative approach to reduce the
impact of multicollinearity is partial least squares.

\hypertarget{partial-least-squares}{%
\section{Partial least squares}\label{partial-least-squares}}

\emph{Partial least squares (PLS)} can be viewed as a supervised
dimension reduction procedure \citep{apm}. Similar to PCR this technique
also constructs a set of linear combinations of the inputs for
regression, but unlike PCR it uses the response variable to aid the
construction of the principal components. Thus, we can think of PLS as a
supervised dimension reduction procedure that finds new features that
not only appromxate the old features well, but also that are related to
the response.

\textbf{TODO: IMAGE of PCR vs PLS}

Referring back to Equation \eqref{eq:pca1}, PLS will compute the first
principal (\(z_1\)) by setting each \(\phi_{j1}\) to the coefficient
from a simple linear regression model of \(y\) onto that respective
\(x_j\). One can show that this coefficient is proportional to the
correlation between \(y\) and \(x_j\). Hence, in computing
\(z_1 = \sum^p_{j=1} \phi_{j1}x_j\), PLS places the highest weight on
the variables that are most strongly related to the response.

To compute the second principal (\(z_2\)), we first regress each
variable on \(z_1\). The residuals from this regression captures the
remaining signal that has not been explained by the first principal. We
substitute these residual values for the predictor values in Equation
\eqref{eq:pca2}. This process continues until all \(m\) components have
been computed and then we use OLS to regress the response on
\(z_1, \dots, z_m\).

\begin{note}
See @friedman2001elements and @geladi1986partial for a thorough
discussion of PLS.
\end{note}

Similar to PCR, we can easily fit a PLS model by changing the
\texttt{method} argument in \texttt{caret::train}. As with PCR, the
number of principal components to use is a tuning parameter that is
determined by the model that maximize predictive accuracy (minimizes
RMSE in this case). The following performs cross validated PLS with
\(1, 2, \dots, 20\) principal components, and Figure
\ref{fig:pls-regression} illustrates the cross-validated RMSE. You can
see a greater drop in prediction error than PCR. Using PLS with
\(m = 10\) principal components provided the lowest RMSE of \$31,522.47.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform 10-fold cross validation on a PLS model tuning the number of}
\CommentTok{# principal components to use as predictors from 1-20}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pls <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{20}
\NormalTok{  )}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_model_pls}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##    ncomp}
\NormalTok{## 10    10}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{plot}\NormalTok{(cv_model_pls)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pls-regression-1} 

}

\caption{The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components.}\label{fig:pls-regression}
\end{figure}

\hypertarget{lm-model-interp}{%
\section{Feature Interpretation}\label{lm-model-interp}}

Once we've found the model that minimizes the predictive accuracy, our
next goal is to interpret the model structure. Linear regression models
provide a very intuitive model structure as they assume a
\textbf{\emph{monotonic linear relationship}} between the predictor
variables and the response. The \emph{linear} relationship part of that
statement just means, for a given predictor variable, it assumes for
every one unit change in a given predictor variable there is a constant
change in the response. As discussed earlier in the chapter, this
constant change is provided by the given coefficient for a predictor.
The \emph{monotonic} relationship means that a given predictor variable
will always have a positive or negative relationship. But how do we
determine the most influential variables?

Variable importance seeks to identify those variables that are most
influential in our model. For linear regression models, this is most
often measured by the absolute value of the \emph{t}-statistic (Equation
\eqref{eq:tstat}) for each model parameter used. For a PLS model, variable
importance is based on weighted sums of the absolute regression
coefficients. The weights are a function of the reduction of the RSS
across the number of PLS components and are computed separately for each
outcome. Therefore, the contribution of the coefficients are weighted
proportionally to the reduction in the RSS.

We can use \texttt{vip::vip} to extract and plot the most important
variables. The importance measure is normalized from 100 (most
important) to 0 (least important). Figure \ref{fig:pls-vip} illustrates
that the top 4 most important variables are \texttt{Gr\_liv\_Area},
\texttt{First\_Flr\_SF}, \texttt{Garage\_Area}, and
\texttt{Garage\_Cars} respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(cv_model_pls, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pls-vip-1} 

}

\caption{Top 20 most important variables for the PLS model.}\label{fig:pls-vip}
\end{figure}

As stated earlier, linear regression models assume a monotonic linear
relationship. To illustrate this, we can apply partial dependence plots
(PDPs). PDPs plot the change in the average predicted value (\(\hat y\))
as specified feature(s) vary over their marginal distribution. As you
will see in later chapters, PDPs become more useful when non-linear
relationships are present. However, PDPs of linear models help
illustrate how a fixed change in \(x_i\) relates to a fixed linear
change in \(\hat y_i\).

\begin{tip}
The \textbf{pdp} package {[}@pkg-pdp{]} provides convenient functions
for computing and plotting PDPs. For example, the following code chunk
would plot the PDP for the \texttt{Gr\_Liv\_Area} predictor.

\texttt{pdp::partial(cv\_model\_pls,\ pred.var\ =\ "Gr\_Liv\_Area",\ grid.resolution\ =\ 20)\ \%\textgreater{}\%\ autoplot()}
\end{tip}

All four of the most important predictors have a positive relationship
with sale price; however, we see that the slope (\(\beta_i\)) is
steepest for the most important predictor and gradually decreases for
lessor important variables.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/unnamed-chunk-79-1} 

}

\caption{Partial dependence plots for the first four most important variables.}\label{fig:unnamed-chunk-79}
\end{figure}

\hypertarget{final-thoughts}{%
\section{Final thoughts}\label{final-thoughts}}

Linear regression is a great starting point in learning more advanced
predictive analytic approaches because, in its simplest form, it is very
intuitive and easy to interpret. Training a linear regression model is
very easy and computationally efficient. However, due to the many
assumptions required, the disadvantages of linear regression often
outweigh their benefits. In our example, we saw how multicollinearity
was interferring with predictive accuracy. By controlling
multicollinearity with PCR and PLS we were able to improve predictive
accuracy. Later chapters will build on the concepts illustrated in this
chapter and will compare cross-validated performance results to identify
the best predictive model. The following summarizes some of the
advantages and disadvantages discussed regarding linear regression.

\textbf{FIXME: refine this section}

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Normal linear regression has no hyperparameters to tune and PCR and
  PLS have only one hyperparameter to tune; making these methods very
  simple to train.
\item
  Computationally efficient - relatively fast compared to other
  algorithms in this book and does not require large memory.
\item
  Easy to interpret results.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Makes strong asssumptions about the data.
\item
  Does not handle missing data - must impute or remove observations with
  missing values.
\item
  Not robust to outliers as they can still bias the coefficients.
\item
  Assumes relationships between predictors and response variable to be
  monotonic linear (always increasing or decreasing in a linear
  fashion).
\item
  Typically does not perform as well as more advanced methods that allow
  non-monotonic and non-linear relationships (i.e.~random forests,
  gradient boosting machines, neural networks).
\item
  Most large data sets violate one of the several assumptions made for
  linear regression to hold, which cause instability in the modeling
  results.
\end{itemize}

\hypertarget{learning-more}{%
\section{Learning more}\label{learning-more}}

This will get you up and running with linear regression. Keep in mind
that there is a lot more you can dig into so the following resources
will help you learn more:

\begin{itemize}
\tightlist
\item
  \href{http://www-bcf.usc.edu/~gareth/ISL/}{An Introduction to
  Statistical Learning}
\item
  \href{http://appliedpredictivemodeling.com/}{Applied Predictive
  Modeling}
\item
  \href{https://statweb.stanford.edu/~tibs/ElemStatLearn/}{Elements of
  Statistical Learning}
\end{itemize}

\hypertarget{logistic-regression}{%
\chapter{Logistic regression}\label{logistic-regression}}

Linear regression is used to approximate the relationship between a
continuous response variable and a set of predictor variables. However,
when the response variable is categorical rather than continuous, linear
regression is not appropriate. Fortunately, analysts can turn to an
analogous method, \emph{logistic regression}, which is similar to linear
regression in many ways. This chapter explores the use of logistic
regression for binary response variables. Logistic regression can be
expanded for multinomial problems (see \citet{faraway2016extending} for
discussion of multinomial logistic regression in R); however, that goes
beyond our intent here.

\hypertarget{prerequisites-4}{%
\section{Prerequisites}\label{prerequisites-4}}

For this section we will use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# data manipulation & visualization}
\KeywordTok{library}\NormalTok{(rsample)    }\CommentTok{# data splitting}
\KeywordTok{library}\NormalTok{(caret)      }\CommentTok{# logistic regression modeling}
\KeywordTok{library}\NormalTok{(vip)        }\CommentTok{# variable importance}
\end{Highlighting}
\end{Shaded}

To illustrate logistic regression concepts we will use the employee
attrition data, where our intent is to predict the \texttt{Attrition}
response variable (``Yes''\textbar{}``no''). As in the previous chapter,
we'll set aside 30\% of our data as a test set to assess our
generalizability error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the rsample::attrition data.}
\CommentTok{# Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{why-logistic-regression}{%
\section{Why logistic regression}\label{why-logistic-regression}}

To provide a clear motivation of logistic regression, assume we have
credit card default data for customers and we want to understand if the
credit card balance the customer has is an indicator of whether or not
the customer will default on their credit card. To classify a customer
as a high- vs.~low-risk defaulter based on their balance we could use
linear regression; however, the left plot in Figure \ref{fig:whylogit}
illustrates how linear regression would predict the probability of
defaulting. Unfortunately, for balances close to zero we predict a
negative probability of defaulting; if we were to predict for very large
balances, we would get values bigger than 1. These predictions are not
sensible, since of course the true probability of defaulting, regardless
of credit card balance, must fall between 0 and 1. Contrast this with
the logistic regression line (right plot) that is nonlinear
(sigmoidal-shaped).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/whylogit-1} 

}

\caption{Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1.}\label{fig:whylogit}
\end{figure}

To avoid the inadequecies of the linear model fit on a binary response,
we must model the probability of our response using a function that
gives outputs between 0 and 1 for all values of \(X\). Many functions
meet this description. In logistic regression, we use the logistic
function, which is defined in Equation \eqref{eq:logistic} and produces
the S-curve in the right plot above.

\begin{equation}
\label{eq:logistic}
  p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\end{equation}

The \(\beta_i\) parameters represent the coefficients as in linear
regression and \(p(x)\) may be interpreted as the probability that the
positive class (default in the above example) is present. The minimum
for \(p(x)\) is obtained at
\(\text{lim}_{a \rightarrow -\infty} \big[ \frac{e^a}{1+e^a} \big] = 0\),
and the maximium for \(p(x)\) is obtained at
\(\text{lim}_{a \rightarrow \infty} \big[ \frac{e^a}{1+e^a} \big] = 1\)
which restricts the output probabilities to 0-1. Furthermore, a useful
transformation for logistic regression is the \emph{logit
transformation} with follows:

\begin{equation}
\label{eq:logit}
  g(X) = \text{ln} \bigg[ \frac{p(x)}{1 - p(x)} \bigg] = \beta_0 + \beta_1x
\end{equation}

The logit transformation exhibits several attractive properties of the
linear regression model such as its linearity and interpretability,
which we will come back to shortly.

\hypertarget{simple-logistic-regression}{%
\section{Simple logistic regression}\label{simple-logistic-regression}}

We will fit two logistic regression models in order to predict the
probability of an employee attriting. The first predicts the probability
of attrition based on their monthly income (\texttt{MonthlyIncome}) and
the second is based on whether or not the employee works overtime
(\texttt{OverTime}). The \texttt{glm} function fits generalized linear
models, a class of models that includes logistic regression. The syntax
of the \texttt{glm} function is similar to that of \texttt{lm}, except
that we must pass the argument \texttt{family\ =\ binomial} in order to
tell R to run a logistic regression rather than some other type of
generalized linear model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ train)}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{OverTime, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

In the background \texttt{glm}, uses \emph{maximum likelihood} to fit
the model. The basic intuition behind using maximum likelihood to fit a
logistic regression model is as follows: we seek estimates for
\(\beta_0\) and \(\beta_1\) such that the predicted probability
\(\hat p(x_i)\) of attrition for each employee corresponds as closely as
possible to the employee's observed attrition status. In other words, we
try to find \(\hat \beta_0\) and \(\hat \beta_1\) such that plugging
these estimates into the model for \(p(x)\) (Equation \eqref{eq:logistic})
yields a number close to one for all employees who attrited, and a
number close to zero for all employees who did not. This intuition can
be formalized using a mathematical equation called a \emph{likelihood
function}:

\begin{equation}
\label{eq:max-like}
  \ell(\beta_0, \beta_1) = \prod_{i:y_i=1}p(x_i) \prod_{i':y_i'=0}(1-p(x_i'))
\end{equation}

The estimates \(\beta_0\) and \(\beta_1\) are chosen to \emph{maximize}
this likelihood function. Maximum likelihood is a very general approach
that is used to fit many of the non-linear models that we will examine
in future chapters. What results is the predicted probability of
attrition. Figure \ref{fig:glm-sigmoid} illustrates the predicted
probablities for the two models.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glm-sigmoid-1} 

}

\caption{Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, `model1` predicts a decreased probability of attrition and if employees work overtime `model2` predicts an increased probability.}\label{fig:glm-sigmoid}
\end{figure}

The below table shows the coefficient estimates and related information
that result from fitting a logistic regression model in order to predict
the probability of \emph{Attrition = Yes} for our two models. Bear in
mind that the coefficient estimates from logistic regression
characterize the relationship between the predictor and response
variable on a \emph{log-odds} scale.

Thus, we see that the \texttt{MonthlyIncome} \(\hat \beta_1 =\)
-1.1446\times 10\^{}\{-4\}. This indicates that an increase in
\texttt{MonthlyIncome} is associated with a decrease in the probability
of attrition. To be precise, a one-unit increase in
\texttt{MonthlyIncome} is associated with a decrease in the log odds of
attrition by -1.1446\times 10\^{}\{-4\} units. Similarly for
\texttt{model2}, an employee that works \texttt{OverTime} has an
increase of 1.3076 logg odds of attrition.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(model1)}
\NormalTok{## # A tibble: 2 x 5}
\NormalTok{##   term           estimate std.error statistic  p.value}
\NormalTok{##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>}
\NormalTok{## 1 (Intercept)   -0.984    0.152         -6.47 9.62e-11}
\NormalTok{## 2 MonthlyIncome -0.000114 0.0000244     -4.69 2.74e- 6}
\KeywordTok{tidy}\NormalTok{(model2)}
\NormalTok{## # A tibble: 2 x 5}
\NormalTok{##   term        estimate std.error statistic  p.value}
\NormalTok{##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>}
\NormalTok{## 1 (Intercept)    -2.14     0.120    -17.9  2.21e-71}
\NormalTok{## 2 OverTimeYes     1.31     0.175      7.47 8.03e-14}
\end{Highlighting}
\end{Shaded}

Taking an exponential transformation of these coefficients converts them
from log odds to odds. Furthermore, we can convert odds to a probability
with \(\text{probability} = \frac{odds}{1 + odds}\) Thus, for every one
dollar increase in \texttt{MonthlyIncome}, the odds of an employee
attriting decreases slightly, represented by a slightly less than 50\%
probability. Whereas an employee that works \texttt{OverTime} has nearly
4-1 odds of attriting over an employee that does not work
\texttt{OverTime}, represented by an increased probability of 78.7\%.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert to odds}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model1))}
\NormalTok{##   (Intercept) MonthlyIncome }
\NormalTok{##        0.3740        0.9999}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model2))}
\NormalTok{## (Intercept) OverTimeYes }
\NormalTok{##      0.1178      3.6974}

\CommentTok{# convert to probability}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model1)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model1)))}
\NormalTok{##   (Intercept) MonthlyIncome }
\NormalTok{##        0.2722        0.5000}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model2)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(model2)))}
\NormalTok{## (Intercept) OverTimeYes }
\NormalTok{##      0.1054      0.7871}
\end{Highlighting}
\end{Shaded}

Many aspects of the coefficient output are similar to those discussed in
the linear regression output. For example, we can measure the confidence
intervals and accuracy of the coefficient estimates by computing their
standard errors. For instance, both models's \(\hat \beta_1\) have a
p-value \textless{} 0.05 suggesting a strong probability that a
relationship between these predictors and the probability of attrition
exists. We can also use the standard errors to get confidence intervals
as we did in the linear regression tutorial:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model1)}
\NormalTok{##                    2.5 %     97.5 %}
\NormalTok{## (Intercept)   -1.2812812 -0.6848677}
\NormalTok{## MonthlyIncome -0.0001648 -0.0000689}
\KeywordTok{confint}\NormalTok{(model2)}
\NormalTok{##               2.5 % 97.5 %}
\NormalTok{## (Intercept) -2.3808 -1.911}
\NormalTok{## OverTimeYes  0.9653  1.652}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-logistic-regression}{%
\section{Multiple logistic
regression}\label{multiple-logistic-regression}}

We can also extend our model as seen in Eq. 1 so that we can predict a
binary response using multiple predictors where \(X = (X_1,\dots, X_p)\)
are \emph{p} predictors:

\begin{equation}
\label{eq:multi-logistic}
p(X) = \frac{e^{\beta_0 + \beta_1X + \cdots + \beta_pX_p }}{1 + e^{\beta_0 + \beta_1X + \cdots + \beta_pX_p}} 
\end{equation}

Let's go ahead and fit a model that predicts the probability of
\texttt{Attrition} based on the \texttt{MonthlyIncome} and
\texttt{OverTime}. Our results show that both features are statistically
significant and Figure \ref{fig:glm-sigmoid2} illustrates common trends
between \texttt{MonthlyIncome} and \texttt{Attrition}; however, working
\texttt{OverTime} tends to nearly double the probability of attrition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome }\OperatorTok{+}\StringTok{ }\NormalTok{OverTime, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{tidy}\NormalTok{(model3)}
\NormalTok{## # A tibble: 3 x 5}
\NormalTok{##   term           estimate std.error statistic  p.value}
\NormalTok{##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>}
\NormalTok{## 1 (Intercept)   -1.44     0.173         -8.32 9.00e-17}
\NormalTok{## 2 MonthlyIncome -0.000124 0.0000254     -4.88 1.06e- 6}
\NormalTok{## 3 OverTimeYes    1.36     0.179          7.61 2.75e-14}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glm-sigmoid2-1} 

}

\caption{Predicted probability of attrition based on monthly income and whether or not employees work overtime.}\label{fig:glm-sigmoid2}
\end{figure}

\hypertarget{assessing-model-accuracy-1}{%
\section{Assessing model accuracy}\label{assessing-model-accuracy-1}}

With a basic understanding of logistic regression under our belt,
similar to linear regression our concern now shifts to how well do our
models predict. As in the last chapter, we will use
\texttt{caret::train} and fit three 10-fold cross validated logistic
regression models. Extracting the accuracy measures, we see that both
\texttt{cv\_model1} and \texttt{cv\_model2} had an average accuracy of
83.89\%. However, \texttt{cv\_model3} which used all predictor variables
in our data achieved an average accuracy rate of 86.3\%.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model1 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome, }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{MonthlyIncome }\OperatorTok{+}\StringTok{ }\NormalTok{OverTime, }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model3 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{model1 =}\NormalTok{ cv_model1, }
  \DataTypeTok{model2 =}\NormalTok{ cv_model2, }
  \DataTypeTok{model3 =}\NormalTok{ cv_model3}
\NormalTok{  )))}\OperatorTok{$}\NormalTok{statistics}\OperatorTok{$}\NormalTok{Accuracy}
\NormalTok{##          Min. 1st Qu. Median   Mean 3rd Qu.   Max.}
\NormalTok{## model1 0.8350  0.8353 0.8365 0.8389  0.8431 0.8447}
\NormalTok{## model2 0.8350  0.8353 0.8365 0.8389  0.8431 0.8447}
\NormalTok{## model3 0.8058  0.8389 0.8586 0.8632  0.8949 0.9135}
\NormalTok{##        NA's}
\NormalTok{## model1    0}
\NormalTok{## model2    0}
\NormalTok{## model3    0}
\end{Highlighting}
\end{Shaded}

We can get greater understanding of our model's performance by assessing
the confusion matrix (see section \ref{reg-perf-eval}). We can use
\texttt{train::confusionMatrix} to compute a confusion matrix. We need
to supply our model's predicted class and the actuals from our trainin
data. Our confusion matrix provides a host of information. Particularly,
we can see that although we do well predicting cases of non-attrition
(note the high specificity), our model does particularly poor predicting
actual cases of attrition (note the low sensitivity).

\begin{tip}
By default the \texttt{predict} function predicts the response class for
a \textbf{caret} model; however, you can change the \texttt{type}
argument to predict the probabilities (see \texttt{?predict.train}).
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict class}
\NormalTok{pred_class <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model3, train)}

\CommentTok{# create confusion matrix}
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{relevel}\NormalTok{(pred_class, }\DataTypeTok{ref =} \StringTok{"Yes"}\NormalTok{), }\KeywordTok{relevel}\NormalTok{(train}\OperatorTok{$}\NormalTok{Attrition, }\DataTypeTok{ref =} \StringTok{"Yes"}\NormalTok{))}
\NormalTok{## Confusion Matrix and Statistics}
\NormalTok{## }
\NormalTok{##           Reference}
\NormalTok{## Prediction Yes  No}
\NormalTok{##        Yes  79  33}
\NormalTok{##        No   87 831}
\NormalTok{##                                         }
\NormalTok{##                Accuracy : 0.883         }
\NormalTok{##                  95% CI : (0.862, 0.902)}
\NormalTok{##     No Information Rate : 0.839         }
\NormalTok{##     P-Value [Acc > NIR] : 3.05e-05      }
\NormalTok{##                                         }
\NormalTok{##                   Kappa : 0.504         }
\NormalTok{##  Mcnemar's Test P-Value : 1.31e-06      }
\NormalTok{##                                         }
\NormalTok{##             Sensitivity : 0.4759        }
\NormalTok{##             Specificity : 0.9618        }
\NormalTok{##          Pos Pred Value : 0.7054        }
\NormalTok{##          Neg Pred Value : 0.9052        }
\NormalTok{##              Prevalence : 0.1612        }
\NormalTok{##          Detection Rate : 0.0767        }
\NormalTok{##    Detection Prevalence : 0.1087        }
\NormalTok{##       Balanced Accuracy : 0.7189        }
\NormalTok{##                                         }
\NormalTok{##        'Positive' Class : Yes           }
\NormalTok{## }
\end{Highlighting}
\end{Shaded}

One thing to point out, in the confusion matrix above you will note the
metric \texttt{No\ Information\ Rate:\ 0.8388}. This represents the
ratio of non-attrition versus attrition in our trainin data
(\texttt{table(train\$Attrition)\ \%\textgreater{}\%\ prop.table()}).
Consequently, if we simply predicted ``No'' for every employee we would
still get an accuracy rate of 83.88\%. Therefore, our goal is to
maximize our accuracy rate over and above this no information benchmark
while also trying to balance sensitivity and specificity. To understand
how well we are achieving this we can visualize the ROC curve (section
\ref{reg-perf-eval}). If we compare our simple model
(\texttt{cv\_model1}) to our full model \texttt{cv\_model3}, we can see
that we the lift achieved with the more accurate model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ROCR)}

\CommentTok{# create predicted probabilities}
\NormalTok{m1_prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model1, train, }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{)}\OperatorTok{$}\NormalTok{Yes}
\NormalTok{m3_prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_model3, train, }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{)}\OperatorTok{$}\NormalTok{Yes}

\CommentTok{# compute AUC metrics for cv_model1 and cv_model3}
\NormalTok{perf1 <-}\StringTok{ }\KeywordTok{prediction}\NormalTok{(m1_prob, train}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{performance}\NormalTok{(}\DataTypeTok{measure =} \StringTok{"tpr"}\NormalTok{, }\DataTypeTok{x.measure =} \StringTok{"fpr"}\NormalTok{)}

\NormalTok{perf2 <-}\StringTok{ }\KeywordTok{prediction}\NormalTok{(m3_prob, train}\OperatorTok{$}\NormalTok{Attrition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{performance}\NormalTok{(}\DataTypeTok{measure =} \StringTok{"tpr"}\NormalTok{, }\DataTypeTok{x.measure =} \StringTok{"fpr"}\NormalTok{)}

\CommentTok{# plot both ROC curves for cv_model1 and cv_model3}
\KeywordTok{plot}\NormalTok{(perf1, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(perf2, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}

\KeywordTok{legend}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"cv_model1"}\NormalTok{, }\StringTok{"cv_model3"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/unnamed-chunk-82-1}

\}

\textbackslash{}caption\{ROC curve for \texttt{cv\_model1} and
\texttt{cv\_model3}. The increase in the AUC represents the `lift' that
we achieve with \texttt{cv\_model3}.\}\label{fig:unnamed-chunk-82}
\textbackslash{}end\{figure\}

Similar to linear regression, we can perform a PLS logistic regression
to assess if reducing the dimension of our numeric predictors helps to
achieve improved accuracy. There are 16 numeric features in our data set
so the following performs a 10-fold cross-validated PLS model while
tuning the number of principal components to use from 1-16. The optimal
model uses 14 principal components, which is not reducing the dimension
by much. However, the mean accuracy of 0.866 was only marginally better
than the average CV accuracy of \texttt{cv\_model3} (0.863), likely
within the margin of error.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform 10-fold cross validation on a PLS model tuning the number of}
\CommentTok{# principal components to use as predictors from 1-20}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{cv_model_pls <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{16}
\NormalTok{  )}

\CommentTok{# model with lowest RMSE}
\NormalTok{cv_model_pls}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##    ncomp}
\NormalTok{## 14    14}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{plot}\NormalTok{(cv_model_pls)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pls-logistic-regression-1} 

}

\caption{The 10-fold cross valdation RMSE obtained using PLS with 1-16 principal components.}\label{fig:pls-logistic-regression}
\end{figure}

\hypertarget{feature-interpretation}{%
\section{Feature interpretation}\label{feature-interpretation}}

Similar to linear regression, once our preferred logistic regression
model is identified, next we need to interpret how the features are
influencing the results. As with normal linear regression models,
variable importance for logistic regression models are computed with the
absolute value of the \emph{t}-statistic for each model parameter is
used. Using \texttt{vip} we can extract our top 20 influential
variables. Figure \ref{fig:glm-vip} illustrates that \texttt{OverTime}
is the most influential followed by \texttt{JobSatisfaction},
\texttt{NumCompaniesWorked}, and \texttt{EnvironmentSatisfaction}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(cv_model3, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glm-vip-1} 

}

\caption{Top 20 most important variables for the PLS model.}\label{fig:glm-vip}
\end{figure}

Similar to linear regression, logistic regression assumes a monotonic
linear relationship. However, the linear relationship is in the form of
a log-odds probability; therefore, the regular probability relationship
will have a curvilinear effect. This is illustrated in Figure
\ref{fig:glm-pdp} by the change in predicted probability of attrition
associated with the marginal change in the number of companies an
employee has work for (\texttt{NumCompaniesWorked}). Employees that have
experienced more employment changes tend to have a high probability of
making another future change.

Furthermore, the partial dependence plots for the three top categorical
predictors (\texttt{OverTime}, \texttt{JobSatisfaction}, and
\texttt{EnvironmentSatisfaction}) illustrate the change in predicted
probability of attrition based on the employee's status for each
predictor.

\begin{tip}
See the supplemental material at
{[}\url{https://github.com/koalaverse/abar}{]}(\url{https://github.com/koalaverse/abar}{]}
for the code to produce the following plots.
\end{tip}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glm-pdp-1} 

}

\caption{Partial dependence plots for the first four most important variables.  We can see how the predicted probability of attrition changes for each value of the influential predictors.}\label{fig:glm-pdp}
\end{figure}

\hypertarget{final-thoughts-1}{%
\section{Final thoughts}\label{final-thoughts-1}}

Logistic regression is a natural starting point for learning predictive
models for classification purposes due to its similarity to linear
regression. Later chapters will build on the concepts illustrated in
this chapter and will compare cross-validated performance results to
identify the best predictive model for our employee attrition problem.
The following summarizes some of the advantages and disadvantages
discussed regarding logistic regression.

\textbf{FIXME: refine this section}

\textbf{Advantages}:

\textbf{Disadvantages}:

\hypertarget{learning-more-1}{%
\section{Learning more}\label{learning-more-1}}

This will get you up and running with logistic regression. Keep in mind
that there is a lot more you can dig into so the following resources
will help you learn more:

\begin{itemize}
\tightlist
\item
  \href{http://www-bcf.usc.edu/~gareth/ISL/}{An Introduction to
  Statistical Learning}
\item
  \href{http://appliedpredictivemodeling.com/}{Applied Predictive
  Modeling}
\item
  \href{https://statweb.stanford.edu/~tibs/ElemStatLearn/}{Elements of
  Statistical Learning}
\end{itemize}

\hypertarget{regularized-regression}{%
\chapter{Regularized regression}\label{regularized-regression}}

Generalized linear models (GLMs) such as ordinary least squares
regression and logistic regression are simple and fundamental approaches
for supervised learning. Moreover, when the assumptions required by GLMs
are met, the coefficients produced are unbiased and, of all unbiased
linear techniques, have the lowest variance. However, in today's world,
data sets being analyzed typically have a large amount of features. As
the number of features grow, our GLM assumptions typically break down
and our models often overfit (aka have high variance) to the training
sample, causing our out of sample error to increase.
\textbf{\emph{Regularization}} methods provide a means to control our
coefficients, which can reduce the variance and decrease out of sample
error.

\hypertarget{prerequisites-5}{%
\section{Prerequisites}\label{prerequisites-5}}

This chapter leverages the following packages. Most of these packages
are playing a supporting role while the main emphasis will be on the
\textbf{glmnet} package \citep{pkg-glmnet}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rsample)  }\CommentTok{# data splitting }
\KeywordTok{library}\NormalTok{(glmnet)   }\CommentTok{# implementing regularized regression approaches}
\KeywordTok{library}\NormalTok{(caret)    }\CommentTok{# automating the tuning process}
\KeywordTok{library}\NormalTok{(vip)      }\CommentTok{# variable importance}
\end{Highlighting}
\end{Shaded}

To illustrate various regularization concepts we will use the Ames
Housing data; however, at the end of the chapter we will also apply
regularization to the employee attrition data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.}
\CommentTok{# Use set.seed for reproducibility}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ames_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{(), }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Sale_Price"}\NormalTok{)}
\NormalTok{ames_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(ames_split)}
\NormalTok{ames_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(ames_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{why}{%
\section{Why Regularize}\label{why}}

The easiest way to understand regularized regression is to explain how
it is applied to ordinary least squares regression (OLS). The objective
of OLS regression is to find the plane that minimizes the sum of squared
errors (SSE) between the observed and predicted response. Illustrated
below, this means identifying the plane that minimizes the grey lines,
which measure the distance between the observed (red dots) and predicted
response (blue plane).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{illustrations/sq.errors-1} 

}

\caption{Fitted regression line using Ordinary Least Squares.}\label{fig:unnamed-chunk-86}
\end{figure}

More formally, this objective function can be written as:

\begin{equation}
\label{eq:ols-objective}
\text{minimize} \bigg \{ SSE = \sum^n_{i=1} (y_i - \hat{y}_i)^2 \bigg \}
\end{equation}

As we discussed in Chapter \ref{linear-regression}, the OLS objective
function performs quite well when our data align to the key assumptions
of OLS regression:

\begin{itemize}
\tightlist
\item
  Linear relationship
\item
  Multivariate normality
\item
  No autocorrelation
\item
  Homoscedastic (constant variance in residuals)
\item
  There are more observations (\emph{n}) than features (\emph{p})
  (\(n > p\))
\item
  No or little multicollinearity
\end{itemize}

However, for many real-life data sets we have very \emph{wide} data,
meaning we have a large number of features (\emph{p}) that we believe
are informative in predicting some outcome. As \emph{p} increases, we
can quickly violate some of the OLS assumptions and we require
alternative approaches to provide predictive analytic solutions. This
was illustrated in Chapter \ref{linear-regression} where
multicollinearity was biasing our coefficients and preventing us from
maximizing our predictive accuracy. By reducing multicollinearity, we
were able to increase our model's accuracy.

In addition to the above barriers to OLS performing well, with a large
number of features, we often would like to identify a smaller subset of
these features that exhibit the strongest effects. In essence, we
sometimes prefer techniques that provide \textbf{\emph{feature
selection}}. One approach to this is called hard threshholding feature
selection, which can be performed with linear model selection
approaches. However, model selection approaches can be computationally
inefficient, do not scale well, and they simply assume a feature as in
or out. We may wish to use a soft threshholding approach that slowly
pushes a feature's effect towards zero. As will be demonstrated, this
can provide additional understanding regarding predictive signals.

When we experience these concerns, one alternative to OLS regression is
to use regularized regression (also commonly referred to as
\emph{penalized} models or \emph{shrinkage} methods) to control the
parameter estimates. Regularized regression puts contraints on the
magnitude of the coefficients and will progressively shrink them towards
zero. This constraint helps to reduce the magnitude and fluctuations of
the coefficients and will reduce the variance of our model.

The objective function of regularized regression methods is very similar
to OLS regression; however, we add a penalty parameter (\emph{P}).

\begin{equation}
\label{eq:penalty}
\text{minimize} \big \{ SSE + P \big \}
\end{equation}

This penalty parameter constrains the size of the coefficients such that
the only way the coefficients can increase is if we experience a
comparable decrease in the sum of squared errors (SSE).

This concept generalizes to all GLM models. So far, we have be
discussing OLS and the sum of squared errors. However, different models
within the GLM family (i.e.~logistic regression, Poisson regression)
have different loss functions. Yet we can think of the penalty parameter
all the same - it constrains the size of the coefficients such that the
only way the coefficients can increase is if we experience a comparable
decrease in the model's loss function.

There are three types of penalty parameters we can implement:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ridge
\item
  Lasso
\item
  Elastic net, which is a combination of Ridge and Lasso
\end{enumerate}

\hypertarget{ridge}{%
\subsection{Ridge penalty}\label{ridge}}

Ridge regression \citep{hoerl1970ridge} controls the coefficients by
adding \(\lambda \sum^p_{j=1} \beta_j^2\) to the objective function.
This penalty parameter is also referred to as ``\(L_2\)'' as it
signifies a second-order penalty being used on the
coefficients.\footnote{Note that our pentalty is only applied to our
  feature coefficients (\(\beta_1, \beta_2, \dots, \beta_p\)) and not
  the intercept (\(\beta_0\)).}

\begin{equation}
\label{eq:ridge-penalty}
\text{minimize } \bigg \{ SSE + \lambda \sum^p_{j=1} \beta_j^2 \bigg \}
\end{equation}

This penalty parameter can take on a wide range of values, which is
controlled by the \emph{tuning parameter} \(\lambda\). When
\(\lambda = 0\) there is no effect and our objective function equals the
normal OLS regression objective function of simply minimizing SSE.
However, as \(\lambda \rightarrow \infty\), the penalty becomes large
and forces our coefficients to \emph{near zero}. This is illustrated in
Figure \ref{fig:ridge-coef-example} where exemplar coefficients have
been regularized with \(\lambda\) ranging from 0 to over 8,000
(\(log(8103) = 9\)).

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{illustrations/ridge_coef} 

}

\caption{Ridge regression coefficients as $\lambda$ grows from  $0 \rightarrow \infty$.}\label{fig:ridge-coef-example}
\end{figure}

Although these coefficients were scaled and centered prior to the
analysis, you will notice that some are extremely large when
\(\lambda \rightarrow 0\). Furthermore, you'll notice the large negative
parameter that fluctuates until \(log(\lambda) \approx 2\) where it then
continuously skrinks to zero. This is indicitive of multicollinearity
and likely illustrates that constraining our coefficients with
\(log(\lambda) > 2\) may reduce the variance, and therefore the error,
in our model.

In essence, the ridge regression model pushes many of the correlated
features towards each other rather than allowing for one to be wildly
positive and the other wildly negative. Furthermore, many of the
non-important features get pushed to near zero. This allows us to reduce
the noise in our data, which provides us more clarity in identifying the
true signals in our model.

However, a ridge model will retain all variables. Therefore, a ridge
model is good if you believe there is a need to retain all features in
your model yet reduce the noise that less influential variables may
create and minimize multicollinearity. However, a ridge model does not
perform automated feature selection. If greater interpretation is
necessary where you need to reduce the signal in your data to a smaller
subset then a lasso or elastic net penalty may be preferable.

\hypertarget{lasso}{%
\subsection{Lasso penalty}\label{lasso}}

The \emph{least absolute shrinkage and selection operator} (lasso) model
\citep{tibshirani1996regression} is an alternative to the ridge penalty
that has a small modification to the penalty in the objective function.
Rather than the \(L_2\) penalty we use the following \(L_1\) penalty
\(\lambda \sum^p_{j=1} | \beta_j|\) in the objective function.

\begin{equation}
\label{eq:lasso-penalty}
\text{minimize } \bigg \{ SSE + \lambda \sum^p_{j=1} | \beta_j | \bigg \}
\end{equation}

Whereas the ridge penalty approach pushes variables to
\emph{approximately but not equal to zero}, the lasso penalty will
actually push coefficients to zero as illustrated in Figure
\ref{fig:lasso-coef-example}. Thus the lasso model not only improves the
model with regularization but it also conducts automated feature
selection.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/lasso-coef-example-1} 

}

\caption{Lasso regression coefficients as $\lambda$ grows from  $0 \rightarrow \infty$. Numbers on top axis illustrate how many non-zero coefficients remain.}\label{fig:lasso-coef-example}
\end{figure}

In the figure above we see that when \(log(\lambda) = -5\) all 15
variables are in the model, when \(log(\lambda) = -1\) 12 variables are
retained, and when \(log(\lambda) = 1\) only 3 variables are retained.
Consequently, when a data set has many features, lasso can be used to
identify and extract those features with the largest (and most
consistent) signal.

\hypertarget{elastic}{%
\subsection{Elastic nets}\label{elastic}}

A generalization of the ridge and lasso penalties is the \emph{elastic
net} penalty \citep{zou2005regularization}, which combines the two
penalties.

\begin{equation}
\label{eq:elastic-penalty}
\text{minimize } \bigg \{ SSE + \lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} | \beta_j | \bigg \}
\end{equation}

Although lasso models perform feature selection, a result of their
penalty parameter is that typically when two strongly correlated
features are pushed towards zero, one may be pushed fully to zero while
the other remains in the model. Furthermore, the process of one being in
and one being out is not very systematic. In contrast, the ridge
regression penalty is a little more effective in systematically reducing
correlated features together. Consequently, the advantage of the elastic
net penalty is that it enables effective regularization via the ridge
penalty with the feature selection characteristics of the lasso penalty.

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

We illustrate implementation of regularized regression with the
\textbf{glmnet} package; however, realize there are other
implementations available (i.e. \textbf{h2o}, \textbf{elasticnet},
\textbf{penalized}). The \textbf{glmnet} package is a fast
implementation, but it requires some extra processing up-front to your
data if it's not already represented as a numeric matrix.
\textbf{glmnet} does not use the formula method
(\texttt{y\ \textasciitilde{}\ x}) so prior to modeling we need to
create our feature and target set. Furthermore, we use the
\texttt{model.matrix} function on our feature set (see
\texttt{Matrix::sparse.model.matrix} for increased efficiency on large
dimension data). We also log transform our response variable due to its
skeweness.

\begin{tip}
The log transformation of the response variable is not required;
however, parametric models such as regularized regression are sensitive
to skewed values so it is always recommended to normalize your response
variable.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training and testing feature matrices}
\CommentTok{# we use model.matrix(...)[, -1] to discard the intercept}
\NormalTok{train_x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., ames_train)[, }\DecValTok{-1}\NormalTok{]}
\NormalTok{test_x  <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., ames_test)[, }\DecValTok{-1}\NormalTok{]}

\CommentTok{# Create training and testing response vectors}
\CommentTok{# transform y with log transformation}
\NormalTok{train_y <-}\StringTok{ }\KeywordTok{log}\NormalTok{(ames_train}\OperatorTok{$}\NormalTok{Sale_Price)}
\NormalTok{test_y  <-}\StringTok{ }\KeywordTok{log}\NormalTok{(ames_test}\OperatorTok{$}\NormalTok{Sale_Price)}
\end{Highlighting}
\end{Shaded}

To apply a regularized model we can use the \texttt{glmnet::glmnet}
function. The \texttt{alpha} parameter tells \textbf{glmnet} to perform
a ridge (\texttt{alpha\ =\ 0}), lasso (\texttt{alpha\ =\ 1}), or elastic
net (\(0 < alpha < 1\)) model. Behind the scenes, \textbf{glmnet} is
doing two things that you should be aware of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since regularized methods apply a penalty to the coefficients, we need
  to ensure our coefficients are on a common scale. If not, then
  predictors with naturally larger values (i.e.~total square footage)
  will be penalized more than predictors with naturally smaller values
  (i.e.~total number of rooms). \textbf{glmnet} automatically
  standardizes your features. If you standardize your predictors prior
  to \textbf{glmnet} you can turn this argument off with
  \texttt{standardize\ =\ FALSE}.
\item
  \textbf{glmnet} will perform ridge models across a wide range of
  \(\lambda\) parameters, which are illustrated in the figure below.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply Ridge regression to attrition data}
\NormalTok{ridge <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(ridge, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/ridge1-1} 

}

\caption{Coefficients for our ridge regression model as $\lambda$ grows from  $0 \rightarrow \infty$.}\label{fig:ridge1}
\end{figure}

In fact, we can see the exact \(\lambda\) values applied with
\texttt{ridge\$lambda}. Although you can specify your own \(\lambda\)
values, by default \textbf{glmnet} applies 100 \(\lambda\) values that
are data derived.

\begin{tip}
glmnet has built-in functions to auto-generate the appropriate
\(\lambda\) values based on the data so the vast majority of the time
you will have little need to adjust the default \(\lambda\) values.
\end{tip}

We can also directly access the coefficients for a model using
\texttt{coef}. \textbf{glmnet} stores all the coefficients for each
model in order of largest to smallest \(\lambda\). Due to the number of
features, here I just peak at the two largest coefficients
(\texttt{Latitude} \& \texttt{Overall\_QualVery\_Excellent}) features
for the largest \(\lambda\) (279.1035) and smallest \(\lambda\)
(0.02791035). You can see how the largest \(\lambda\) value has pushed
these coefficients to nearly 0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lambdas applied to penalty parameter}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\NormalTok{## [1] 279.1 254.3 231.7 211.1 192.4 175.3}

\CommentTok{# small lambda results in large coefficients}
\KeywordTok{coef}\NormalTok{(ridge)[}\KeywordTok{c}\NormalTok{(}\StringTok{"Latitude"}\NormalTok{, }\StringTok{"Overall_QualVery_Excellent"}\NormalTok{), }\DecValTok{100}\NormalTok{]}
\NormalTok{##                   Latitude Overall_QualVery_Excellent }
\NormalTok{##                     0.6059                     0.0980}

\CommentTok{# large lambda results in small coefficients}
\KeywordTok{coef}\NormalTok{(ridge)[}\KeywordTok{c}\NormalTok{(}\StringTok{"Latitude"}\NormalTok{, }\StringTok{"Overall_QualVery_Excellent"}\NormalTok{), }\DecValTok{1}\NormalTok{] }
\NormalTok{##                   Latitude Overall_QualVery_Excellent }
\NormalTok{##                  6.228e-36                  9.373e-37}
\end{Highlighting}
\end{Shaded}

However, at this point, we do not understand how much improvement we are
experiencing in our loss function across various \(\lambda\) values.

\hypertarget{regression-glmnet-tune}{%
\section{Tuning}\label{regression-glmnet-tune}}

Recall that \(\lambda\) is a tuning parameter that helps to control our
model from over-fitting to the training data. However, to identify the
optimal \(\lambda\) value we need to perform cross-validation (CV).
\texttt{cv.glmnet} provides a built-in option to perform k-fold CV, and
by default, performs 10-fold CV. Here we perform a CV glmnet model for
both a ridge and lasso penalty.

\begin{rmdtip}
By default, \texttt{cv.glmnet} uses MSE as the loss function but you can
also use mean absolute error by changing the \texttt{type.measure}
argument.
\end{rmdtip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply CV Ridge regression to Ames data}
\NormalTok{ridge <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{# Apply CV Lasso regression to Ames data}
\NormalTok{lasso <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{alpha =} \DecValTok{1}
\NormalTok{)}

\CommentTok{# plot results}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(ridge, }\DataTypeTok{main =} \StringTok{"Ridge penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(lasso, }\DataTypeTok{main =} \StringTok{"Lasso penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/ridge-lasso-cv-models-1} 

}

\caption{10-fold cross validation MSE for a ridge and lasso model. First dotted vertical line in each plot represents the $\lambda$ with the smallest MSE and the second represents the $\lambda$ with an MSE within one standard error of the minimum MSE.}\label{fig:ridge-lasso-cv-models}
\end{figure}

Figure \ref{fig:ridge-lasso-cv-models} illustrate the 10-fold CV mean
squared error (MSE) across the \(\lambda\) values. In both models we see
a slight improvement in the MSE as our penalty \(log(\lambda)\) gets
larger , suggesting that a regular OLS model likely overfits our data.
But as we constrain it further (continue to increase the penalty), our
MSE starts to increase. The numbers at the top of the plot refer to the
number of variables in the model. Ridge regression does not force any
variables to exactly zero so all features will remain in the model but
we see the number of variables retained in the lasso model go down as
our penalty increases.

The first and second vertical dashed lines represent the \(\lambda\)
value with the minimum MSE and the largest \(\lambda\) value within one
standard error of the minimum MSE. The minimum MSE for our ridge model
is 0.0215 (produced when \(\lambda = 0.1026649\)) whereas the minimium
MSE for our lasso model is 0.0228 (produced when
\(\lambda = 0.003521887\)).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ridge model}
\KeywordTok{min}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{cvm)       }\CommentTok{# minimum MSE}
\NormalTok{## [1] 0.02148}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda.min     }\CommentTok{# lambda for this min MSE}
\NormalTok{## [1] 0.1237}

\NormalTok{ridge}\OperatorTok{$}\NormalTok{cvm[ridge}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{==}\StringTok{ }\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]  }\CommentTok{# 1 st.error of min MSE}
\NormalTok{## [1] 0.02488}
\NormalTok{ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se  }\CommentTok{# lambda for this MSE}
\NormalTok{## [1] 0.6599}

\CommentTok{# Lasso model}
\KeywordTok{min}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{cvm)       }\CommentTok{# minimum MSE}
\NormalTok{## [1] 0.02411}
\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda.min     }\CommentTok{# lambda for this min MSE}
\NormalTok{## [1] 0.003865}

\NormalTok{lasso}\OperatorTok{$}\NormalTok{cvm[lasso}\OperatorTok{$}\NormalTok{lambda }\OperatorTok{==}\StringTok{ }\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se]  }\CommentTok{# 1 st.error of min MSE}
\NormalTok{## [1] 0.02819}
\NormalTok{lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se  }\CommentTok{# lambda for this MSE}
\NormalTok{## [1] 0.0156}
\end{Highlighting}
\end{Shaded}

We can assess this visually. Figure \ref{fig:ridge-lasso-cv-viz-results}
plot the coefficients across the \(\lambda\) values and the dashed red
line represents the \(\lambda\) with the smallest MSE and the dashed
blue line represents largest \(\lambda\) that falls within one standard
error of the minimum MSE. This shows you how much we can constrain the
coefficients while still maximizing predictive accuracy.

\begin{tip}
Above, we saw that both ridge and lasso penalties provide similiar MSEs;
however, these plots illustrate that ridge is still using all 299
variables whereas the lasso model can get a similar MSE by reducing our
feature set from 299 down to 131. However, there will be some
variability with this MSE and we can reasonably assume that we can
achieve a similar MSE with a slightly more constrained model that uses
only 63 features. Although this lasso model does not offer significant
improvement over the ridge model, we get approximately the same accuracy
by using only 63 features! If describing and interpreting the predictors
is an important outcome of your analysis, this may significantly aid
your endeavor.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ridge model}
\NormalTok{ridge_min <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{alpha =} \DecValTok{0}
\NormalTok{)}

\CommentTok{# Lasso model}
\NormalTok{lasso_min <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{alpha =} \DecValTok{1}
\NormalTok{)}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{# plot ridge model}
\KeywordTok{plot}\NormalTok{(ridge_min, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Ridge penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{lambda.min), }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(ridge}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}

\CommentTok{# plot lasso model}
\KeywordTok{plot}\NormalTok{(lasso_min, }\DataTypeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Lasso penalty}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{lambda.min), }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{log}\NormalTok{(lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/ridge-lasso-cv-viz-results-1} 

}

\caption{Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the $\lambda$ with the smallest MSE and the second represents the $\lambda$ with an MSE within one standard error of the minimum MSE.}\label{fig:ridge-lasso-cv-viz-results}
\end{figure}

So far we've implemented a pure ridge and pure lasso model. However, we
can implement an elastic net the same way as the ridge and lasso models,
by adjusting the \texttt{alpha} parameter. Any \texttt{alpha} value
between 0-1 will perform an elastic net. When \texttt{alpha\ =\ 0.5} we
perform an equal combination of penalties whereas \texttt{alpha}
\(\rightarrow 0\) will have a heavier ridge penalty applied and
\texttt{alpha} \(\rightarrow 1\) will have a heavier lasso penalty.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glmnet-elastic-comparison-1} 

}

\caption{Coefficients for various penalty parameters.}\label{fig:glmnet-elastic-comparison}
\end{figure}

Often, the optimal model contains an \texttt{alpha} somewhere between
0-1, thus we want to tune both the \(\lambda\) and the \texttt{alpha}
parameters. As in Chapters \ref{linear-regression} and
\ref{logistic-regression}, we can use the \textbf{caret} package to
automate the tuning process. The following performs a grid search over
10 values of the alpha parameter between 0-1 and ten values of the
lambda parameter from the lowest to highest lambda values identified by
\textbf{glmnet}.

\begin{warning}
This grid search took \textbf{71 seconds} to compute.
\end{warning}

The following shows the model that minimized RMSE used an alpha of 0.1
and lambda of 0.0453. The minimum RMSE of 0.1448677
(\(MSE = 0.1448677^2 = 0.02099\)) is only slightly lower than our full
ridge model produced earlier. Figure \ref{fig:glmnet-tuning-grid}
illustrates how the combination of alpha values (x-axis) and lambda
values (line color) influence the RMSE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# grid search across }
\NormalTok{tuned_mod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train_x,}
  \DataTypeTok{y =}\NormalTok{ train_y,}
  \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{10}
\NormalTok{)}

\CommentTok{# model with lowest RMSE}
\NormalTok{tuned_mod}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##   alpha  lambda}
\NormalTok{## 8   0.1 0.04528}

\CommentTok{# plot cross-validated RMSE}
\KeywordTok{plot}\NormalTok{(tuned_mod)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/glmnet-tuning-grid-1} 

}

\caption{The 10-fold cross valdation RMSE across 10 alpha values (x-axis) and 10 lambda values (line color).}\label{fig:glmnet-tuning-grid}
\end{figure}

So how does this compare to our previous best model for the Ames data?
Keep in mind that for this chapter we log transformed our response
variable. Consequently, to provide a fair comparison to our partial
least squares RMSE of \$31,522.47, we need to re-transform our predicted
values. The following illustrates that our optimal regularized model
achieves an RMSE of \$26,608.12. Introducing a penalty parameter to
constrain the coefficients provides quite an improvement over our
dimension reduction approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict sales price on training data}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tuned_mod, train_x)}

\CommentTok{# compute RMSE of transformed predicted}
\KeywordTok{RMSE}\NormalTok{(}\KeywordTok{exp}\NormalTok{(pred), }\KeywordTok{exp}\NormalTok{(train_y))}
\NormalTok{## [1] 26608}
\end{Highlighting}
\end{Shaded}

\hypertarget{lm-features}{%
\section{Feature interpretation}\label{lm-features}}

Variable importance for regularized models provide a similar
interpretation as in linear (or logistic) regression. Importance is
determined by the absolute value of the \emph{t}-statistic and we can
see in Figure \ref{fig:regularize-vip} some of the same variables that
were considered highly influential in our partial least squares model,
albeit in differing order (i.e. \texttt{Gr\_Liv\_Area},
\texttt{Overall\_Qual}, \texttt{First\_Flr\_SF}, \texttt{Garage\_Cars}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vip}\NormalTok{(tuned_mod, }\DataTypeTok{num_features =} \DecValTok{20}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/regularize-vip-1} 

}

\caption{Top 20 most important variables for the optimal regularized regression model.}\label{fig:regularize-vip}
\end{figure}

Similar to linear and logistic regression, the relationship between
these influential variables and the response is monotonic linear.
However, since we modeled our response with a log transformation, the
relationship between will be monotonic but non-linear for the
untransformed relationship. Figure \ref{fig:regularized-top4-pdp}
illustrates the relationship between the top four most influential
variables and the non-transformed sales price. All relationships are
positive in nature, as the values in these features increase (or for
\texttt{Overall\_QualExcellent} if it exists) the average predicted
sales price increases.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/regularized-top4-pdp-1} 

}

\caption{Partial dependence plots for the first four most important variables.}\label{fig:regularized-top4-pdp}
\end{figure}

However, we see the \(5^{th}\) most influential variable is
\texttt{Overall\_QualPoor}. When a home has an overall quality rating of
poor we see that the average predicted sales price decreases versus when
it has some other overall quality rating. Consequently, its important to
not only look at the variable importance ranking, but also observe the
positive or negative nature of the relationship.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/regularized-num5-pdp-1} 

}

\caption{Partial dependence plots for the first four most important variables.}\label{fig:regularized-num5-pdp}
\end{figure}

\hypertarget{attrition-data}{%
\section{Attrition data}\label{attrition-data}}

We saw that regularization significantly improved our predictive
accuracy for the Ames data, but how about for the attrition data. In
Chapter \ref{logistic-regression} we saw a maximum cross-validated
accuracy of 86.3\% for our logistic regression model. Performing a
regularized logistic regression model provides us with about 1.5\%
improvement in our accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the rsample::attrition data.}
\CommentTok{# Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}

\CommentTok{# train logistic regression model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{glm_mod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glm"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# train regularized logistic regression model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{penalized_mod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train, }
  \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"zv"}\NormalTok{, }\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneLength =} \DecValTok{10}
\NormalTok{  )}

\CommentTok{# extract out of sample performance measures}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{logistic_model =}\NormalTok{ glm_mod, }
  \DataTypeTok{penalized_model =}\NormalTok{ penalized_mod}
\NormalTok{  )))}\OperatorTok{$}\NormalTok{statistics}\OperatorTok{$}\NormalTok{Accuracy}
\NormalTok{##                   Min. 1st Qu. Median   Mean 3rd Qu.}
\NormalTok{## logistic_model  0.8058  0.8389 0.8586 0.8632  0.8949}
\NormalTok{## penalized_model 0.8447  0.8568 0.8744 0.8787  0.9069}
\NormalTok{##                   Max. NA's}
\NormalTok{## logistic_model  0.9135    0}
\NormalTok{## penalized_model 0.9135    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{final-thoughts-2}{%
\section{Final thoughts}\label{final-thoughts-2}}

Regularized regression is a great start for building onto generalized
linear models (i.e.~OLS, logistic regression) to make them more robust
to assumption violations and perform automated feature selection. This
chapter illustrated how constraining our coefficients with a
regulazation penalty helped to improve predictive accuary for both the
ames and attrition data. However, regularized models still assume linear
relationships. The chapters that follow will start exploring non-linear
algorithms to see if we can further improve our predictive accuracy. The
following summarizes some of the advantages and disadvantages discussed
regarding regularized regression.

\textbf{FIXME: refine this section}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  Normal GLM models require that you have more observations than
  variables (\(n>p\)); regularized regression allows you to model wide
  data where \(n<p\).
\item
  Minimizes the impact of multicollinearity.
\item
  Provides automatic feature selection (at least when you apply a Lasso
  or elastic net penalty).
\item
  Minimal hyperparameters making it easy to tune.
\item
  Computationally efficient - relatively fast compared to other
  algorithms in this guide and does not require large memory.
\end{itemize}

\textbf{Disdvantages:}

\begin{itemize}
\tightlist
\item
  Requires data pre-processing - requires all variables to be numeric
  (i.e.~one-hot encode). However, some implementations (i.e.
  \textbf{h2o} package) helps to automate this process.
\item
  Does not handle missing data - must impute or remove observations with
  missing values.
\item
  Not robust to outliers as they can still bias the coefficients.
\item
  Assumes relationships between predictors and response variable to be
  monotonic linear (always increasing or decreasing in a linear
  fashion).
\item
  Typically does not perform as well as more advanced methods that allow
  non-monotonic and non-linear relationships (i.e.~random forests,
  gradient boosting machines, neural networks).
\end{itemize}

\hypertarget{learning-more-2}{%
\section{Learning more}\label{learning-more-2}}

This serves as an introduction to regularized regression; however, it
just scrapes the surface. Regularized regression approaches have been
extended to other parametric (i.e.~Cox proportional hazard, poisson,
support vector machines) and non-parametric (i.e.~Least Angle
Regression, the Bayesian Lasso, neural networks) models. The following
are great resources to learn more (listed in order of complexity):

\begin{itemize}
\tightlist
\item
  \href{https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?ie=UTF8\&qid=1522246635\&sr=8-1\&keywords=applied+predictive+modelling}{Applied
  Predictive Modeling}
\item
  \href{https://www.amazon.com/Practical-Machine-Learning-H2O-Techniques/dp/149196460X}{Practical
  Machine Learning with H2o}
\item
  \href{https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_2?ie=UTF8\&qid=1522246635\&sr=8-2\&keywords=applied+predictive+modelling}{Introduction
  to Statistical Learning}
\item
  \href{https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_3?ie=UTF8\&qid=1522246635\&sr=8-3\&keywords=applied+predictive+modelling}{The
  Elements of Statistical Learning}
\item
  \href{https://www.amazon.com/Statistical-Learning-Sparsity-Generalizations-Probability/dp/1498712169/ref=sr_1_1?ie=UTF8\&qid=1522246685\&sr=8-1\&keywords=statistical+learning+with+sparsity}{Statistical
  Learning with Sparsity}
\end{itemize}

\hypertarget{MARS}{%
\chapter{Multivariate Adaptive Regression Splines}\label{MARS}}

The previous chapters discussed algorithms that are intrinsically
linear. Many of these models can be adapted to nonlinear patterns in the
data by manually adding model terms (i.e.~squared terms, interaction
effects); however, to do so you must know the specific nature of the
nonlinearity a priori. Alternatively, there are numerous algorithms that
are inherently nonlinear. When using these models, the exact form of the
nonlinearity does not need to be known explicitly or specified prior to
model training. Rather, these algorithms will search for, and discover,
nonlinearities in the data that help maximize predictive accuracy.

This chapter discusses multivariate adaptive regression splines (MARS),
an algorithm that essentially creates a piecewise linear model which
provides an intuitive stepping block into nonlinearity after grasping
the concept of multiple linear regression. Future chapters will focus on
other nonlinear algorithms.

\hypertarget{prerequisites-6}{%
\section{Prerequisites}\label{prerequisites-6}}

For this chapter we will use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rsample)   }\CommentTok{# data splitting }
\KeywordTok{library}\NormalTok{(ggplot2)   }\CommentTok{# plotting}
\KeywordTok{library}\NormalTok{(earth)     }\CommentTok{# fit MARS models}
\KeywordTok{library}\NormalTok{(caret)     }\CommentTok{# automating the tuning process}
\KeywordTok{library}\NormalTok{(vip)       }\CommentTok{# variable importance}
\KeywordTok{library}\NormalTok{(pdp)       }\CommentTok{# variable relationships}
\end{Highlighting}
\end{Shaded}

To illustrate various MARS modeling concepts we will use the Ames
Housing data; however, at the end of the chapter we will also apply a
MARS model to the employee attrition data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.}
\CommentTok{# Use set.seed for reproducibility}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ames_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{(), }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Sale_Price"}\NormalTok{)}
\NormalTok{ames_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(ames_split)}
\NormalTok{ames_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(ames_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-basic-idea}{%
\section{The basic idea}\label{the-basic-idea}}

In the previous chapters, we focused on linear models. We illustrated
some of the advantages of linear models such as their ease and speed of
computation and also the intuitive nature of interpreting their
coefficients. However, linear models make a strong assumption about
linearity, and this assumption is often a poor one, which can affect
predictive accuracy.

We can extend linear models to capture non-linear relationships.
Typically, this is done by explicitly including polynomial parameters or
step functions. Polynomial regression is a form of regression in which
the relationship between the independent variable \emph{x} and the
dependent variable \emph{y} is modeled as an n\(^{th}\) degree
polynomial of \emph{x}. For example, Equation \eqref{eq:poly} represents a
polynomial regression function where \emph{y} is modeled as a function
of \emph{x} with \emph{d} degrees. Generally speaking, it is unusual to
use \emph{d} greater than 3 or 4 as the larger \emph{d} becomes, the
easier the function fit becomes overly flexible and oddly
shapened\ldots{}especially near the boundaries of the range of \emph{x}
values.

\begin{equation}
\label{eq:poly}
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i \dots + \beta_d x^d_i + \epsilon_i,
\end{equation}

An alternative to polynomial regression is step function regression.
Whereas polynomial functions impose a global non-linear relationship,
step functions break the range of \emph{x} into bins, and fit a
different constant for each bin. This amounts to converting a continuous
variable into an ordered categorical variable such that our linear
regression function is converted to Equation \eqref{eq:steps}

\begin{equation}
\label{eq:steps}
  y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \beta_3 C_3(x_i) \dots + \beta_d C_d(x_i) + \epsilon_i,
\end{equation}

where \(C_1(x)\) represents \emph{x} values ranging from
\(c_1 \leq x < c_2\), \(C_2(x)\) represents \emph{x} values ranging from
\(c_2 \leq x < c_3\), \(\dots\), \(C_d(x)\) represents \emph{x} values
ranging from \(c_{d-1} \leq x < c_d\). Figure
\ref{fig:nonlinear-comparisons} illustrate polynomial and step function
fits for \texttt{Sale\_Price} as a function of \texttt{Year\_Built} in
our \textbf{ames} data.

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/nonlinear-comparisons-1}

\}

\textbackslash{}caption\{Blue line represents predicted
\texttt{Sale\_Price} values as a function of \texttt{Year\_Built} for
alternative approaches to modeling explicit nonlinear regression
patterns. (A) Traditional nonlinear regression approach does not capture
any nonlinearity unless the predictor or response is transformed
(i.e.~log transformation). (B) Degree-2 polynomial, (C) Degree-3
polynomial, (D) Step function fitting cutting \texttt{Year\_Built} into
three categorical levels.\}\label{fig:nonlinear-comparisons}
\textbackslash{}end\{figure\}

Although useful, the typical implementation of polynomial regression and
step functions require the user to explicitly identify and incorporate
which variables should have what specific degree of interaction or at
what points of a variable \emph{x} should cut points be made for the
step functions. Considering many data sets today can easily contain 50,
100, or more features, this would require an enormous and unncessary
time commitment from an analyst to determine these explicit non-linear
settings.

\hypertarget{multivariate-regression-splines}{%
\subsection{Multivariate regression
splines}\label{multivariate-regression-splines}}

Multivariate adaptive regression splines (MARS) provide a convenient
approach to capture the nonlinearity aspect of polynomial regression by
assessing cutpoints (\emph{knots}) similar to step functions. The
procedure assesses each data point for each predictor as a knot and
creates a linear regression model with the candidate feature(s). For
example, consider our simple model of
\texttt{Sale\_Price\ \textasciitilde{}\ Year\_Built}. The MARS procedure
will first look for the single point across the range of
\texttt{Year\_Built} values where two different linear relationships
between \texttt{Sale\_Price} and \texttt{Year\_Built} achieve the
smallest error. What results is known as a hinge function (\(h(x-a)\)
where \emph{a} is the cutpoint value). For a single knot (Figure
\ref{fig:examples-of-multiple-knots} (A)), our hinge function is
\(h(\text{Year_Built}-1968)\) such that our two linear models for
\texttt{Sale\_Price} are

\begin{equation}
\label{eq:hinge}
  \text{Sale_Price} = 
  \begin{cases}
    136091.022 & \text{Year_Built} \leq 1968, \\
    136091.022 + 3094.208(\text{Year_Built} - 1968) & \text{Year_Built} > 1968
  \end{cases}
\end{equation}

Once the first knot has been found, the search continues for a second
knot which is found at 2006 (Figure \ref{fig:examples-of-multiple-knots}
(B)). This results in three linear models for \texttt{Sale\_Price}:

\begin{equation}
\label{eq:hinge2}
  \text{Sale_Price} = 
  \begin{cases}
    136091.022 & \text{Year_Built} \leq 1968, \\
    136091.022 + 2898.424(\text{Year_Built} - 1968) & 1968 < \text{Year_Built} \leq 2006, \\
    136091.022 + 20176.284(\text{Year_Built} - 2006) & \text{Year_Built} > 2006
  \end{cases}
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/examples-of-multiple-knots-1} 

}

\caption{Examples of fitted regression splines of one (A), two (B), three (C), and four (D) knots.}\label{fig:examples-of-multiple-knots}
\end{figure}

This procedure can continue until many knots are found, producing a
highly non-linear pattern. Although including many knots may allow us to
fit a really good relationship with our training data, it may not
generalize very well to new, unseen data. For example, Figure
\ref{fig:example-9-knots} includes nine knots but this likley will not
generalize very well to our test data.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/example-9-knots-1} 

}

\caption{Too many knots may not generalize well to unseen data.}\label{fig:example-9-knots}
\end{figure}

Consequently, once the full set of knots have been created, we can
sequentially remove knots that do not contribute significantly to
predictive accuracy. This process is known as ``pruning'' and we can use
cross-validation, as we have with the previous models, to find the
optimal number of knots.

\hypertarget{fitting-a-basic-mars-model}{%
\section{Fitting a basic MARS model}\label{fitting-a-basic-mars-model}}

We can fit a MARS model with the \textbf{earth} package \citep{R-earth}.
By default, \texttt{earth::earth()} will assess all potential knots
across all supplied features and then will prune to the optimal number
of knots based on an expected change in \(R^2\) (for the training data)
of less than 0.001. This calculation is performed by the Generalized
cross-validation procedure (GCV statistic), which is a computational
shortcut for linear models that produces an error value that
\emph{approximates} leave-one-out cross-validation
\citep{golub1979generalized}.

\begin{note}
The term ``MARS'' is trademarked and licensed exclusively to Salford
Systems \url{http://www.salfordsystems.com}. We can use MARS as an
abbreviation; however, it cannot be used for competing software
solutions. This is why the R package uses the name \textbf{earth}.
\end{note}

The following applies a basic MARS model to our \textbf{ames} data and
performs a search for required knots across all features. The results
show us the final models GCV statistic, generalized \(R^2\) (GRSq), and
more.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit a basic MARS model}
\NormalTok{mars1 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{.,  }
  \DataTypeTok{data =}\NormalTok{ ames_train   }
\NormalTok{)}

\CommentTok{# Print model summary}
\KeywordTok{print}\NormalTok{(mars1)}
\NormalTok{## Selected 37 of 45 terms, and 26 of 307 predictors}
\NormalTok{## Termination condition: RSq changed by less than 0.001 at 45 terms}
\NormalTok{## Importance: Gr_Liv_Area, Year_Built, ...}
\NormalTok{## Number of terms at each degree of interaction: 1 36 (additive model)}
\NormalTok{## GCV 521186626    RSS 9.958e+11    GRSq 0.9165    RSq 0.9223}
\end{Highlighting}
\end{Shaded}

It also shows us that 38 of 41 terms were used from 27 of the 307
original predictors. But what does this mean? If we were to look at all
the coefficients, we would see that there are 38 terms in our model
(including the intercept). These terms include hinge functions produced
from the original 307 predictors (307 predictors because the model
automatically dummy encodes our categorical variables). Looking at the
first 10 terms in our model, we see that \texttt{Gr\_Liv\_Area} is
included with a knot at 2945 (the coefficient for
\(h(2945-Gr_Liv_Area)\) is -49.85), \texttt{Year\_Built} is included
with a knot at 2003, etc.

\begin{tip}
You can check out all the coefficients with \texttt{summary(mars1)}.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(mars1) }\OperatorTok{%>%}\StringTok{ }\NormalTok{.}\OperatorTok{$}\NormalTok{coefficients }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{##                            Sale_Price}
\NormalTok{## (Intercept)                 301399.98}
\NormalTok{## h(2945-Gr_Liv_Area)            -49.85}
\NormalTok{## h(Year_Built-2003)            2698.40}
\NormalTok{## h(2003-Year_Built)            -357.11}
\NormalTok{## h(Total_Bsmt_SF-2171)         -265.31}
\NormalTok{## h(2171-Total_Bsmt_SF)          -29.77}
\NormalTok{## Overall_QualExcellent        88345.90}
\NormalTok{## Overall_QualVery_Excellent  116330.49}
\NormalTok{## Overall_QualVery_Good        36646.56}
\NormalTok{## h(Bsmt_Unf_SF-278)             -21.16}
\end{Highlighting}
\end{Shaded}

The plot method for MARS model objects provide convenient performance
and residual plots. Figure \ref{fig:basic-mod-plot} illustrates the
model selection plot that graphs the GCV \(R^2\) (left-hand y-axis and
solid black line) based on the number of terms retained in the model
(x-axis) which are constructed from a certain number of original
predictors (right-hand y-axis). The vertical dashed lined at 37 tells us
the optimal number of non-intercept terms retained where marginal
increases in GCV \(R^2\) are less than 0.001.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(mars1, }\DataTypeTok{which =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/basic-mod-plot-1} 

}

\caption{Model summary capturing GCV $R^2$ (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 37 non-intercept terms were retained which are based on 26 predictors.  Any additional terms retained in the model, over and above these 37, results in less than 0.001 improvement in the GCV $R^2$.}\label{fig:basic-mod-plot}
\end{figure}

In addition to pruning the number of knots, \texttt{earth::earth()}
allows us to also assess potential interactions between different hinge
functions. The following illustrates by including a
\texttt{degree\ =\ 2} argument. You can see that now our model includes
interaction terms between multiple hinge functions (i.e.
\texttt{h(Year\_Built-2003)*h(Gr\_Liv\_Area-2274)}) is an interaction
effect for those houses built prior to 2003 and have less than 2,274
square feet of living space above ground).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit a basic MARS model}
\NormalTok{mars2 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(}
\NormalTok{  Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{.,  }
  \DataTypeTok{data =}\NormalTok{ ames_train,}
  \DataTypeTok{degree =} \DecValTok{2}
\NormalTok{)}

\CommentTok{# check out the first 10 coefficient terms}
\KeywordTok{summary}\NormalTok{(mars2) }\OperatorTok{%>%}\StringTok{ }\NormalTok{.}\OperatorTok{$}\NormalTok{coefficients }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{##                                           Sale_Price}
\NormalTok{## (Intercept)                                242611.64}
\NormalTok{## h(Gr_Liv_Area-2945)                           144.39}
\NormalTok{## h(2945-Gr_Liv_Area)                           -57.72}
\NormalTok{## h(Year_Built-2003)                          10909.70}
\NormalTok{## h(2003-Year_Built)                           -780.24}
\NormalTok{## h(Year_Built-2003)*h(Gr_Liv_Area-2274)         18.55}
\NormalTok{## h(Year_Built-2003)*h(2274-Gr_Liv_Area)        -10.31}
\NormalTok{## h(Total_Bsmt_SF-1035)                          62.12}
\NormalTok{## h(1035-Total_Bsmt_SF)                         -33.04}
\NormalTok{## h(Total_Bsmt_SF-1035)*Kitchen_QualTypical     -32.76}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning}{%
\section{Tuning}\label{tuning}}

Since there are two tuning parameters associated with our MARS model:
the degree of interactions and the number of retained terms, we need to
perform a grid search to identify the optimal combination of these
hyperparameters that minimize prediction error (the above pruning
process was based only on an approximation of cross-validated
performance on the training data rather than an actual \emph{k}-fold
cross validation process). As in previous chapters, we will perform a
cross-validated grid search to identify the optimal mix. Here, we set up
a search grid that assesses 30 different combinations of interaction
effects (\texttt{degree}) and the number of terms to retain
(\texttt{nprune}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a tuning grid}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{degree =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }
  \DataTypeTok{nprune =} \KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{floor}\NormalTok{()}
\NormalTok{  )}

\KeywordTok{head}\NormalTok{(hyper_grid)}
\NormalTok{##   degree nprune}
\NormalTok{## 1      1      2}
\NormalTok{## 2      2      2}
\NormalTok{## 3      3      2}
\NormalTok{## 4      1     12}
\NormalTok{## 5      2     12}
\NormalTok{## 6      3     12}
\end{Highlighting}
\end{Shaded}

As in the previous chapters, we can use \textbf{caret} to perform a grid
search using 10-fold cross-validation. The model that provides the
optimal combination includes second degree interactions and retains 34
terms. The cross-validated RMSE for these models are illustrated in
Figure \ref{fig:grid-search} and the optimal model's cross-validated
RMSE is \$24,021.68.

\begin{warning}
This grid search took 5 minutes to complete.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproducibiity}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# cross validated model}
\NormalTok{tuned_mars <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(ames_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Sale_Price),}
  \DataTypeTok{y =}\NormalTok{ ames_train}\OperatorTok{$}\NormalTok{Sale_Price,}
  \DataTypeTok{method =} \StringTok{"earth"}\NormalTok{,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{tuned_mars}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##    nprune degree}
\NormalTok{## 14     34      2}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(tuned_mars)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/grid-search-1} 

}

\caption{Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 34 terms and includes up to 2$^{nd}$ degree interactions.}\label{fig:grid-search}
\end{figure}

The above grid search helps to focus where we can further refine our
model tuning. As a next step, we could perform a grid search that
focuses in on a refined grid space for \texttt{nprune} (i.e.~comparing
25-40 terms retained). However, for brevity we will leave this as an
exercise for the reader.

So how does this compare to our previously built linear models for the
Ames housing data? The following table compares the cross-validated RMSE
for our tuned MARS model to a regular multiple regression model along
with tuned principal component regression (PCR), partial least squares
(PLS), and regularized regression (elastic net) models. By incorporating
non-linear relationships and interaction effects, the MARS model
provides a substantial improvement over the previous linear models that
we have explored.

\begin{table}[H]
\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & NA's\\
\hline
Multiple\_regression & 21305 & 24403 & 46475 & 41438 & 53958 & 63247 & 0\\
\hline
PCR & 28214 & 30775 & 36548 & 35770 & 40253 & 44760 & 0\\
\hline
PLS & 22812 & 24196 & 31162 & 31522 & 35383 & 44895 & 0\\
\hline
Elastic\_net & 20970 & 24265 & 30590 & 30717 & 34216 & 45241 & 0\\
\hline
MARS & 18440 & 20745 & 23147 & 24022 & 26241 & 31755 & 0\\
\hline
\end{tabular}
\end{table}

\hypertarget{feature-interpretation-1}{%
\section{Feature interpretation}\label{feature-interpretation-1}}

MARS models via \texttt{earth::earth()} include a backwards elimination
feature selection routine that looks at reductions in the GCV estimate
of error as each predictor is added to the model. This total reduction
is used as the variable importance measure (\texttt{value\ =\ "gcv"}).
Since MARS will automatically include and exclude terms during the
pruning process, it essentially performs automated feature selection. If
a predictor was never used in any of the MARS basis functions in the
final model (after pruning), it has an importance value of zero. This is
illustrated in Figure \ref{fig:vip} where 27 features have \(>0\)
importance values while the rest of the features have an importance
value of zero since they were no included in the final model.
Alternatively, you can also monitor the change in the residual sums of
squares (RSS) as terms are added (\texttt{value\ =\ "rss"}); however,
you will see very little difference between these methods.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# variable importance plots}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(tuned_mars, }\DataTypeTok{num_features =} \DecValTok{40}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{value =} \StringTok{"gcv"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"GCV"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(tuned_mars, }\DataTypeTok{num_features =} \DecValTok{40}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{value =} \StringTok{"rss"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"RSS"}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/vip-1} 

}

\caption{Variable importance based on impact to GCV (left) and RSS (right) values as predictors are added to the model. Both variable importance measures will usually give you very similar results.}\label{fig:vip}
\end{figure}

Its important to realize that variable importance will only measure the
impact of the prediction error as features are included; however, it
does not measure the impact for particular hinge functions created for a
given feature. For example, in Figure \ref{fig:vip} we see that
\texttt{Gr\_Liv\_Area} and \texttt{Year\_Built} are the two most
influential variables; however, variable importance does not tell us how
our model is treating the non-linear patterns for each feature. Also, if
we look at the interaction terms our model retained, we see interactions
between different hinge functions for \texttt{Gr\_Liv\_Area} and
\texttt{Year\_Built}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(tuned_mars}\OperatorTok{$}\NormalTok{finalModel) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(stringr}\OperatorTok{::}\KeywordTok{str_detect}\NormalTok{(names, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{*"}\NormalTok{))}
\NormalTok{## # A tibble: 16 x 2}
\NormalTok{##    names                                             x}
\NormalTok{##    <chr>                                         <dbl>}
\NormalTok{##  1 h(Year_Built-2003) * h(Gr_Liv_Area-2274)    1.87e+1}
\NormalTok{##  2 h(Year_Built-2003) * h(2274-Gr_Liv_Area)   -1.09e+1}
\NormalTok{##  3 h(Total_Bsmt_SF-1035) * Kitchen_QualTypi~  -3.31e+1}
\NormalTok{##  4 NeighborhoodEdwards * h(Gr_Liv_Area-2945)  -5.07e+2}
\NormalTok{##  5 h(Lot_Area-4058) * h(3-Garage_Cars)        -7.91e-1}
\NormalTok{##  6 h(2003-Year_Built) * h(Year_Remod_Add-19~   7.00e+0}
\NormalTok{##  7 Overall_QualExcellent * h(Total_Bsmt_SF-~   1.04e+2}
\NormalTok{##  8 NeighborhoodCrawford * h(2003-Year_Built)   4.24e+2}
\NormalTok{##  9 h(Lot_Area-4058) * Overall_CondFair        -3.29e+0}
\NormalTok{## 10 Overall_QualAbove_Average * h(2003-Year_~   1.36e+2}
\NormalTok{## 11 h(Lot_Area-4058) * Overall_CondGood         1.35e+0}
\NormalTok{## 12 Bsmt_ExposureNo * h(Total_Bsmt_SF-1035)    -2.25e+1}
\NormalTok{## 13 NeighborhoodGreen_Hills * h(5-Bedroom_Ab~   2.74e+4}
\NormalTok{## 14 Overall_QualVery_Good * Bsmt_QualGood      -1.86e+4}
\NormalTok{## 15 h(2003-Year_Built) * Sale_ConditionNormal   1.92e+2}
\NormalTok{## 16 h(Lot_Area-4058) * h(Full_Bath-2)           1.61e+0}
\end{Highlighting}
\end{Shaded}

To better understand the relationship between these features and
\texttt{Sale\_Price}, we can create partial dependence plots (PDPs) for
each feature individually and also an interaction PDP. The individual
PDPs illustrate that our model found that one knot in each feature
provides the best fit. For \texttt{Gr\_Liv\_Area}, as homes exceed 2,945
square feet, each additional square foot demands a higher marginal
increase in sale price than homes with less than 2,945 square feet.
Similarly, for homes built after 2003, there is a greater marginal
effect on sales price based on the age of the home than for homes built
prior to 2003. The interaction plot (far right plot) illustrates the
strong effect these two features have when combined.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(tuned_mars, }\DataTypeTok{pred.var =} \StringTok{"Gr_Liv_Area"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{autoplot}\NormalTok{()}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(tuned_mars, }\DataTypeTok{pred.var =} \StringTok{"Year_Built"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{autoplot}\NormalTok{()}
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{partial}\NormalTok{(tuned_mars, }\DataTypeTok{pred.var =} \KeywordTok{c}\NormalTok{(}\StringTok{"Gr_Liv_Area"}\NormalTok{, }\StringTok{"Year_Built"}\NormalTok{), }\DataTypeTok{grid.resolution =} \DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{plotPartial}\NormalTok{(}\DataTypeTok{levelplot =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{zlab =} \StringTok{"yhat"}\NormalTok{, }\DataTypeTok{drape =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{colorkey =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{screen =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{z =} \DecValTok{-20}\NormalTok{, }\DataTypeTok{x =} \DecValTok{-60}\NormalTok{))}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, p3, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/pdp-1}

\}

\textbackslash{}caption\{Partial dependence plots to understand the
relationship between \texttt{Sale\_Price} and the \texttt{Gr\_Liv\_Area}
and \texttt{Year\_Built} features. The PDPs tell us that as
\texttt{Gr\_Liv\_Area} increases and for newer homes,
\texttt{Sale\_Price} increases dramatically.\}\label{fig:pdp}
\textbackslash{}end\{figure\}

\hypertarget{attrition-data-1}{%
\section{Attrition data}\label{attrition-data-1}}

We saw significant improvement to our predictive accuracy on the Ames
data with a MARS model, but how about the attrition data? In Chapter
\ref{logistic-regression} we saw a slight improvement in our
cross-validated accuracy rate using regularized regression. Here, we
tune a MARS model using the same search grid as we did above. We see our
best models include no interaction effects and the optimal model retains
45 terms.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get attrition data}
\NormalTok{df <-}\StringTok{ }\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the rsample::attrition data.}
\CommentTok{# Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{churn_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{churn_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}


\CommentTok{# for reproducibiity}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# cross validated model}
\NormalTok{tuned_mars <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(churn_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Attrition),}
  \DataTypeTok{y =}\NormalTok{ churn_train}\OperatorTok{$}\NormalTok{Attrition,}
  \DataTypeTok{method =} \StringTok{"earth"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{tuned_mars}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##   nprune degree}
\NormalTok{## 5     45      1}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(tuned_mars)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/tuned-marts-attrition-1} 

}

\caption{Cross-validated accuracy rate for the 30 different hyperparameter combinations in our grid search. The optimal model retains 45 terms and includes no interaction effects.}\label{fig:tuned-marts-attrition}
\end{figure}

However, comparing our MARS model to the previous linear models
(logistic regression and regularized regression), we do not see any
improvement in our overall accuracy rate.

\begin{table}[H]
\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & NA's\\
\hline
Logistic\_model & 0.8058 & 0.8389 & 0.8586 & 0.8632 & 0.8949 & 0.9135 & 0\\
\hline
Elastic\_net & 0.8447 & 0.8568 & 0.8744 & 0.8787 & 0.9069 & 0.9135 & 0\\
\hline
MARS\_model & 0.7961 & 0.8450 & 0.8641 & 0.8593 & 0.8824 & 0.9135 & 0\\
\hline
\end{tabular}
\end{table}

\hypertarget{final-thoughts-3}{%
\section{Final thoughts}\label{final-thoughts-3}}

MARS provides a great stepping stone into nonlinear modeling and tends
to be fairly intuitive due to being closely related to multiple
regression techniques. They are also easy to train and tune. This
chapter illustrated how incorporating non-linear relationships via MARS
modeling greatly improved predictive accuracy on our Ames housing data.
The chapters that follow will explore additional non-linear algorithms
to see if we can further improve our predictive accuracy. The following
summarizes some of the advantages and disadvantages discussed regarding
MARS modeling:

\textbf{FIXME: refine this section}

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Accurate if the local linear relationships are correct.
\item
  Quick computation.
\item
  Can work well even with large and small data sets.
\item
  Provides automated feature selection.
\item
  The non-linear relationship between the features and response are
  fairly intuitive.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Not accurate if the local linear relationships are correct.
\item
  Typically not as accurate as more advanced non-linear algorithms
  (random forests, gradient boosting machines).
\item
  The \textbf{earth} package does not incorporate more advanced spline
  features (i.e.~Piecewise cubic models).
\item
  Missing values must be pre-processed.
\end{itemize}

\hypertarget{learning-more-3}{%
\section{Learning more}\label{learning-more-3}}

This will get you up and running with MARS modeling. Keep in mind that
there is a lot more you can dig into so the following resources will
help you learn more:

\begin{itemize}
\tightlist
\item
  \href{http://www-bcf.usc.edu/~gareth/ISL/}{An Introduction to
  Statistical Learning, Ch. 7}
\item
  \href{http://appliedpredictivemodeling.com/}{Applied Predictive
  Modeling, Ch. 7}
\item
  \href{https://statweb.stanford.edu/~tibs/ElemStatLearn/}{Elements of
  Statistical Learning, Ch. 5}
\item
  \href{http://www.milbo.org/doc/earth-notes.pdf}{Notes on the
  \texttt{earth} package} by Stephen Milborrow
\end{itemize}

\hypertarget{RF}{%
\chapter{Random Forests}\label{RF}}

The previous chapters covered models where the algorithm is based on a
linear expansions in simple basis functions of the form

\[
  \sum_{i=1}^p\beta_ih_i\left(\boldsymbol{x}\right),
  \label{eq:linear-combo}
\]

where the \(\beta_i\) are unknown coefficients to be estimated and the
\(h_i\left(\cdot\right)\) are transformations applied to the features
\(\boldsymbol{x}\). For ordinalry linear regression (with or without
regularization), these transformations are supplied by the user; hence,
these are parametric models. For example, the prediction equation
\(f\left(\boldsymbol{x}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + \beta_4x_1^2\)
has

\[h_1\left(\boldsymbol{x}\right) = x_1 \quad \text{(main effect})\]

\[h_2\left(\boldsymbol{x}\right) = x_2 \quad \text{(main effect})\]

\[h_3\left(\boldsymbol{x}\right) = x_2x_4 \quad \text{(two-way interaction effect)}\]

\[h_2\left(\boldsymbol{x}\right) = x_2^2 \quad \text{(quadratic effect})\]

MARS, on the other hand, uses a specific algorithm to find the
transformations to use automatically; hence, MARS is a nonparametric
model.

\emph{Tree-based models}, are also nonparametric, but they work very
differently. Tree-based models use algorithms to partition the feature
space into a number of smaller (non-overlapping) regions based on a set
of splitting rules and then fits a simpler model (e.g., a constant) in
each region. Such \emph{divide-and-conquor} methods (e.g., a single
decision tree) can produce simple rules that are easy to interpret and
visualize with \emph{tree diagrams}. Although fitted tree-based models
can still be written as a linear expansions in simple basis functions
(here the basis functions define the feature space partitioning), there
is no benfit to doing so as other techniques, like \emph{tree diagrams},
are better and conveying the information. Simple decision trees
typically lack in predictive performance compared to more complex
algorithms like neural networks and MARS. More sophisticated tree-based
models, such as random forests and gradient boosting machines, are less
interpretable, but tend to have very good predictive accuracy. This
chapter will get you familiar with the basics behind decision trees, and
two ways inwhich to combine them into a more accurate ensemble; namely,
bagging and random forests. In the next chapter, we'll cover
(stochastic) gradient boosting machines, which is another powerful way
of combining decision trees into a more accurate ensemble.

\hypertarget{prerequisites-7}{%
\section{Prerequisites}\label{prerequisites-7}}

For this chapter we'll use the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)         }\CommentTok{# for classification and regression training}
\KeywordTok{library}\NormalTok{(randomForest)  }\CommentTok{# for Breiman and Cutler's random forest}
\KeywordTok{library}\NormalTok{(ranger)        }\CommentTok{# for fast implementation of random forests}
\KeywordTok{library}\NormalTok{(rpart)         }\CommentTok{# for fitting CART-like decision trees}
\KeywordTok{library}\NormalTok{(rpart.plot)    }\CommentTok{# for flexible decision tree diagrams}
\KeywordTok{library}\NormalTok{(rsample)       }\CommentTok{# for data splitting }
\KeywordTok{library}\NormalTok{(pdp)           }\CommentTok{# for partial dependence plots}
\KeywordTok{library}\NormalTok{(vip)           }\CommentTok{# for variable importance plots}
\end{Highlighting}
\end{Shaded}

To illustrate the various concepts we'll use the Ames Housing data
(regression); however, at the end of the chapter we'll also apply fit a
random forest model to the employee attrition data (classification).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.}
\CommentTok{# Use set.seed for reproducibility}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ames_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{(), }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{)}
\NormalTok{ames_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(ames_split)}
\NormalTok{ames_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(ames_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{decision-trees}{%
\section{Decision trees}\label{decision-trees}}

There are many methodologies for constructing decision trees but the
most well-known is the \textbf{c}lassification \textbf{a}nd
\textbf{r}egression \textbf{t}ree (CART©) algorithm proposed in
\citet{breiman2017classification}. Basic decision trees partition the
training data into homogenious subgroups and then fit a simple
\emph{constant} in each (e.g., the mean of the within group response
values for regression). The partitioning is achieved by recursive binary
partitions formed by asking yes-or-no questions about each feature
(e.g., is \texttt{age\ \textless{}\ 18}?). This is done a number of
times until a suitable stopping critera is satiscfied (e.g., a maximum
depth of the tree is reached). After all the partitioning has been done,
the model predicts the output based on (1) the average response values
for all observations that fall in that subgroup (regression problem), or
(2) the class that has majority representation (classification problem).
For classification, predicted probabilites can be obtained using the
proportion of each class within the subgroups.

\begin{tip}
The basic idea behind decision trees is to ask simple yes-or-no
questions about each feature in order partion the training data into
subgroups with similar response rates. Ideally, the repsonses within
each subgroup will be as similar or homegenous as possible, while
responses across subgroups will be as different or as heterogenous as
possible.
\end{tip}

\hypertarget{a-simple-regression-tree-example}{%
\subsection{A simple regression tree
example}\label{a-simple-regression-tree-example}}

For example, suppose we want to use a decision tree to predict the miles
per gallon a car will average (\texttt{mpg}) based on two features: the
number of cylinders (\texttt{cyl}) and the horsepower (\texttt{hp});
such data are available in the \texttt{mtcars} data frame which is part
of the standard \textbf{datasets} package. A (regression) tree is built
using the \textbf{rpart} package \citep{R-rpart} (see the code chunk
below and Figure \ref{rf-decision-tree-example-image} which was produced
using the \textbf{rpart.plot} package \citep{R-rpart.plot}), and all the
training data are passed down this tree. Whenever an obervation reaches
a partiular node in the tree (i.e., a yes-or-no question about one of
the features; in this case, \texttt{cyl} or \texttt{hp}), it proceeds
either to the left (if the answer is ``yes'') or to the right (if the
answer is ``no''). This tree has only three terminal nodes. To start
traversing the tree, all observations that have 6 or 8 cylinders go to
the left branch, all other observations proceed to the right branch.
Next, the left branch is further partitioned by \texttt{hp}. Of all the
observations with 6 or 8 cylinders with \texttt{hp} equal to or greater
than 192 proceed to the left branch; those with less than 192
\texttt{hp} proceed to the right. These branches lead to \emph{terminal
nodes} or \emph{leafs} which contain the predicted response value; in
this case, the average mpg of cars that fall within that terminal node.
In short, cars with less than 5 cylinders (region \(R_1\)) average 27
mpg, cars with \texttt{cyl} \(\ge 5\) and \texttt{hp} \(< 193\) (region
\(R_2\)) average 18 mpg, and all other cars (region \(R_3\)) in the
training data average 13 mph. This is sumamrized in the tree diagram in
Figure \ref{fig:decision-tree-example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{cyl }\OperatorTok{+}\StringTok{ }\NormalTok{hp, }\DataTypeTok{data =}\NormalTok{ mtcars)  }\CommentTok{# CART-like regression tree}
\KeywordTok{rpart.plot}\NormalTok{(tree)  }\CommentTok{# tree diagram}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-decision-tree-example-1} 

}

\caption{Using a CART-like decision tree to predict `mpg` based on `cyl` and `hp`.}\label{fig:rf-decision-tree-example}
\end{figure}

Using a linear combination in simple basis functions, we can write the
predicted \texttt{mpg} as

\[
\widehat{mpg} = 27 \cdot I\left(cyl < 5\right) + 18 \cdot I\left(cyl \ge 5 \text{ and } hp < 193 \right) + 13 \cdot I\left(cyl \ge 5 \text{ and } hp \ge 193\right),
\]

where \(I\left(\cdot\right)\) is an \emph{indicator function} that
evaluates to one if its argument is true and zero otherwise. The
coefficients (i.e., 27, 18, 13) correspond to the average response value
within the respective region. While this simple example illustrates that
decision trees are estimating a model of the same form as equation
\eqref{eq:linear-combo}, the results are more easily interpreted in the
form of a tree diagram like in Figure \ref{fig:decision-tree-example}.

\hypertarget{deciding-on-splits}{%
\subsection{Deciding on splits}\label{deciding-on-splits}}

\textbf{FIXME:} Continue with the previous example?

How do decision trees partition the data into nonoverlapping regions? In
particular, how did the tree in Figure \ref{fig:decision-tree-example}
decide which variables to split on and which split points to use? The
answer depends on the tree algorithm used and therther or not context is
classification or regression. For CART-like decision trees (like those
discussed in this book and implemented in \textbf{rpart}), the
partitioning of the feature space is done in a top-down, \emph{greedy}
fashion. This means that any partion in the tree depends on the previous
partitions. But how are these partions made? The algorithm begins with
the entire training data set and searches every distinct value of every
input variable to find the ``best'' feature/split combination that
partitions the data into two regions (\(R_1\) and \(R_2\)) such that the
overall error is minimized (typically, SSE for regression problems, and
cross-entropy or the Gini index for classification problems--see Section
\ref{reg-perf-eval}). Having found the best feature/split combination,
the data are partitioned into two regions and the splitting process is
repeated on each of the two regions (hence the name \emph{binary
\textbf{r}ecursive \textbf{part}ioning}. This process is continued until
some stopping criterion is reached (e.g., a mxaimu depth is reached or
the tree becomes ``too complex''). What results is, typically, a very
deep, complex tree that may produce good predictions on the training
set, but is likely to generalize well to new unseen data (i.e.,
overfitting), leading to poor generalization performance.

To illustrate, we print the struture of the previous regression tree
below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(tree)}
\NormalTok{## n= 32 }
\NormalTok{## }
\NormalTok{## node), split, n, deviance, yval}
\NormalTok{##       * denotes terminal node}
\NormalTok{## }
\NormalTok{## 1) root 32 1126.00 20.09  }
\NormalTok{##   2) cyl>=5 21  198.50 16.65  }
\NormalTok{##     4) hp>=192.5 7   28.83 13.41 *}
\NormalTok{##     5) hp< 192.5 14   59.87 18.26 *}
\NormalTok{##   3) cyl< 5 11  203.40 26.66 *}
\end{Highlighting}
\end{Shaded}

The root node refers to all of the training data. In this node, the mean
response is 20.0906 and the SSE is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{((x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\KeywordTok{sse}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg)  }\CommentTok{# see the row labeled 1)}
\NormalTok{## [1] 1126}
\end{Highlighting}
\end{Shaded}

The feature/splint combination that gives the largest rediction to this
SSE is defined by the regions \texttt{cyl\textgreater{}=5} and
\texttt{cyl\textless{}\ 5}. The resulting SSEs within these regions are

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sse}\NormalTok{(mtcars[mtcars}\OperatorTok{$}\NormalTok{cyl }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{, ]}\OperatorTok{$}\NormalTok{mpg)  }\CommentTok{# see the row labeled 2)}
\NormalTok{## [1] 198.5}
\KeywordTok{sse}\NormalTok{(mtcars[mtcars}\OperatorTok{$}\NormalTok{cyl }\OperatorTok{<}\StringTok{  }\DecValTok{5}\NormalTok{, ]}\OperatorTok{$}\NormalTok{mpg)  }\CommentTok{# see the row labeled 3)}
\NormalTok{## [1] 203.4}
\end{Highlighting}
\end{Shaded}

The algorithm further split the region defined by
\texttt{cyl\textgreater{}=5} into to further regions. The feature/split
combination that gave the biggest reduction to that node's SSE (i.e.,
198.47240) is defined by the regions \texttt{hp\textgreater{}=192.5} and
\texttt{hp\textless{}\ 192.5}. The resulting SSEs within these regions
are

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sse}\NormalTok{(mtcars[mtcars}\OperatorTok{$}\NormalTok{cyl }\OperatorTok{>=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\NormalTok{mtcars}\OperatorTok{$}\NormalTok{hp }\OperatorTok{>=}\StringTok{ }\FloatTok{192.5}\NormalTok{, ]}\OperatorTok{$}\NormalTok{mpg)  }\CommentTok{# see the row labeled 4)}
\NormalTok{## [1] 28.83}
\KeywordTok{sse}\NormalTok{(mtcars[mtcars}\OperatorTok{$}\NormalTok{cyl }\OperatorTok{>=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\NormalTok{mtcars}\OperatorTok{$}\NormalTok{hp }\OperatorTok{<}\StringTok{  }\FloatTok{192.5}\NormalTok{, ]}\OperatorTok{$}\NormalTok{mpg)  }\CommentTok{# see the row labeled 5)}
\NormalTok{## [1] 59.87}
\end{Highlighting}
\end{Shaded}

At this point, a stopping criteria was reached (in this case, the
minimum number of observations that must exist in a node in order for a
split to be attempted) and the tree algorithm stopped partitioning the
data. For a list of all the stopping criteria, see
\texttt{?rpart::rpart.control}.

\hypertarget{bagging}{%
\section{Bagging}\label{bagging}}

The major drawback of decision trees is that they are not typically as
accurate as current state-of-the-art ML algorithms like neural networks.
However, decision trees to have a number of desirable properties which
are listed in Table X below. The idea behind the ensembles of decision
trees discussed in this chapter and the next is to improve the
predictive performance of trees while retaining most of the other
properties.

Table X. A comparison of binary recursive partitioning and neural
networks.

Property

Recursive \n partitioning

Neural networks

Naturally handles numeric and categorical variables

✅

❌

Naturally handles missing values

✅

❌

Robust to features with outliers

✅

❌

Insensitive to monotone transformations of features

✅

❌

Computational scalability

✅

❌

Ability to deal with irrelevant inputs

✅

❌

Interpretibility

✅

❌

Predictive accuracy

⚠️

✅

As discussed in \citet{friedman2001elements}, the key to accuracy is low
bias and low variance. Trees are naturally high-variance models; a small
change in the training data can lead to substantial different trees.
Although \emph{pruning} the tree (i.e., determining a nested sequence of
subtrees by recursively snipping off the least important splits) helps
reduce the variance\footnote{See \citet{esposito1997comparative} for
  various methods of pruning. In \textbf{rpart}, the amount of pruning
  is controlled by the \emph{complexity parameters} \texttt{cp}.} of a
single tree (i.e., by helping to avoid overfitting), there are methods
that exploit this variance in a way that can significantly improve
performance over and above that of single trees.
\emph{\textbf{B}ootstrap \textbf{agg}regat\textbf{ing}} (bagging)
\citep{breiman1996bagging} is one such approach.

Bagging creates an ensemble{[}\^{}Combining multiple models is referred
to as \emph{ensembling}.{]} of decision trees with low bias and high
variance. The bias of each tree is kept at a minimum by constructing
overgrown decision trees (i.e., no pruning). In other words, the tree
models constructed in bagging are intentionally overfitting the training
data. The variance of the final predictions is reduced by averaging
(regression) or popular vote (classification). In regression, for
example, the prediction of a new observation is obtained by averaging
the predictions from each individual tree. To construct bagger model of
size \(B\), we follow three simple steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create \(B\) bootstrap replciates of the original training data
  (Section \ref{bootstrap}) by selecting rows with replacement
\item
  For each bootstrap sample, train a single, unpruned decision tree
\item
  Average individual predictions from each tree to create an overall
  average predicted value (regression) or use a popular vote
  (classification)
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{illustrations/bagging} 

}

\caption{The bagging process.}\label{fig:rf-bagging-image}
\end{figure}

Technically, bagging can is a general algorithm that can be applied to
any regression or classification algorithm; however, it provides the
greatest improvement for models that are adaptive and have high
variance. For example, more stable parametric models, such as linear
regression and MARS, tend to experience less improvement in predictive
performance with bagging.

Although bagging trees can help reduce the variance of a single tree's
prediction and improve predictive performance, the trees in bagging are
not completely independent of each other since all the original
predictors are considered at every split of every tree. Rather, trees
from different bootstrap samples typically have similar structure to
each other (especially at the top of the tree) due to the fact that
stronger relationships appear at the top.

For example, if we create six decision trees with different bootstrapped
samples of the Boston housing data, we see that the top of the trees all
have a very similar structure. Although there are 15 predictor variables
to split on, all six trees have both \texttt{lstat} and \texttt{rm}
driving the first few splits. This \emph{between-tree correlation}
limits the effect of averaging to reduce the variance of the overall
ensemble. In order to reduce variance further, we need to take an
additiaonl step to help de-correlate the trees in the ensemble.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{abar_files/figure-latex/unnamed-chunk-98-1} 

}

\caption{Six bagged decision trees fit to the Boston housing data set.}\label{fig:unnamed-chunk-98}
\end{figure}

\hypertarget{random-forests}{%
\section{Random forests}\label{random-forests}}

Random forests are an extension of bagging and injects more randomness
into the tree-growing process. Random forests achieve this in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bootstrap}: similar to bagging, each tree is grown to a
  bootstrap resampled data set, which makes them different and
  \emph{somewhat} decorrelates them.
\item
  \textbf{Split-variable randomization}: each time a split is to be
  performed, the search for the split variable is limited to a random
  subset of \emph{m} of the \emph{p} variables. Typical default values
  for \(m\) are \(m = \frac{p}{3}\) (regression problems) and
  \(m = \sqrt{p}\) for classification models. However, this should be
  considered a tuning parameter. When \(m = p\), the randomization
  amounts to using only step 1 and is the same as \emph{bagging}.
\end{enumerate}

The basic algorithm for a random forest model can be generalized to the
following:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{1.}\NormalTok{  Given training data set}
\FloatTok{2.}\NormalTok{  Select number of trees to }\KeywordTok{build}\NormalTok{ (ntrees)}
\FloatTok{3.}  \ControlFlowTok{for}\NormalTok{ i =}\StringTok{ }\DecValTok{1}\NormalTok{ to ntrees do}
\FloatTok{4.}  \OperatorTok{|}\StringTok{  }\NormalTok{Generate a bootstrap sample of the original data}
\FloatTok{5.}  \OperatorTok{|}\StringTok{  }\NormalTok{Grow a regression or classification tree to the bootstrapped data}
\FloatTok{6.}  \OperatorTok{|}\StringTok{  }\ControlFlowTok{for}\NormalTok{ each split do}
\FloatTok{7.}  \OperatorTok{|}\StringTok{  }\ErrorTok{|}\StringTok{ }\NormalTok{Select m variables at random from all p variables}
\FloatTok{8.}  \OperatorTok{|}\StringTok{  }\ErrorTok{|}\StringTok{ }\NormalTok{Pick the best variable}\OperatorTok{/}\NormalTok{split}\OperatorTok{-}\NormalTok{point among the m}
\FloatTok{9.}  \OperatorTok{|}\StringTok{  }\ErrorTok{|}\StringTok{ }\NormalTok{Split the node into two child nodes}
\FloatTok{10.} \OperatorTok{|}\StringTok{  }\NormalTok{end}
\FloatTok{11.} \OperatorTok{|}\StringTok{ }\NormalTok{Use typical tree model stopping criteria to determine when a tree is }\KeywordTok{complete}\NormalTok{ (but do not prune)}
\FloatTok{12.}\NormalTok{ end}
\end{Highlighting}
\end{Shaded}

Since the algorithm randomly selects a bootstrap sample to train on
\textbf{\emph{and}} predictors to use at each split, tree correlation
will be lessened beyond bagged trees.

\hypertarget{oob-error-vs.test-set-error}{%
\subsection{OOB error vs.~test set
error}\label{oob-error-vs.test-set-error}}

One benefit of bagging (and thus also random forests) is that, on
average, a bootstrap sample will contain 63\% of the training data. This
leaves about 37\% of the data out of the bootstrapped sample. We call
this the out-of-bag (OOB) sample. We can use the OOB observations to
estimate the model's accuracy, creating a natural cross-validation
process, which allows you to not need to sacrifice any of your training
data to use for validation. This makes identifying the number of trees
required to stablize the error rate during tuning more efficient;
however, as illustrated below some difference between the OOB error and
test error are expected.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{illustrations/oob-error-compare-1} 

}

\caption{Random forest out-of-bag error versus validation error.}\label{fig:rf-oob-example-image}
\end{figure}

Furthermore, many packages do not keep track of which observations were
part of the OOB sample for a given tree and which were not. If you are
comparing multiple models to one-another, you'd want to score each on
the same validation set to compare performance. Also, although
technically it is possible to compute certain metrics such as root mean
squared logarithmic error (RMSLE) on the OOB sample, it is not built in
to all packages. So if you are looking to compare multiple models or use
a slightly less traditional loss function you will likely want to still
perform cross validation.

\hypertarget{fitting-a-basic-random-forest-model}{%
\section{Fitting a basic random forest
model}\label{fitting-a-basic-random-forest-model}}

There are over 20 random forest packages in R.\footnote{See the Random
  Forest section in the
  \href{https://CRAN.R-project.org/view=MachineLearning}{Machine
  Learning Task View} on CRAN and Erin LeDell's
  \href{https://koalaverse.github.io/machine-learning-in-R/random-forest.html\#random-forest-software-in-r}{useR!
  Machine Learning Tutorial} for a non-comprehensive list.} To
demonstrate the basic implementation we illustrate the use of the
\textbf{randomForest} package \citep{R-randomForest}, the oldest and
most well known implementation of the Random Forest algorithm in R.

\begin{tip}
However, as your data set grows in size \textbf{randomForest} does not
scale well (although you can parallelize with \textbf{foreach}).
\end{tip}

\texttt{randomForest()} can use the formula or separate x, y matrix
notation for specifying our model. Below we apply the default
\textbf{randomForest} model using the formulaic specification. The
default random forest performs 500 trees and \(\frac{features}{3} = 26\)
randomly selected predictor variables at each split. Averaging across
all 500 trees provides an OOB \(MSE = 661089658\) (\(RMSE = \$25,711\)).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for reproduciblity}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# default RF model}
\NormalTok{rf1 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
  \DataTypeTok{data =}\NormalTok{ ames_train}
\NormalTok{)}

\NormalTok{rf1}
\NormalTok{## }
\NormalTok{## Call:}
\NormalTok{##  randomForest(formula = Sale_Price ~ ., data = ames_train) }
\NormalTok{##                Type of random forest: regression}
\NormalTok{##                      Number of trees: 500}
\NormalTok{## No. of variables tried at each split: 26}
\NormalTok{## }
\NormalTok{##           Mean of squared residuals: 661089658}
\NormalTok{##                     % Var explained: 89.8}
\end{Highlighting}
\end{Shaded}

Plotting the model will illustrate the OOB error rate as we average
across more trees and shows that our error rate stabalizes with around
100 trees but continues to decrease slowly until around 300 or so trees.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rf1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-basic-model-plot-1} 

}

\caption{OOB error (MSE) as a function of the number of trees.  We see the MSE reduces quickly for the first 100 trees and then slowly thereafter.  We want to make sure that we are providing enough trees so that our OOB error has stabalized or flatlined.}\label{fig:rf-basic-model-plot}
\end{figure}

The plotted error rate above is based on the OOB sample error and can be
accessed directly at \texttt{rf1\$mse}. Thus, we can find which number
of trees provides the lowest error rate, which is 447 trees providing an
average home sales price error of \$25,649.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# number of trees with lowest MSE}
\KeywordTok{which.min}\NormalTok{(rf1}\OperatorTok{$}\NormalTok{mse)}
\NormalTok{## [1] 447}

\CommentTok{# RMSE of this optimal random forest}
\KeywordTok{sqrt}\NormalTok{(rf1}\OperatorTok{$}\NormalTok{mse[}\KeywordTok{which.min}\NormalTok{(rf1}\OperatorTok{$}\NormalTok{mse)])}
\NormalTok{## [1] 25649}
\end{Highlighting}
\end{Shaded}

Random forests are one of the best ``out-of-the-box'' machine learning
algorithms. They typically perform remarkably well with very little
tuning required. As illustrated above, we were able to get an RMSE of
\$25,649 without any tuning which is nearly as good as the best, fully
tuned model we've explored thus far. However, we can still seek
improvement by tuning hyperparameters in our random forest model.

\hypertarget{tuning-1}{%
\section{Tuning}\label{tuning-1}}

Compared to the algorithms explored in the previous chapters, random
forests have more hyperparameters to tune. However, compared to gradient
boosting machines and neural networks, which we explore in future
chapters, random forests are much easier to tune. Typically, the primary
concern when starting out is tuning the number of candidate variables to
select from at each split.

\begin{tip}
The two primary tuning parameters you should always tune in a random
forest model are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Number of trees as you want to ensure you apply enough trees to
  minimize and stabalize the error rate.
\item
  Number of candidate variables to select from at each split.
\end{enumerate}
\end{tip}

However, there are a few additional hyperparameters that we should be
aware of. Although the argument names may differ across packages, these
hyperparameters should be present:

\begin{itemize}
\tightlist
\item
  \textbf{Number of trees}: We want enough trees to stabalize the error
  but using too many trees is unncessarily inefficient, especially when
  using large data sets.
\item
  \textbf{Number of variables to randomly sample as candidates at each
  split}: Commonly referred to as ``mtry''. When \texttt{mtry} \(=p\)
  the model equates to bagging. When \texttt{mtry} \(=1\) the split
  variable is completely random, so all variables get a chance but can
  lead to overly biased results. A common suggestion is to start with 5
  values evenly spaced across the range from 2 to \emph{p}.
\item
  \textbf{Sample size to train on}: The default value is 63.25\% of the
  training set since this is the expected value of unique observations
  in the bootstrap sample. Lower sample sizes can reduce the training
  time but may introduce more bias than necessary. Increasing the sample
  size can increase performance but at the risk of overfitting because
  it introduces more variance. Typically, when tuning this parameter we
  stay near the 60-80\% range.
\item
  \textbf{Minimum number of samples within the terminal nodes}: Controls
  the complexity of the trees. Smaller node size allows for deeper, more
  complex trees and larger node size results in shallower trees. This is
  another bias-variance tradeoff where deeper trees introduce more
  variance (risk of overfitting) and shallower trees introduce more bias
  (risk of not fully capturing unique patters and relatonships in the
  data).
\item
  \textbf{Maximum number of terminal nodes}: Another way to control the
  complexity of the trees. More nodes equates to deeper, more complex
  trees and less nodes result in shallower trees.
\item
  \textbf{Split rule}:As stated in the introduction, the most
  traditional splitting rules are based on minimizing the variance or
  MSE across the terminal nodes for regression problems and
  Cross-entropy or Gini index for classification problems. However,
  additional splitrules have been developed that can offer improved
  predictive accuracy. For example, the extra trees split rule chooses
  cut-points fully at random and uses the whole learning sample (rather
  than a bootstrap replica) to grow the trees
  \citep{geurts2006extremely}.
\end{itemize}

Tuning a larger set of hyperparameters requires a larger grid search
than we've performed thus far. Unfortunately, this is where
\textbf{randomForest} becomes quite inefficient since it does not scale
well. Instead, we can use \textbf{ranger} \citep{R-ranger} which is a
C++ implementation of Brieman's random forest algorithm and, as the
following illustrates, is over 27 times faster than
\textbf{randomForest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# names of features}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(ames_train), }\StringTok{"Sale_Price"}\NormalTok{)}

\CommentTok{# randomForest speed}
\KeywordTok{system.time}\NormalTok{(}
\NormalTok{  ames_randomForest <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}
    \DataTypeTok{formula =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
    \DataTypeTok{data    =}\NormalTok{ ames_train, }
    \DataTypeTok{ntree   =} \DecValTok{500}\NormalTok{,}
    \DataTypeTok{mtry    =} \KeywordTok{floor}\NormalTok{(}\KeywordTok{length}\NormalTok{(features) }\OperatorTok{/}\StringTok{ }\DecValTok{3}\NormalTok{)}
\NormalTok{  )}
\NormalTok{)}
\NormalTok{##    user  system elapsed }
\NormalTok{##  34.113   0.044  34.162}

\CommentTok{# ranger speed}
\KeywordTok{system.time}\NormalTok{(}
\NormalTok{  ames_ranger <-}\StringTok{ }\KeywordTok{ranger}\NormalTok{(}
    \DataTypeTok{formula   =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
    \DataTypeTok{data      =}\NormalTok{ ames_train, }
    \DataTypeTok{num.trees =} \DecValTok{500}\NormalTok{,}
    \DataTypeTok{mtry      =} \KeywordTok{floor}\NormalTok{(}\KeywordTok{length}\NormalTok{(features) }\OperatorTok{/}\StringTok{ }\DecValTok{3}\NormalTok{)}
\NormalTok{  )}
\NormalTok{)}
\NormalTok{##    user  system elapsed }
\NormalTok{##   6.971   0.108   1.053}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning-via-ranger}{%
\subsection{Tuning via ranger}\label{tuning-via-ranger}}

There are two approaches to tuning a \textbf{ranger} model. The first is
to tune \textbf{ranger} manually using a \texttt{for} loop. To perform a
manual grid search, first we want to construct our grid of
hyperparameters. We're going to search across 48 different models with
varying mtry, minimum node size, sample size, and trying different split
rules.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a tuning grid}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{mtry            =} \KeywordTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DataTypeTok{by =} \DecValTok{5}\NormalTok{),}
  \DataTypeTok{min.node.size   =} \KeywordTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DataTypeTok{by =} \DecValTok{3}\NormalTok{),}
  \DataTypeTok{sample.fraction =} \KeywordTok{c}\NormalTok{(.}\DecValTok{632}\NormalTok{, }\FloatTok{.80}\NormalTok{),}
  \DataTypeTok{splitrule       =} \KeywordTok{c}\NormalTok{(}\StringTok{"variance"}\NormalTok{, }\StringTok{"extratrees"}\NormalTok{),}
  \DataTypeTok{OOB_RMSE        =} \DecValTok{0}
\NormalTok{)}

\KeywordTok{dim}\NormalTok{(hyper_grid)}
\NormalTok{## [1] 48  5}
\end{Highlighting}
\end{Shaded}

We loop through each hyperparameter combination and apply 500 trees
since our previous examples illustrated that 500 was plenty to achieve a
stable error rate. Also note that we set the random number generator
seed. This allows us to consistently sample the same observations for
each sample size and make it more clear the impact that each change
makes. Our OOB RMSE ranges between \textasciitilde{}25,900-28,500. Our
top 10 performing models all have RMSE values right around 26,000 and
the results show that models with larger sample sizes (80\%) and a
variance splitrule perform best. However, no definitive evidence
suggests that certain values of \texttt{mtry} or \texttt{min.node.size}
are better than other values.

\begin{warning}
This grid search took 69 seconds to complete.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \KeywordTok{seq_len}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(hyper_grid))) \{}
  
  \CommentTok{# train model}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{ranger}\NormalTok{(}
    \DataTypeTok{formula         =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
    \DataTypeTok{data            =}\NormalTok{ ames_train, }
    \DataTypeTok{num.trees       =} \DecValTok{500}\NormalTok{,}
    \DataTypeTok{mtry            =}\NormalTok{ hyper_grid}\OperatorTok{$}\NormalTok{mtry[i],}
    \DataTypeTok{min.node.size   =}\NormalTok{ hyper_grid}\OperatorTok{$}\NormalTok{min.node.size[i],}
    \DataTypeTok{sample.fraction =}\NormalTok{ hyper_grid}\OperatorTok{$}\NormalTok{sample.fraction[i],}
    \DataTypeTok{splitrule       =}\NormalTok{ hyper_grid}\OperatorTok{$}\NormalTok{splitrule[i],}
    \DataTypeTok{seed            =} \DecValTok{123}
\NormalTok{  )}
  
  \CommentTok{# add OOB error to grid}
\NormalTok{  hyper_grid}\OperatorTok{$}\NormalTok{OOB_RMSE[i] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(model}\OperatorTok{$}\NormalTok{prediction.error)}
\NormalTok{\}}

\NormalTok{hyper_grid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{arrange}\NormalTok{(OOB_RMSE) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{##    mtry min.node.size sample.fraction splitrule}
\NormalTok{## 1    20             3             0.8  variance}
\NormalTok{## 2    25             3             0.8  variance}
\NormalTok{## 3    20             6             0.8  variance}
\NormalTok{## 4    20             9             0.8  variance}
\NormalTok{## 5    30             3             0.8  variance}
\NormalTok{## 6    25             6             0.8  variance}
\NormalTok{## 7    30             6             0.8  variance}
\NormalTok{## 8    35             3             0.8  variance}
\NormalTok{## 9    35             6             0.8  variance}
\NormalTok{## 10   25             9             0.8  variance}
\NormalTok{##    OOB_RMSE}
\NormalTok{## 1     25964}
\NormalTok{## 2     25981}
\NormalTok{## 3     25982}
\NormalTok{## 4     26091}
\NormalTok{## 5     26123}
\NormalTok{## 6     26145}
\NormalTok{## 7     26167}
\NormalTok{## 8     26176}
\NormalTok{## 9     26189}
\NormalTok{## 10    26210}
\end{Highlighting}
\end{Shaded}

However, using this approach does not provide us with a cross validated
measure of error. To get a k-fold CV error, we would have to expand our
\texttt{for} loop approach or use an alternative approach. One such
approach follows.

\hypertarget{tuning-via-caret}{%
\subsection{Tuning via caret}\label{tuning-via-caret}}

The second tuning approach is to use the \textbf{caret} package.
\textbf{caret} only allows you to tune some, not all, of the available
\textbf{ranger} hyperparameters (\texttt{mtry}, \texttt{splitrule},
\texttt{min.node.size}). However, \textbf{caret} will allow us to get a
CV measure of error to compare to our previous models (i.e.~regularized
regression, MARS). The following creates a similar tuning grid as before
but with only those hyperparameters that \textbf{caret} will accept.

\begin{tip}
If you do not know what hyperparameters \textbf{caret} allows you to
tune for a specific model you can find that info at
\url{https://topepo.github.io/caret/train-models-by-tag.html} or with
\texttt{caret::getModelInfo}. For example, we can find the parameters
available for a \texttt{ranger} with
\texttt{caret::getModelInfo("ranger")\$ranger\$parameter}.
\end{tip}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a tuning grid}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{mtry            =} \KeywordTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DataTypeTok{by =} \DecValTok{5}\NormalTok{),}
  \DataTypeTok{min.node.size   =} \KeywordTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DataTypeTok{by =} \DecValTok{3}\NormalTok{),}
  \DataTypeTok{splitrule       =} \KeywordTok{c}\NormalTok{(}\StringTok{"variance"}\NormalTok{, }\StringTok{"extratrees"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Tuning with \textbf{caret} provides similar results as our
\textbf{ranger} grid search. Both results suggest \texttt{mtry\ =\ 20}
and \texttt{min.node.size\ =\ 3}. With \textbf{ranger}, our OOB RMSE was
25963.96 and with \textbf{caret} our 10-fold CV RMSE was 25531.47.

\begin{warning}
This grid search took a little over 6 minutes to complete.
\end{warning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cross validated model}
\NormalTok{tuned_rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(ames_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Sale_Price),}
  \DataTypeTok{y =}\NormalTok{ ames_train}\OperatorTok{$}\NormalTok{Sale_Price,}
  \DataTypeTok{method =} \StringTok{"ranger"}\NormalTok{,}
  \DataTypeTok{metric =} \StringTok{"RMSE"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid,}
  \DataTypeTok{num.trees =} \DecValTok{500}\NormalTok{,}
  \DataTypeTok{seed =} \DecValTok{123}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{tuned_rf}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##   mtry splitrule min.node.size}
\NormalTok{## 7   25  variance             3}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(tuned_rf)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-grid-search2-1}

\}

\textbackslash{}caption\{Cross validated RMSE for the \textbf{caret}
grid search.\}\label{fig:rf-grid-search2} \textbackslash{}end\{figure\}

\hypertarget{feature-interpretation-2}{%
\section{Feature interpretation}\label{feature-interpretation-2}}

Whereas many of the linear models discussed previously use the
standardized coefficients to signal importance, random forests have,
historically, applied two different approaches to measure variable
importance.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Impurity}: At each split in each tree, compute the improvement
  in the split-criterion (MSE for regression Gini for classification).
  Then average the improvement made by each variable across all the
  trees that the variable is used. The variables with the largest
  average decrease in the error metric are considered most important.
\item
  \textbf{Permutation}: For each tree, the OOB sample is passed down the
  tree and the prediction accuracy is recorded. Then the values for each
  variable (one at a time) are randomly permuted and the accuracy is
  again computed. The decrease in accuracy as a result of this randomly
  ``shaking up'' of variable values is averaged over all the trees for
  each variable. The variables with the largest average decrease in
  accuracy are considered most important.
\end{enumerate}

To compute these variable importance measures with \textbf{ranger}, you
must include the importance argument.

\begin{note}
Once you've identified the optimal parameter values from the grid
search, you will want to re-run your model with these hyperparameter
values.
\end{note}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# re-run model with impurity-based variable importance}
\NormalTok{rf_impurity <-}\StringTok{ }\KeywordTok{ranger}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data            =}\NormalTok{ ames_train, }
  \DataTypeTok{num.trees       =} \DecValTok{500}\NormalTok{,}
  \DataTypeTok{mtry            =} \DecValTok{20}\NormalTok{,}
  \DataTypeTok{min.node.size   =} \DecValTok{3}\NormalTok{,}
  \DataTypeTok{sample.fraction =} \FloatTok{.80}\NormalTok{,}
  \DataTypeTok{splitrule       =} \StringTok{"variance"}\NormalTok{,}
  \DataTypeTok{importance      =} \StringTok{'impurity'}\NormalTok{,}
  \DataTypeTok{verbose         =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{seed            =} \DecValTok{123}
\NormalTok{  )}

\CommentTok{# re-run model with permutation-based variable importance}
\NormalTok{rf_permutation <-}\StringTok{ }\KeywordTok{ranger}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Sale_Price }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data            =}\NormalTok{ ames_train, }
  \DataTypeTok{num.trees       =} \DecValTok{500}\NormalTok{,}
  \DataTypeTok{mtry            =} \DecValTok{20}\NormalTok{,}
  \DataTypeTok{min.node.size   =} \DecValTok{3}\NormalTok{,}
  \DataTypeTok{sample.fraction =} \FloatTok{.80}\NormalTok{,}
  \DataTypeTok{splitrule       =} \StringTok{"variance"}\NormalTok{,}
  \DataTypeTok{importance      =} \StringTok{'permutation'}\NormalTok{,}
  \DataTypeTok{verbose         =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{seed            =} \DecValTok{123}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

For both options, you can directly access the variable importance values
with \texttt{model\_name\$variable.importance}. However, here we'll plot
the variable importance using the \texttt{vip} package. Typically, you
will not see the same variable importance order between the two options;
however, you will often see similar variables at the top of the plots.
Consquently, in this example, we can comfortably state that there
appears to be enough evidence to suggest that two variables stand out as
most influential:

\begin{itemize}
\tightlist
\item
  \texttt{Overall\_Qual}
\item
  \texttt{Gr\_Liv\_Area}
\end{itemize}

Looking at the next \textasciitilde{}10 variables in both plots, you
will also see some commonality in influential variables (i.e.
\texttt{Garage\_Cars}, \texttt{Bsmt\_Qual}, \texttt{Year\_Built},
\texttt{Exter\_Qual}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(rf_impurity, }\DataTypeTok{num_features =} \DecValTok{25}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Impurity-based variable importance"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{vip}\NormalTok{(rf_permutation, }\DataTypeTok{num_features =} \DecValTok{25}\NormalTok{, }\DataTypeTok{bar =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Permutation-based variable importance"}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-vip-plots-1} 

}

\caption{Top 25 most important variables based on impurity (left) and permutation (right).}\label{fig:rf-vip-plots}
\end{figure}

To better understand the relationship between these important features
and \texttt{Sale\_Price}, we can create partial dependence plots (PDPs).
If you recall in the linear and regularized regression sections, we saw
that the linear model assumed a continously increasing relationship
between \texttt{Gr\_Liv\_Area} and \texttt{Sale\_Price}. In the MARS
chapter, we saw as homes exceed 2,945 square feet, each additional
square foot demands a higher marginal increase in sale price than homes
with less than 2,945 square feet. However, in between the knots of a
MARS model, the relationship will remain linear. However, the PDP plot
below displays how random forest models can capture unique non-linear
and non-monotonic relationships between predictors and the target. In
this case, \texttt{Sale\_Price} appears to not be influenced by
\texttt{Gr\_Liv\_Area} values below 750 sqft or above 3500 sqft. This
change in realtionship was not well captured by the prior parametric
models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partial dependence of Sale_Price on Gr_Liv_Area}
\NormalTok{rf_impurity }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{partial}\NormalTok{(}\DataTypeTok{pred.var =} \StringTok{"Gr_Liv_Area"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{50}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{autoplot}\NormalTok{(}\DataTypeTok{rug =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{train =}\NormalTok{ ames_train)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-pdp-GrLiv-Area-1} 

}

\caption{The mean predicted sale price as the above ground living area increases.}\label{fig:rf-pdp-GrLiv-Area}
\end{figure}

Additionally, if we assess the relationship between the
\texttt{Overall\_Qual} predictor and \texttt{Sale\_Price}, we see a
continual increase as the overall quality increases. This provides a
more comprehensive understanding of the relationship than the results we
saw in the regularized regression section (\ref{lm-features}). We see
that the largest impact on \texttt{Sale\_Price} occurs when houses go
from ``Good'' overall quality to ``Very Good''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partial dependence of Sale_Price on Overall_Qual}
\NormalTok{rf_impurity }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{partial}\NormalTok{(}\DataTypeTok{pred.var =} \StringTok{"Overall_Qual"}\NormalTok{, }\DataTypeTok{train =} \KeywordTok{as.data.frame}\NormalTok{(ames_train)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-pdp-Overall-Qual-1} 

}

\caption{The mean predicted sale price for each level of the overall quality variable.}\label{fig:rf-pdp-Overall-Qual}
\end{figure}

Individual conditional expectation (ICE) curves
\citep{goldstein2015peeking} are an extension of PDP plots but, rather
than plot the \emph{average} marginal effect on the response variable,
we plot the change in the predicted response variable \textbf{\emph{for
each observation}} as we vary each predictor variable. Below shows the
regular ICE curve plot (left) and the centered ICE curves (right). When
the curves have a wide range of intercepts and are consequently
``stacked'' on each other, heterogeneity in the response variable values
due to marginal changes in the predictor variable of interest can be
difficult to discern. The centered ICE can help draw these inferences
out and can highlight any strong heterogeneity in our results.

The plots below show that marginal changes in \texttt{Gr\_Liv\_Area}
have a fairly homogenous effect on our response variable. As
\texttt{Gr\_Liv\_Area} increases, the vast majority of observations show
a similar increasing effect on the predicted \texttt{Sale\_Price} value.
The primary differences is in the magnitude of the increasing effect.
However, in the centered ICE plot you see evidence of a few observations
that display a different pattern. Some have a higher \(\hat y\) value
when \texttt{Gr\_Liv\_Area} is between 2500-4000 and some have a
decreasing \(\hat y\) as \texttt{Gr\_Liv\_AreA} increases. These results
may be a sign of interaction effects and would be worth exploring more
closely.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ice curves of Sale_Price on Gr_Liv_Area}
\NormalTok{ice1 <-}\StringTok{ }\NormalTok{rf_impurity }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{partial}\NormalTok{(}\DataTypeTok{pred.var =} \StringTok{"Gr_Liv_Area"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{50}\NormalTok{, }\DataTypeTok{ice =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{autoplot}\NormalTok{(}\DataTypeTok{rug =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{train =}\NormalTok{ ames_train, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Non-centered ICE plot"}\NormalTok{)}

\NormalTok{ice2 <-}\StringTok{ }\NormalTok{rf_impurity }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{partial}\NormalTok{(}\DataTypeTok{pred.var =} \StringTok{"Gr_Liv_Area"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{50}\NormalTok{, }\DataTypeTok{ice =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{autoplot}\NormalTok{(}\DataTypeTok{rug =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{train =}\NormalTok{ ames_train, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Centered ICE plot"}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(ice1, ice2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-ice-Gr-Liv-Area-1} 

}

\caption{Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations.}\label{fig:rf-ice-Gr-Liv-Area}
\end{figure}

Both PDPs and ICE curves should be assessed for the most influential
variables as they help to explain the underlying patterns in the data
that the random forest model is picking up.

\hypertarget{attrition-data-2}{%
\section{Attrition data}\label{attrition-data-2}}

With the Ames data, the random forest models obtained predictive
accuracy that was close to our best MARS model, but how about the
attrition data? The following performs a grid search across 48
hyperparameter combinations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get attrition data}
\NormalTok{df <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate_if}\NormalTok{(is.ordered, factor, }\DataTypeTok{ordered =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Create training (70%) and test (30%) sets for the rsample::attrition data.}
\CommentTok{# Use set.seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{churn_split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(df, }\DataTypeTok{prop =} \FloatTok{.7}\NormalTok{, }\DataTypeTok{strata =} \StringTok{"Attrition"}\NormalTok{)}
\NormalTok{churn_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(churn_split)}
\NormalTok{churn_test  <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(churn_split)}

\CommentTok{# create a tuning grid}
\NormalTok{hyper_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
  \DataTypeTok{mtry            =} \KeywordTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DataTypeTok{by =} \DecValTok{3}\NormalTok{),}
  \DataTypeTok{min.node.size   =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{by =} \DecValTok{3}\NormalTok{),}
  \DataTypeTok{splitrule       =} \KeywordTok{c}\NormalTok{(}\StringTok{"gini"}\NormalTok{, }\StringTok{"extratrees"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# cross validated model}
\NormalTok{tuned_rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}
  \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(churn_train, }\DataTypeTok{select =} \OperatorTok{-}\NormalTok{Attrition),}
  \DataTypeTok{y =}\NormalTok{ churn_train}\OperatorTok{$}\NormalTok{Attrition,}
  \DataTypeTok{method =} \StringTok{"ranger"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =}\NormalTok{ hyper_grid,}
  \DataTypeTok{num.trees =} \DecValTok{500}\NormalTok{,}
  \DataTypeTok{seed =} \DecValTok{123}
\NormalTok{)}

\CommentTok{# best model}
\NormalTok{tuned_rf}\OperatorTok{$}\NormalTok{bestTune}
\NormalTok{##    mtry splitrule min.node.size}
\NormalTok{## 31   12      gini            10}

\CommentTok{# plot results}
\KeywordTok{ggplot}\NormalTok{(tuned_rf)}
\end{Highlighting}
\end{Shaded}

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.7\linewidth]{abar_files/figure-latex/rf-tuned-rf-attrition-1}

\}

\textbackslash{}caption\{Cross-validated accuracy rate for the 48
different hyperparameter combinations in our grid search. The optimal
model uses \texttt{mtry} = 9, \texttt{splitrule} = gini, and
\texttt{min.node.size} = 4, which obtained a 10-fold CV accuracy rate of
85.8\%.\}\label{fig:rf-tuned-rf-attrition} \textbackslash{}end\{figure\}

Similar to the MARS model, the random forest model does not improve
predictive accuracy over an above the regularized regression model.

\begin{table}[H]
\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & NA's\\
\hline
Logistic\_model & 0.8447 & 0.8533 & 0.8558 & 0.8670 & 0.8900 & 0.8942 & 0\\
\hline
Elastic\_net & 0.8269 & 0.8662 & 0.8744 & 0.8749 & 0.8900 & 0.9314 & 0\\
\hline
MARS\_model & 0.8039 & 0.8474 & 0.8647 & 0.8651 & 0.8811 & 0.9314 & 0\\
\hline
RF\_model & 0.8235 & 0.8410 & 0.8654 & 0.8612 & 0.8802 & 0.9020 & 0\\
\hline
\end{tabular}
\end{table}

\hypertarget{final-thoughts-4}{%
\section{Final thoughts}\label{final-thoughts-4}}

Random forests provide a very powerful out-of-the-box algorithm that
often has great predictive accuracy. Because of their more simplistic
tuning nature and the fact that they require very little, if any,
feature pre-processing they are often one of the first go-to algorithms
when facing a predictive modeling problem. However, as we illustrated in
this chapter, random forests are not guaranteed to improve predictive
accuracy over and above linear models and their cousins. The following
summarizes some of the advantages and disadvantages discussed regarding
random forests modeling:

\textbf{TODO}: may need to better tie in some of these advantages and
disadvantages throughout the chapter.

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  Typically have very good performance.
\item
  Remarkably good ``out-of-the box'' - very little tuning required.
\item
  Built-in validation set - don't need to sacrifice data for extra
  validation.
\item
  Does not overfit.
\item
  No data pre-processing required - often works great with categorical
  and numerical values as is.
\item
  Robust to outliers.
\item
  Handles missing data - imputation not required.
\item
  Provide automatic feature selection.
\end{itemize}

\textbf{Disadvantages:}

\begin{itemize}
\tightlist
\item
  Can become slow on large data sets.
\item
  Although accurate, often cannot compete with the accuracy of advanced
  boosting algorithms.
\item
  Less interpretable although this is easily addressed with various
  tools (variable importance, partial dependence plots, LIME, etc.).
\end{itemize}

\hypertarget{learning-more-4}{%
\section{Learning more}\label{learning-more-4}}

The literature behind random forests are rich and we have only touched
on the fundamentals. To learn more I would start with the following
resources listed in order of complexity:

\begin{itemize}
\tightlist
\item
  \href{http://www-bcf.usc.edu/~gareth/ISL/}{An Introduction to
  Statistical Learning}
\item
  \href{http://appliedpredictivemodeling.com/}{Applied Predictive
  Modeling}
\item
  \href{https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894}{Computer
  Age Statistical Inference}
\item
  \href{https://web.stanford.edu/~hastie/ElemStatLearn/}{The Elements of
  Statistical Learning}
\end{itemize}

\hypertarget{appendix-data}{%
\chapter{(APPENDIX) Appendix \{-\}}\label{appendix-data}}

\hypertarget{data-sets}{%
\chapter*{Data sets}\label{data-sets}}
\addcontentsline{toc}{chapter}{Data sets}

We have strived to use real data sets throughout the book. An
introduction to each data set, as well as it's source and how to import
it, is given in the following subsections.

\hypertarget{ames-iowa-housing-data-1}{%
\section*{Ames Iowa housing data}\label{ames-iowa-housing-data-1}}
\addcontentsline{toc}{section}{Ames Iowa housing data}

\citet{ames-cock-2011} describes a data set containing the sale of
individual residential property in Ames, Iowa from 2006 to 2010. The
data set contains 2930 observations and a large number of explanatory
variables involved in assessing home values. These data offer a
contemporary alternative to the often used Boston housing data described
in \citet{harrison1978hedonic}.

The Ames housing data, which we refer to as simply the \texttt{ames}
data, are available in the \texttt{AmesHousing} package
\citep{pkg-AmesHousing} which is available from CRAN:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"AmesHousing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The raw data are also available from Kaggle:
\url{https://www.kaggle.com/c/house-prices-advanced-regression-techniques}.
In the code chunk below, we use the \texttt{make\_ames()} function from
\texttt{AmesHousing} to create a processed version of the data. For full
details on the difference between the processed and raw versions of the
\texttt{ames} data, see the help file \texttt{?AmesHousing::make\_ames}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames <-}\StringTok{ }\NormalTok{AmesHousing}\OperatorTok{::}\KeywordTok{make_ames}\NormalTok{()}
\KeywordTok{dim}\NormalTok{(ames)}
\NormalTok{## [1] 2930   81}
\KeywordTok{table}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(ames, class))}
\NormalTok{## }
\NormalTok{##  factor integer numeric }
\NormalTok{##      46      23      12}
\end{Highlighting}
\end{Shaded}

The function \texttt{make\_ames()} returns a \texttt{"tibble"} object,
rather than just an R data frame. For more information on
\emph{tibbles}, see the corresponding vignette available in the
\texttt{tibble} package \citep{R-tibble}
\texttt{browseVignettes(package\ =\ "tibble")}. Running the code chunk
above, we see that there are 2930 observations on 81 variables (46 are
factors, 23 are integer valued, and 12 are numeric).

\hypertarget{employee-attrition-data-1}{%
\section*{Employee attrition data}\label{employee-attrition-data-1}}
\addcontentsline{toc}{section}{Employee attrition data}

Due to the continuing concerns organizations have with attracting and
retaining top talent, the
\href{https://www.ibm.com/communities/analytics/watson-analytics-blog}{IBM
Watson Analytics Lab} published an employee attrition data set that
allows analysts to explore factors that lead to employee attrition and
investigate important questions such as `show me a breakdown of distance
from home by job role and attrition' or `compare average monthly income
by education and attrition'.

The employee attrition data, which we refer to as simply the
\texttt{churn} data, are available in the \texttt{rsample} package
\citep{pkg-rsample} which is available from CRAN:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"rsample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The raw data are also available from IBM Watson Analytics Lab:
\url{https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/}.
In the code chunk below, we import the attrition data from
\texttt{rsample}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn <-}\StringTok{ }\NormalTok{rsample}\OperatorTok{::}\NormalTok{attrition}
\KeywordTok{dim}\NormalTok{(churn)}
\NormalTok{## [1] 1470   31}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(churn)}
\NormalTok{## Observations: 1,470}
\NormalTok{## Variables: 31}
\NormalTok{## $ Age                      <int> 41, 49, 37, 33, 2...}
\NormalTok{## $ Attrition                <fct> Yes, No, Yes, No,...}
\NormalTok{## $ BusinessTravel           <fct> Travel_Rarely, Tr...}
\NormalTok{## $ DailyRate                <int> 1102, 279, 1373, ...}
\NormalTok{## $ Department               <fct> Sales, Research_D...}
\NormalTok{## $ DistanceFromHome         <int> 1, 8, 2, 3, 2, 2,...}
\NormalTok{## $ Education                <ord> College, Below_Co...}
\NormalTok{## $ EducationField           <fct> Life_Sciences, Li...}
\NormalTok{## $ EnvironmentSatisfaction  <ord> Medium, High, Ver...}
\NormalTok{## $ Gender                   <fct> Female, Male, Mal...}
\NormalTok{## $ HourlyRate               <int> 94, 61, 92, 56, 4...}
\NormalTok{## $ JobInvolvement           <ord> High, Medium, Med...}
\NormalTok{## $ JobLevel                 <int> 2, 2, 1, 1, 1, 1,...}
\NormalTok{## $ JobRole                  <fct> Sales_Executive, ...}
\NormalTok{## $ JobSatisfaction          <ord> Very_High, Medium...}
\NormalTok{## $ MaritalStatus            <fct> Single, Married, ...}
\NormalTok{## $ MonthlyIncome            <int> 5993, 5130, 2090,...}
\NormalTok{## $ MonthlyRate              <int> 19479, 24907, 239...}
\NormalTok{## $ NumCompaniesWorked       <int> 8, 1, 6, 1, 9, 0,...}
\NormalTok{## $ OverTime                 <fct> Yes, No, Yes, Yes...}
\NormalTok{## $ PercentSalaryHike        <int> 11, 23, 15, 11, 1...}
\NormalTok{## $ PerformanceRating        <ord> Excellent, Outsta...}
\NormalTok{## $ RelationshipSatisfaction <ord> Low, Very_High, M...}
\NormalTok{## $ StockOptionLevel         <int> 0, 1, 0, 0, 1, 0,...}
\NormalTok{## $ TotalWorkingYears        <int> 8, 10, 7, 8, 6, 8...}
\NormalTok{## $ TrainingTimesLastYear    <int> 0, 3, 3, 3, 3, 2,...}
\NormalTok{## $ WorkLifeBalance          <ord> Bad, Better, Bett...}
\NormalTok{## $ YearsAtCompany           <int> 6, 10, 0, 8, 2, 7...}
\NormalTok{## $ YearsInCurrentRole       <int> 4, 7, 0, 7, 2, 7,...}
\NormalTok{## $ YearsSinceLastPromotion  <int> 0, 1, 0, 3, 2, 3,...}
\NormalTok{## $ YearsWithCurrManager     <int> 5, 7, 0, 0, 2, 6,...}
\end{Highlighting}
\end{Shaded}

Running the code chunk above, we see that there are 1470 observations on
31 variables, which consist of a mixture of integers, factors, and
ordered factors data types).

\bibliography{book.bib,packages.bib}


\end{document}
