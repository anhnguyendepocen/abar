[
["index.html", "Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods Preface Who this book is for Who this book is not for Who this book is really not for Why R How this book is organized Conventions used in this book Using code examples Feedback Acknowledgments Software information", " Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods Bradley C. Boehmke and Brandon M. Greenwell 2018-10-21 Preface TODO: Rewrite/fill in each section Somewhere we need to highlight that a lot of good tips are given in the code comments as well!! Is it ‚Äúin R‚Äù or ‚Äúwith R‚Äù in the book title/description? I‚Äôve seen both in a few places. I‚Äôm starting to think ‚Äúwith R‚Äù is the better choice? ü§î I agree‚Ä¶it should be ‚Äú‚Ä¶with R‚Äù Damn, do I need to get rid of the emo::jis in this book!? Rewrite more specific to this book Welcome to Advanced Business Analytics in R. This book provides practical modules for many of the descriptive, predictive, and prescriptive analytic methodologies‚Ä¶ Who this book is for We intend this work to be a practitioner‚Äôs guide to the machine learning process and a place where one can come to learn about the approach and to gain intuition about the many commonly used, modern, and powerful methods accepted in the machine learning community. If you are familiar with the analytic methodologies, this book may still serve as a reference for how to work with the various R packages for implementation. While an abundance of videos, blog posts, and tutorials exist online, I‚Äôve long been frustrated by the lack of consistency, completeness, and bias towards singular packages for implementation. This is what inspired this book. Part I of this book assumes basic knowledge of statistics (e.g., hypothesis testing, effect sizes, and confidence intervals). Part II assumes knowledge of regression and some knowledge of optimization (e.g., through business calculus). Part III ‚Ä¶ Who this book is not for FIXME: But this book is not just about machine learning? I know, this was just filler material I copied-n-pasted. Instead, this book is meant to help R users learn to use the machine learning stack within R, which includes using various R packages such as glmnet, h2o, ranger, xgboost, lime, and others to effectively model and gain insight from your data. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory. While you can read this book without opening R, I highly recommend you experiment with the code examples provided throughout. Who this book is really not for TBD. Why R R has emerged over the last couple decades as a first-class tool for scientific computing tasks, and has been a consistent leader in implementing fundamental and advanced methodologies for analyzing data. The usefulness of R for data science stems from the large, active, and growing ecosystem of third-party packages: tidyverse for common data analysis activities; glmnet, ranger, xgboost, and others for fast and scalable machine learning; lime, pdp, DALEX, and others for machine learning interpretability; and many more tools will be mentioned throughout the pages that follow. How this book is organized Each chapter of this book focuses on a particular part of the descriptive, predictive, and prescriptive analytic processes along with various packages to perform those processes. TBD‚Ä¶ There are many great resources available to learn about machine learning. At the end of each chapter we provide a Learn More section that lists resources that we have found extremely useful for digging deeper into the methodology and applying with code. Conventions used in this book The following typographical conventions are used in this book: italic: indicates new terms, bold: indicates package &amp; file names, inline code: indicates commands or other text that could be typed literally by the user (function names are followed by parentheses (i.e. blogdown::serve_site())), use of double-colon operator :: means accessing an object from a package (i.e. dplyr::filter() uses the filter() function from the dplyr package), code chunk: indicates commands or other text that could be typed literally by the user. We do not add prompts (&gt; and +) to R source code in this book, and we comment out the text output with two hashes ## by default 1 + 2 ## [1] 3 In addition to the general text used throughout, you will notice the following code chunks with images, which signify: This block signifies a tip or suggestion This block signifies a general note This block signifies a warning or caution Using code examples TBD. Feedback Reader comments are greatly appreciated. To report errors or bugs please post an issue at https://github.com/koalaverse/abar/issues. Acknowledgments TBD Software information An online version of this book is available at TBD. The source of the book is available at https://github.com/koalaverse/abar. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with the following packages and R version. # packages used pkgs &lt;- c( &quot;AmesHousing&quot;, &quot;arules&quot;, &quot;boot&quot;, &quot;bookdown&quot;, &quot;caret&quot;, &quot;cluster&quot;, &quot;earth&quot;, &quot;factoextra&quot;, &quot;ggplot2&quot;, &quot;glmnet&quot;, &quot;gridExtra&quot;, &quot;knitr&quot;, &quot;pdp&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;rmarkdown&quot;, &quot;rsample&quot;, &quot;stats&quot;, &quot;tibble&quot;, &quot;vip&quot; ) # package &amp; session info devtools::session_info(pkgs) ## Session info ----------------------------------------- ## setting value ## version R version 3.5.1 (2018-07-02) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/New_York ## date 2018-10-21 ## Packages --------------------------------------------- ## package * version date ## abind 1.4-5 2016-07-21 ## AmesHousing 0.0.3 2017-12-17 ## arules 1.6-1 2018-04-07 ## assertthat 0.2.0 2017-04-11 ## backports 1.1.2 2017-12-13 ## base64enc 0.1-3 2015-07-28 ## BH 1.66.0-1 2018-02-13 ## bindr 0.1.1 2018-03-13 ## bindrcpp 0.2.2 2018-03-29 ## bookdown 0.7.17 2018-08-14 ## boot 1.3-20 2017-08-06 ## broom 0.5.0 2018-07-17 ## car 3.0-0 2018-04-02 ## carData 3.0-1 2018-03-28 ## caret 6.0-80 2018-05-26 ## cellranger 1.1.0 2016-07-27 ## class 7.3-14 2015-08-30 ## cli 1.0.0 2017-11-05 ## cluster 2.0.7-1 2018-04-13 ## codetools 0.2-15 2016-10-05 ## colorspace 1.3-2 2016-12-14 ## compiler 3.5.1 2018-07-05 ## cowplot 0.9.3 2018-07-15 ## crayon 1.3.4 2017-09-16 ## curl 3.2 2018-03-28 ## CVST 0.2-2 2018-05-26 ## data.table 1.11.4 2018-05-27 ## datasets * 3.5.1 2018-07-05 ## ddalpha 1.3.4 2018-06-23 ## dendextend 1.8.0 2018-05-03 ## DEoptimR 1.0-8 2016-11-19 ## digest 0.6.18 2018-10-10 ## dimRed 0.1.0 2017-05-04 ## diptest 0.75-7 2016-12-05 ## dplyr 0.7.6 2018-06-29 ## DRR 0.0.3 2018-01-06 ## earth 4.6.3 2018-05-07 ## ellipse 0.4.1 2018-01-05 ## evaluate 0.11 2018-07-17 ## factoextra 1.0.5 2017-08-22 ## FactoMineR 1.41 2018-05-04 ## fansi 0.2.3 2018-05-06 ## flashClust 1.01-2 2012-08-21 ## flexmix 2.3-14 2017-04-28 ## forcats 0.3.0 2018-02-19 ## foreach 1.4.4 2017-12-12 ## foreign 0.8-70 2017-11-28 ## fpc 2.1-11.1 2018-07-20 ## geometry 0.3-6 2015-09-09 ## ggplot2 3.0.0 2018-07-03 ## ggpubr 0.1.7 2018-06-23 ## ggrepel 0.8.0 2018-05-09 ## ggsci 2.9 2018-05-14 ## ggsignif 0.4.0 2017-08-03 ## glmnet 2.0-16 2018-04-02 ## glue 1.3.0 2018-08-14 ## gower 0.1.2 2017-02-23 ## graphics * 3.5.1 2018-07-05 ## grDevices * 3.5.1 2018-07-05 ## grid 3.5.1 2018-07-05 ## gridExtra 2.3 2017-09-09 ## gtable 0.2.0 2016-02-26 ## haven 1.1.2 2018-06-27 ## highr 0.7 2018-06-09 ## hms 0.4.2 2018-03-10 ## htmltools 0.3.6 2017-04-28 ## ipred 0.9-6 2017-03-01 ## iterators 1.0.10 2018-07-13 ## jsonlite 1.5 2017-06-01 ## kernlab 0.9-26 2018-04-30 ## KernSmooth 2.23-15 2015-06-29 ## knitr 1.20 2018-02-20 ## labeling 0.3 2014-08-23 ## lattice 0.20-35 2017-03-25 ## lava 1.6.2 2018-07-02 ## lazyeval 0.2.1 2017-10-29 ## leaps 3.0 2017-01-10 ## lme4 1.1-17 2018-04-03 ## lubridate 1.7.4 2018-04-11 ## magic 1.5-8 2018-01-26 ## magrittr 1.5 2014-11-22 ## maptools 0.9-3 2018-07-31 ## markdown 0.8 2017-04-20 ## MASS 7.3-50 2018-04-30 ## Matrix 1.2-14 2018-04-13 ## MatrixModels 0.4-1 2015-08-22 ## mclust 5.4.1 2018-06-27 ## methods * 3.5.1 2018-07-05 ## mgcv 1.8-24 2018-06-23 ## mime 0.5 2016-07-07 ## minqa 1.2.4 2014-10-09 ## ModelMetrics 1.2.0 2018-08-10 ## modeltools 0.2-22 2018-07-16 ## munsell 0.5.0 2018-06-12 ## mvtnorm 1.0-8 2018-05-31 ## nlme 3.1-137 2018-04-07 ## nloptr 1.0.4 2017-08-22 ## nnet 7.3-12 2016-02-02 ## numDeriv 2016.8-1 2016-08-27 ## openxlsx 4.1.0 2018-05-26 ## parallel 3.5.1 2018-07-05 ## pbkrtest 0.4-7 2017-03-15 ## pdp 0.6.0 2017-07-20 ## pillar 1.3.0 2018-07-14 ## pkgconfig 2.0.1 2017-03-21 ## plogr 0.2.0 2018-03-25 ## plotmo 3.4.2 2018-07-03 ## plotrix 3.7-2 2018-05-27 ## pls 2.6-0 2016-12-18 ## plyr 1.8.4 2016-06-08 ## prabclus 2.2-6 2015-01-14 ## prodlim 2018.04.18 2018-04-18 ## purrr 0.2.5 2018-05-29 ## quantreg 5.36 2018-05-29 ## R6 2.2.2 2017-06-17 ## randomForest 4.6-14 2018-03-25 ## ranger 0.10.1 2018-06-04 ## RColorBrewer 1.1-2 2014-12-07 ## Rcpp 0.12.18 2018-07-23 ## RcppEigen 0.3.3.4.0 2018-02-07 ## RcppRoll 0.3.0 2018-06-05 ## readr 1.1.1 2017-05-16 ## readxl 1.1.0 2018-04-20 ## recipes 0.1.3 2018-06-16 ## rematch 1.0.1 2016-04-21 ## reshape2 1.4.3 2017-12-11 ## rio 0.5.10 2018-03-29 ## rlang 0.2.2 2018-08-16 ## rmarkdown 1.10 2018-06-11 ## robustbase 0.93-2 2018-07-27 ## rpart 4.1-13 2018-02-23 ## rprojroot 1.3-2 2018-01-03 ## rsample 0.0.2 2017-11-12 ## scales 1.0.0 2018-08-09 ## scatterplot3d 0.3-41 2018-03-14 ## sfsmisc 1.1-2 2018-03-05 ## sp 1.3-1 2018-06-05 ## SparseM 1.77 2017-04-23 ## splines 3.5.1 2018-07-05 ## SQUAREM 2017.10-1 2017-10-07 ## stats * 3.5.1 2018-07-05 ## stats4 3.5.1 2018-07-05 ## stringi 1.2.4 2018-07-20 ## stringr 1.3.1 2018-05-10 ## survival 2.42-3 2018-04-16 ## TeachingDemos 2.10 2016-02-12 ## tibble 1.4.2 2018-01-22 ## tidyr 0.8.1 2018-05-18 ## tidyselect 0.2.4 2018-02-26 ## timeDate 3043.102 2018-02-21 ## tinytex 0.6 2018-07-07 ## tools 3.5.1 2018-07-05 ## trimcluster 0.1-2.1 2018-07-20 ## utf8 1.1.4 2018-05-24 ## utils * 3.5.1 2018-07-05 ## vip 0.1.0 2018-08-22 ## viridis 0.5.1 2018-03-29 ## viridisLite 0.3.0 2018-02-01 ## whisker 0.3-2 2013-04-28 ## withr 2.1.2 2018-03-15 ## xfun 0.3 2018-07-06 ## yaml 2.2.0 2018-07-25 ## zip 1.0.0 2017-04-25 ## source ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## Github (rstudio/bookdown@02b0fd1) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## cran (@0.6.18) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## Github (tidyverse/glue@a292148) ## CRAN (R 3.5.0) ## local ## local ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## cran (@1.2.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## cran (@0.2.2) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.0) ## local ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.1) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## local ## Github (koalaverse/vip@32890d0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) ## CRAN (R 3.5.0) "],
["author.html", "Who are these Guys? Bradley C. Boehmke Brandon M. Greenwell", " Who are these Guys? This book is‚Ä¶ Bradley C. Boehmke Brad Boehmke is a Data Scientist at 84.51¬∞, where he and his team develop algorithmic processes, solutions, and tools that enable 84.51¬∞ and its analysts to efficiently extract insights from data and provide solution alternatives to decision-makers. Brad is also an adjunct professor at the University of Cincinnati, Wake Forest, and Air Force Institute of Technology. He‚Äôs an active contributor to the R community through package development (i.e. anomalyDetection, sure, vip) and giving back via advanced machine learning education available at the UC Business Analytics R Programming Guide. You can follow Brad on Twitter (https://twitter.com/bradleyboehmke) and GitHub (https://github.com/bradleyboehmke). Brandon M. Greenwell Brandon Greenwell is‚Ä¶ You can follow me on Twitter (https://twitter.com/bgreenwell8) and GitHub (https://github.com/bgreenwell). "],
["intro.html", "Chapter 1 Introduction 1.1 Descriptive 1.2 Predictive 1.3 Prescriptive 1.4 Data sets", " Chapter 1 Introduction TODO 1.1 Descriptive TODO 1.2 Predictive TODO 1.3 Prescriptive TODO 1.4 Data sets We have strived to use data sets throughout the book that represent real world applications. An introduction to each data set, as well as it‚Äôs source and how to import it, is given in the following subsections. Ames Iowa housing data TODO: introduce theoretical business problem Suppose that you are an analyst working for a local real estate firm in Ames, Iowa. You have been asked to develop a model to help real estate agents Cock (2011) describes a data set containing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables involved in assessing home values. These data offer a contemporary alternative to the often used Boston housing data described in Harrison Jr and Rubinfeld (1978). The Ames housing data, which we refer to as simply the ames data, are available in the AmesHousing package (Kuhn 2017) which is available from CRAN: install.packages(&quot;AmesHousing&quot;) The raw data are also available from Kaggle: https://www.kaggle.com/c/house-prices-advanced-regression-techniques. In the code chunk below, we use the make_ames() function from AmesHousing to create a processed version of the data. For full details on the difference between the processed and raw versions of the ames data, see the help file ?AmesHousing::make_ames. ames &lt;- AmesHousing::make_ames() dim(ames) ## [1] 2930 81 table(sapply(ames, class)) ## ## factor integer numeric ## 46 23 12 The function make_ames() returns a &quot;tibble&quot; object, rather than just an R data frame. For more information on tibbles, see the corresponding vignette available in the tibble package (M√ºller and Wickham 2018) browseVignettes(package = &quot;tibble&quot;). Running the code chunk above, we see that there are 2930 observations on 81 variables (46 are factors, 23 are integer valued, and 12 are numeric). Employee attrition data TODO: introduce theoretical business problem Due to the continuing concerns organizations have with attracting and retaining top talent, the IBM Watson Analytics Lab published an employee attrition data set that allows analysts to explore factors that lead to employee attrition and investigate important questions such as ‚Äòshow me a breakdown of distance from home by job role and attrition‚Äô or ‚Äòcompare average monthly income by education and attrition‚Äô. The employee attrition data, which we refer to as simply the churn data, are available in the rsample package (Kuhn and Wickham 2017) which is available from CRAN: install.packages(&quot;rsample&quot;) The raw data are also available from IBM Watson Analytics Lab: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/. In the code chunk below, we import the attrition data from rsample. churn &lt;- rsample::attrition dim(churn) ## [1] 1470 31 dplyr::glimpse(churn) ## Observations: 1,470 ## Variables: 31 ## $ Age &lt;int&gt; 41, 49, 37, 33, 2... ## $ Attrition &lt;fct&gt; Yes, No, Yes, No,... ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Tr... ## $ DailyRate &lt;int&gt; 1102, 279, 1373, ... ## $ Department &lt;fct&gt; Sales, Research_D... ## $ DistanceFromHome &lt;int&gt; 1, 8, 2, 3, 2, 2,... ## $ Education &lt;ord&gt; College, Below_Co... ## $ EducationField &lt;fct&gt; Life_Sciences, Li... ## $ EnvironmentSatisfaction &lt;ord&gt; Medium, High, Ver... ## $ Gender &lt;fct&gt; Female, Male, Mal... ## $ HourlyRate &lt;int&gt; 94, 61, 92, 56, 4... ## $ JobInvolvement &lt;ord&gt; High, Medium, Med... ## $ JobLevel &lt;int&gt; 2, 2, 1, 1, 1, 1,... ## $ JobRole &lt;fct&gt; Sales_Executive, ... ## $ JobSatisfaction &lt;ord&gt; Very_High, Medium... ## $ MaritalStatus &lt;fct&gt; Single, Married, ... ## $ MonthlyIncome &lt;int&gt; 5993, 5130, 2090,... ## $ MonthlyRate &lt;int&gt; 19479, 24907, 239... ## $ NumCompaniesWorked &lt;int&gt; 8, 1, 6, 1, 9, 0,... ## $ OverTime &lt;fct&gt; Yes, No, Yes, Yes... ## $ PercentSalaryHike &lt;int&gt; 11, 23, 15, 11, 1... ## $ PerformanceRating &lt;ord&gt; Excellent, Outsta... ## $ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, M... ## $ StockOptionLevel &lt;int&gt; 0, 1, 0, 0, 1, 0,... ## $ TotalWorkingYears &lt;int&gt; 8, 10, 7, 8, 6, 8... ## $ TrainingTimesLastYear &lt;int&gt; 0, 3, 3, 3, 3, 2,... ## $ WorkLifeBalance &lt;ord&gt; Bad, Better, Bett... ## $ YearsAtCompany &lt;int&gt; 6, 10, 0, 8, 2, 7... ## $ YearsInCurrentRole &lt;int&gt; 4, 7, 0, 7, 2, 7,... ## $ YearsSinceLastPromotion &lt;int&gt; 0, 1, 0, 3, 2, 3,... ## $ YearsWithCurrManager &lt;int&gt; 5, 7, 0, 0, 2, 6,... Running the code chunk above, we see that there are 1470 observations on 31 variables, which consist of a mixture of integers, factors, and ordered factors data types). References "],
["descriptive.html", "Chapter 2 Descriptive Statistics 2.1 Prerequisites 2.2 Measures of location 2.3 Measures of spread 2.4 Percentiles 2.5 Robust measures of spread 2.6 Outlier detection 2.7 Describing categorical data 2.8 Exercises", " Chapter 2 Descriptive Statistics The first step in any data analysis problem is to describe the data using descriptive statistics and look at the data using appropriate graphical techniques. In this chapter, we will introduce some basic descriptive statistics (e.g., various measures of location and spread) while graphical techniques are discussed in Chapter 3. Descriptive statistics, in contrast to inferential statistics (Chapter 4), aim to describe a data sample, or sample. A sample is simply a set of data collected from some population of interest (e.g., the annual salaries of males at a particular company). In this book, we refer to the individual sample points as observations. Typically, the data are collected in a way such that the sample is representative of the population from which it came. Other times, we may have access to the entire population, in which case, our sample comprises a census. In either case, descriptive statistics seek to paint a quantitative picture of the data using simple values such as measures of location (i.e., what a typical values looks like) and dispersion (i.e., how spread out the data are). Beyond measures of location and spread, we also discuss percentiles and simple ways for detecting outliers (i.e., unusually small or large observations). Various other descriptive statistics and useful R packages are introduced in the exercises at the end of the chapter. 2.1 Prerequisites The R package stats (which is part of the standard R distribution) provides functions for many of the descriptive statistics discussed in this chapter, plus more. For a complete list of functions, type library(help = &quot;stats&quot;) into the R console. For illustration, we will use two data sets: (1) the Ames housing data set (Cock 2011) (in particular, the column labelled Sale_Price) and (2) the Adult data set (Lichman 2013); both of which are described in Chapter 1. # Ames housing data dim(ames &lt;- AmesHousing::make_ames()) # construct data and print dimensions ## [1] 2930 81 head(Sale_Price &lt;- ames$Sale_Price) # extract Sale_Price column ## [1] 215000 105000 172000 244000 189900 195500 # Adult data data(AdultUCI, package = &quot;arules&quot;) # load data from arules package dim(AdultUCI) # print dimensions ## [1] 48842 15 2.2 Measures of location The first question we might ask of the ames data is ‚ÄúWhat is a typical value for the selling price?‚Äù In particular, we are interested in some measure of central location or central tendency. Such measures try to summarize a set of values with a ‚Äútypical‚Äù number. There are a number of different measures of location, but the simplest and most commonly used is the arithmetic mean or sample mean. 2.2.1 The sample mean Suppose we have a set of \\(n\\) observations denoted \\(x_1, x_2, \\dots, x_n\\). The sample mean, denoted \\(\\bar{x}\\), is defined as the sum of the observations divided \\(n\\): \\[\\begin{equation} \\label{eqn:sample-mean} \\bar{x} = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\frac{1}{n}\\left(x_1 + x_2 + \\dots + x_n\\right), \\end{equation}\\] where \\(\\sum\\) is mathematical notation for summation and simply means ‚Äúadd up the values‚Äù. Another way to think of the sample mean is as the ``center of gravity‚Äô‚Äô for a set of observations. That is, if the observations were placed on a number line, like a teeter-totter, the sample mean would be the balancing point. For example, the sample mean of 1.7, 3.3, 7.5, 8.1, and 8.9 is \\(\\bar{x} = 5.9\\) which is displayed in Figure 2.1. Figure 2.1: The sample mean as the balancing point of a set of five observations. In R, we can obtain the sample mean of a set of observations using the mean() function mean(c(1.7, 3.3, 7.5, 8.1, 8.9)) ## [1] 5.9 For example, the sample mean of Sale_Price can be obtained using mean(Sale_Price) ## [1] 180796 mean(Sale_Price) # alternatively ## [1] 180796 Thus, a typical or central value of selling price for the ames data frame would be $180,796. Most of the descriptive statistical functions in R include an na.rm argument which defaults to FALSE. If na.rm = FALSE, the return value for these functions will be NA (i.e., R‚Äôs representation of a missing value; see ?NA for details.) whenever the sample contains at least one NA. If set to TRUE, then all NAs will be removed from the sample prior to computing the statistic. This is illustrated in the following code chunk: x &lt;- c(18, 4, 12, NA, 19) mean(x) ## [1] NA mean(x, na.rm = TRUE) ## [1] 13.25 (18 + 4 + 12 + 19) / 4 # sanity check ## [1] 13.25 2.2.2 The sample median One problem with the sample mean is that it is not robust to outliers. To illustrate, suppose we have a sample of size two: \\(x_1\\) and \\(x_2\\) so that \\(\\bar{x} = \\left(x_1 + x_2\\right) / 2\\). Then, regardless of the value of \\(x_1\\), we can change \\(x_2\\) to achieve any value for the sample mean. Since it only takes one of the \\(n\\) observations to arbitrarily change \\(\\bar{x}\\), we say that \\(\\bar{x}\\) has a finite sample breakdown point (FSBP) of \\(1 / n\\). The higher the FSBP of a sample statistic, the less affected it is to outliers (i.e., the more robust it is). The highest FSBP a sample statistic can obtain is 50%. A more robust measure of location is given by the sample median. Consider a sample of size \\(n\\): \\(x_1, x_2, \\dots, x_n\\). Let \\(x_{\\left(i\\right)}\\) denote the \\(i\\)-th observation after the sample has been sorted in ascending order. The sample median, denoted \\(M\\), is defined as \\[\\begin{equation*} M = \\begin{cases} x_{\\left(m\\right)} &amp; \\quad \\text{if } n \\text{ is odd}\\\\ \\left(x_{\\left(m\\right)} + x_{\\left(m + 1\\right)}\\right) / 2 &amp; \\quad \\text{if } n \\text{ is even}, \\end{cases} \\end{equation*}\\] where \\(m = \\left(n + 1\\right) / 2\\). In other words, if \\(n\\) is odd, the sample median is just the middle number, otherwise, we take the sample mean of the two middle numbers. Since the sample median only depends on the middle number (or middle two numbers), it is far more robust than the sample mean. In fact, the sample median has an FSBP of roughly 50%. In other words, close to 50% of the observations have to be outliers in order to affect the sample median. This makes the sample median more useful in practice when dealing with data that contain outliers or come from skewed distributions. In R, we can compute the sample median using the median() function. For the Ames housing data, the median sale price is $160,000, which can be computed using median(Sale_Price) ## [1] 160000 median(ames$Sale_Price) # alternatively ## [1] 160000 2.2.3 The mean or the median So which should be used in practice, the sample mean or the sample median? If the data are roughly symmetric, then the sample mean and sample median will be approximately equal. In fact, when the sample mean is most useful for reporting, it will typically be close to the sample median. If, however, the data are skewed to the left or right, then the sample mean will tend to get pulled in the same direction. For example, for right (or positively) skewed data, the sample mean will typically be larger than the sample median (sometimes much larger). In these cases, the sample median is a more reliable measure of location. However, there is nothing wrong with reporting both statistics. For the ames data frame, the sample median was smaller than the sample mean by $20,796.06. This is not surprising since data on housing and sale prices tend to be skewed right which can inflate the sample mean. In this case, the sample median will be a better measure of location than the sample mean. Different approaches to detecting skewness will be discussed in Chapter 3 when we talk about visualizing data. 2.3 Measures of spread Measures of location by themselves are not very useful. We often want to know how ``spread out‚Äô‚Äô the data are. This can be summarized using various measures of spread or dispersion. The most common measure of spread is the sample variance, denoted by \\(s^2\\). The sample variance of a sample is defined as \\[\\begin{equation*} s ^ 2 = \\sum_{i = 1} ^ n \\left(x_i - \\bar{x}\\right) ^ 2 / \\left(n - 1\\right). \\end{equation*}\\] That is, the sample variance is just the sum of the squared deviations of each observation from the sample mean dived by \\(n - 1\\). The \\(n - 1\\) is used to make \\(s ^ 2\\) an unbiased estimator1 of the population variance. Since the sample variance involves squaring the differences, it does not retain the original units, unlike the sample mean and variance. Oftentimes the positive square root of the sample variance, called the sample standard deviation, is used instead. The sample standard deviation, denoted \\(s\\), is more useful because is has the same units as the original observations (e.g., feet, dollars, etc.). In R, the sample variance and sample standard deviation can be computed using the functions var() and sd(), respectively. The following example illustrates their use on the ames data frame using the variable Sale_Price. var(Sale_Price) # sample variance ## [1] 6.382e+09 sd(Sale_Price) # sample standard deviation ## [1] 79887 sqrt(var(Sale_Price)) # sanity check ## [1] 79887 Since the sample standard deviation retains the original units, we can report this in dollar amount (e.g., the standard deviation of sale prices for homes sold from 2006‚Äì2010 is $79,886.69). 2.3.1 The empirical rule It is possible for the distribution of some of the variables in a data sets to exhibit a ‚Äúbell shape‚Äù üîî. For bell-shaped distributions, the empirical rule, also known as the 68-95-99.7 rule, states that (roughly) 68% of the observations should fall within one standard deviation of the mean, 95% should fall within two standard deviations of the mean, and 99.7% should fall within three standard deviations of the mean‚Äîthis is illustrated in Figure 2.2. Therefore, data from bell-shaped distributions can be adequately described my the sample mean and and standard deviation (if it exists). The most important takeaway is that the majority of observations from a bell-shaped distribution should be within a couple of standard deviations from the mean, and it is extremely unlikely for an observation to be beyond three standard deviations from the mean. This provides an intuitive and simple rule for identifying potential outliers (see Section 2.6). Figure 2.2: The empricial rule for bell-shaped distributions. (In progress!) To reiterate, the empirical rule applies to data that are (at least approximately) bell-shaped. To ascertain the shape of the distribution of a sample, a histogram or kernel density estimate can be used‚Äîthese are discussed in Chapter 3. A histogram of the Sale_Price data is displayed in the left side of Figure 2.3. These data are not bell-shaped, or even symmetric‚Äîin fact, Sale_Price appears to be skew right. Data that are skew right can often be transformed to appear more bell-shaped by taking a logarithm or square root transformation. A histogram of log(Sale_Price) is displayed in the right side of Figure 2.3. From the histograms, it is clear that Sale_Price is in fact skew right and that taking the logarithm makes the distribution appear more bell-shaped‚Äîsuch transformations are useful for some of the statistical inference procedures discussed in Chapter 4 which assume that the data are approximately normally distributed. FIXME: What other methods in this book assume normality (and therefore symmetry)? For example, regression. Figure 2.3: Histogram estimates of the distribution of Sale_Price (left) and log(Sale_Price) (right). 2.4 Percentiles The \\(p\\)-th percentile of a sample, denoted \\(x_p\\), is the value for which \\(p\\) percent of the observations are less than \\(x_p\\). The median, for example, is the 50-th percentile (i.e., the middle number). Names are given to special groups of percentiles. Deciles, for example, divide a sample into ten equally sized buckets. Quartiles are the values that divide the data into four equally sized groups‚Äîin other words, the quartiles of a sample consists of the 25-th, 50-th, and 75-th percentiles. The quartiles, denoted \\(Q_1\\), \\(Q_2\\), and \\(Q_3\\) (\\(Q_1\\) and \\(Q_3\\) are also referred to as the lower and upper quartiles, respectively), play an important part in many descriptive and graphical analyses. Together with measures of location and spread, the percentiles help describe the shape of the sample (i.e., what its distribution looks like). In fact, one of the most useful graphics for describing a sample is the boxplot which is described in Chapter 3. The boxplot is a simple visualization capturing the quartiles, median, as well as the maximum and minimum values and is extremely effective at showing the shape of a set of data (these five summary statistics are collectively known as Tukey‚Äôs five-number summary). A modern alternative to boxplots, called violin plots, will also be discussed in Chapter 3. The formula for computing the \\(p\\)-th percentile for a sample is not unique and many definitions exist. In fact, R includes nine different algorithms (controlled via the type argument) for computing percentiles! Therefore, it is important to realize that different software may produce slightly different results when computing percentiles. To reproduce the same quantiles provided by SAS2, specify type = 3 in the call to quantile(). FIXME: This needs to be verified! The R function for computing percentiles is quantile(). Quantiles are essentially the same as percentiles, but specified using decimals rather than percentages. For example, the 5-th percentile is equivalent to the 0.05 quantile. The following code chunk computes the quartiles of Sale_Price from the ames data frame. We use the default algorithm (i.e., type = 7); for specifics, see the help page ?stats::quantile. quantile(Sale_Price, probs = c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 129500 160000 213500 In other words, 25% of the sale prices were below $129,500, 25% between $129,500 and $160,000, 25% between $160,000 and $213,500, and the rest greater than $213,500. 2.5 Robust measures of spread Since the sample standard deviation relies on squared deviations from the sample mean (a non-robust measure of location), it is also sensitive to outliers. A measure of spread less affected by outliers is the interquartile range (IQR). The IQR is defined as the difference between the upper and lower quartiles: \\[\\begin{equation*} \\text{IQR} = Q_3 - Q_1. \\end{equation*}\\] The IQR describes the variability of the middle 50% of the data and has an FSBP of 25%. Therefore, the IQR is a more robust measure of spread than the sample standard deviation. Perhaps a more useful, but less often used, robust measure of spread is the median absolute deviation (MAD). For a sample \\(x_1, x_2, \\dots, x_n\\) with sample median \\(M\\), the MAD is given by the median of the absolute value of the deviations from \\(M\\): \\[\\begin{equation*} \\text{MAD} = median\\left(\\left|x_1 - M\\right|, \\left|x_2 - M\\right|, \\dots, \\left|x_n - M\\right|\\right). \\end{equation*}\\] The MAD, like the median, has an FSBP of 50%, meaning nearly half the observations could be outliers without affecting the MAD. Some software, including R by default, actually computes an adjusted version of MAD, called MADN, by multiplying by the constant \\(1.4826\\): \\(\\text{MADN} = 1.4826 \\times \\text{MAD}\\). This adjustment is to make MAD asymptotically normally consistent for the population standard deviation \\(\\sigma\\). In other words, as the sample size \\(n\\) gets larger, MADN is a good estimator of \\(\\sigma\\) for a normally distributed population. In practice, MADN is typically used/reported. To compute the IQR or MAD(N) in R, we can use the IQR() and mad() functions, respectively. For the ames data frame, the IQR and MAD(N) for Sale_Price are computed below. Note that since the IQR is based on the 25-th and 75-th percentiles, the IQR() function also includes the option type for specifying which algorithm to use for computing \\(Q_1\\) and \\(Q_3\\). IQR(Sale_Price) ## [1] 84000 mad(Sale_Price) # MADN ## [1] 54856 mad(Sale_Price, constant = 1) # MAD ## [1] 37000 mad(Sale_Price, constant = 1) * 1.4826 # sanity check ## [1] 54856 In practice, it is common (and important) to report some measure of spread whenever reporting measures of location (and vice versa). The standard deviation is often reported with the mean. Whenever the median is used, the IQR or MAD(N) is often reported as well. 2.6 Outlier detection In this section, we present a few simple rules for detecting potential outliers in univariate data; that is, data on a single variable. In later chapters, we present more sophisticated methods for detecting potential outliers and anomalies in multivariate data (i.e., data on more than one variable). The empirical rule (Section 2.3.1) probably offers the simplest method for detecting outliers, at least for reasonably bell-shaped data. Recall that for approximately bell-shaped distributions, 95% of the observations should lie within two standard deviations of the mean. Therefore, for approximately bell-shaped distributions, \\(z\\)-scores greater than two in absolute value might be considered unusual (a cutoff of 2.24 has been shown to be more useful in practice (Wilcox 2003)). To make this rule more robust, we can compute a modified \\(z\\)-score based on the median and MADN. Below, we define a simple function for detecting outliers according to the empirical rule: detect_outliers &lt;- function(x, robust = FALSE, cutoff = 2) { z_score &lt;- if (robust) { (x - median(x)) / mad(x) # modified z-score } else { (x - mean(x)) / sd(x) # z-score } sort(x[abs(z_score) &gt; cutoff]) } Next, we simulate some data from a standard normal distribution (i.e., a bell-shaped distribution with mean zero and standard deviation one) with two outliers (5 and -100) and test out our detect_outliers() function. For a standard normal distribution, we would expect any observations greater than two in absolute value to be unlikely and should be flagged as potential outliers. set.seed(101) # for reproducibility x &lt;- c(rnorm(100), 5, -100) detect_outliers(x) ## [1] -100 detect_outliers(x, robust = TRUE) ## [1] -100.000 -2.319 -2.073 -2.050 5.000 detect_outliers(x, cutoff = 2.24) # following Wilcox (2003) ## [1] -100 Notice that five does not get flagged as an outlier when using the standard \\(z\\)-score based on the sample mean and standard deviation. This is due to the fact that the extreme value -100 skews these descriptive statistics. Using the robust method, however, returns reasonable results. Using the empirical rule is rather limited in practice since distributions are often not bell-shaped, or even symmetric‚Äîthough, as seen with Sale_Price, some can be transformed to appear more bell-shaped. A better outlier detection rule can be constructed using the IQR. This is the same method used to flag outliers in boxplots (see Section 3.2.1.3). The general rule is to define an observation an outlier if it lies outside the interval \\(\\left(Q_1 - 1.5 \\times IQR, Q_2 + 1.5 \\times IQR\\right)\\). While it would be simple to write our own function for detecting outliers using the boxplot method, we can use R‚Äôs built-in function boxplot.stats() (see ?grDevices::boxplot.stats for details). Notice how this method happens to catch the true outliers! boxplot.stats(x)$out ## [1] 5 -100 We leave it to Exercise ?? to further explore outliers for the Sale_Price variable in the ames housing data frame. 2.7 Describing categorical data All of the descriptive statistics discussed thus far are appropriate for continuous variables3‚Äîthat is, variables that can be measured on a continuum (e.g., the weight of an object measured in grams). Technically, no variable can be truly measured on a continuous scale due to precision limitations with how all variables are measured. So, in a sense, continuous variables in practice are numeric variables that can take on a lot of values. Oftentimes the data we are describing contains categorical variables. A categorical variable is a variable whose measurement scale consists of a set of categories (e.g., manufacturer and gender). Such variables are easy to describe using contingency or cross-classification tables (e.g., tables of frequencies and proportions). Categorical variables fall into one of two types: nominal and ordinal. Nominal variables are categorical variables whose categories do no have a natural ordering. The categories of an ordinal variable do have a natural ordering, but no defined distance between the categories. For example, the AdultUCI data frame contains the columns income (with unique categories &quot;small&quot; and &quot;large&quot;) and sex (with unique categories &quot;Female&quot; and &quot;Male&quot;). income would be an example of an ordered variable (since &quot;small&quot; &lt; &quot;large&quot;, but &quot;large&quot; - &quot;small&quot; has no meaningful interpretation) while sex is nominal. In R, categorical variables are typically represented using the &quot;factor&quot; class, but can also be represented by character strings (i.e., the more general &quot;character&quot; class). The AdultUCI data set contains several factors: which(sapply(AdultUCI, is.factor)) # positions and names of factor columns ## workclass education marital-status ## 2 4 6 ## occupation relationship race ## 7 8 9 ## sex native-country income ## 10 14 15 which(sapply(AdultUCI, is.character)) # sanity check ## named integer(0) which(sapply(AdultUCI, is.ordered)) # check specifically for ordered factors ## education income ## 4 15 To coerce a variable into a nominal or ordinal factor, we can use the functions as.factor() and as.ordered(), respectively. Some of the techniques discussed in this book can take ordinality into account, so it is good practice to make sure such variables are coerced to ordered factors. 2.7.1 Contingency tables A contingency table is a rectangular table containing the frequencies (or proportions) of observations within the different categories of one or more categorical variables. Contingency tables typically cross-classifiy two categorical variables, but can be used to cross-classifiy more than two. R contains a number of functions for creating such tables, the most useful probably being xtabs() since it has a formula interface. To illustrate, we construct a \\(2 \\times 2\\) table cross-classifying income and sex from the AdultUCI data frame: (tab &lt;- xtabs(~ sex + income, data = AdultUCI)) ## income ## sex small large ## Female 9592 1179 ## Male 15128 6662 This table shows some disparity in income between females and males. In Chapter 4, we discuss ways of testing for association between a set of categorical variables. We can also request that margins (i.e., row/column summaries) be added to the table using the addmargins() function: addmargins(tab) # defaults to adding row/column totals ## income ## sex small large Sum ## Female 9592 1179 10771 ## Male 15128 6662 21790 ## Sum 24720 7841 32561 addmargins(tab, FUN = mean) # add sample means to the margins instead ## Margins computed over dimensions ## in the following order: ## 1: sex ## 2: income ## income ## sex small large mean ## Female 9592 1179 5386 ## Male 15128 6662 10895 ## mean 12360 3920 8140 We can can convert the frequencies to proportions using the prop.table() function: prop.table(tab) ## income ## sex small large ## Female 0.29459 0.03621 ## Male 0.46460 0.20460 In Section ??, we discuss a statistic that can be used to quantify the strength of the association between two categorical variables. 2.8 Exercises Exercise 2.1 (Groupwise descriptive statistics) For the ames data set, we computed various measures of spread and location for the variable Sale_Price. However, these data span multiple years (2006‚Äì2010). Therefore, it might be of interest to see how various descriptive statistics change over time. For this exercise, compute the sample median and MADN for Sale_Price stratified by the year in which the house was sold (i.e., Year_Sold). Hint: use the built-in functions tapply() or by(); see ?tapply and ?by for example usage. Do typical sale prices seem to be different across the five years? Exercise 2.2 (Outlier detection) Use the boxplot outlier detection method on the variable Sale_Price from the ames housing example. How many outliers are detected using this method? How many outliers are detected using the empirical rule? Does the empirical rule seem appropriate for these data? Why or why not? Exercise 2.3 (Kurtosis and skewness) TBD. Exercise 2.4 (The apply() function) The votes.repub data set from the standard R package cluster (Maechler et al. 2016) contains the percentages of votes given to the republican candidate in presidential elections from 1856 to 1976. The rows represent the 50 US states, and the columns represent the 31 elections. The data can be loaded into R using data(votes.repub, package = &quot;cluster&quot;). In this case, we may be interested in computing various descriptive statistics for each state across all years. Since the data are stored in rectangular format (i.e., state are in rows and years are in columns) we can efficiently compute descriptive statistics for each state using R‚Äôs built-in apply() function (see ?apply for details). Using apply(), compute the sample mean percentage of votes and standard deviation for each state, taking care to handle NA‚Äôs appropriately. Which state had the highest average percentage of votes? Which had the lowest? Exercise 2.5 (Trimmed mean) TBD. Exercise 2.6 (Sample variance r emo::ji('scream')) Show that the formula for the sample is equivalent to \\[ s^2 = \\left(\\frac{1}{n} \\sum_{i = 1}^n x_i^2\\right) - \\bar{x}^2. \\] References "],
["visualization.html", "Chapter 3 Visual data exploration 3.1 Prerequisites 3.2 Univariate data 3.3 Bivariate data 3.4 Multivariate data 3.5 Data quality 3.6 Further reading 3.7 Exercises", " Chapter 3 Visual data exploration ‚ÄúPie charts do not provide efficient detection of geometric objects that convey information about differences of values.‚Äù ‚Äî William S. Cleveland As mentioned at the beginning of Chapter 1.1, the first step in any data analysis problem is to simply ‚Äúlook‚Äù at the data. In the previous chapter, we did this using numerical summaries of the data (i.e., descriptive statistics). In this chapter, we discuss graphical summaries of the data‚Äîthese two tasks go hand in hand. One of the biggest strengths of R is its ability to produce high-quality static graphics. While base R can produce high quality graphics with ease, a number of add-on packages provide enhanced graphics capabilities. Two of the most popular packages are lattice (Sarkar 2008) and ggplot2 (Wickham 2009)‚Äîboth of which are built on top of R‚Äôs grid graphics system. Dynamic and interactive graphics are also readily available through add-on packages like plotly (Sievert et al. 2017). This chapter makes extensive use of ggplot2‚Äîa popular package for creating graphics based on The Grammar of Graphics; see help(&quot;ggplot2-package&quot;, package = &quot;ggplot2&quot;) for details and useful links. Although this chapter uses ggplot2, the R code for recreating most of the graphs (where possible) using both lattice and base R graphics is available on the book‚Äôs GitHub repo. One advantage of lattice over ggplot2 is its ability to create 3-dimensional graphics and shingles (see ?lattice::cloud and ?lattice::shingle, respectively, for additional information). Visual data exploration (VDE) is critical to understanding your data. When combined with descriptive statistics (Chapter 1.1), VDE provides an effective way to identify relationships and abnormalities (e.g., outliers), and communicate summaries of the data. In fact, sometimes no elaborate analysis is necessary at all as most of the important conclusions are evident by simply examining the data visually. Regardless of the intent, VDE is about investigating the characteristics of your data set. To do this, we typically create numerous plots in an iterative fashion. This chapter will show you how to create such plots, and use them to answer fundamental questions about the data. 3.1 Prerequisites In this chapter we‚Äôll illustrate the key ideas of VDE using the Ames housing data. We‚Äôll also use several tidyverse packages, including: dplyr, for data manipulation; ggplot2, for creating graphs; as well as some useful graphical functions from other packages (most of which rely on ggplot2). # Load required packages library(dplyr) # for data manipulation library(ggplot2) # for creating graphs library(gridExtra) # for displaying multiple (ggplot2) graphs in one figure library(scales) # for better breaks and labels in ggplot2 axes # Additional packages that need to be installed: AmesHousing, forcats, GGally, # ggridges, RColorBrewer, reshape2, purrr, and viridis ames &lt;- AmesHousing::make_ames() # load the Ames housing data ggplot2 constructs plots in layers. For example, add points, then add trend lines, etc. A general formula for a ggplot2 graph is given below. Notice, we always start by using ggplot() to construct an empty canvas. ggplot() + geom_&lt;geom-1-name&gt;(data = &lt;data-1-name&gt;, aes(&lt;aesthetic mappings&gt;), &lt;options&gt;) + geom_&lt;geom-2-name&gt;(data = &lt;data-2-name&gt;, aes(&lt;aesthetic mappings&gt;), &lt;options&gt;) + ... If all the layers use the same data set (as in most of this chapter), you can provide the data and aesthetics to the ggplot() function instead, as in ggplot(&lt;data-name&gt;, aes(&lt;aesthetic mappings&gt;)) + geom_&lt;geom-1-name&gt;(&lt;options&gt;) + geom_&lt;geom-2-name&gt;(&lt;options&gt;) + ... 3.2 Univariate data An important first step in VDE is to to understand how individual variables are distributed. Visualizing the distribution of a variable allows us to easily understand and describe many of its features. This is complementary to describing the distribution of a variable using various descriptive statistics, as described in Chapter 1.1. 3.2.1 Continuous Variables A variable is continuous if it can take any of an infinite set of ordered values (e.g., income and price). There are several different features we are generally interested in with continuous variables (i.e. measures of location, measures of spread, asymmetry, outliers). Different plots can effectively communicate these different features of continuous variables. Techincally, no variable in a data set is truly continuous, as measurements are never recorded with infinite precision. In a census, for example, age is typically recorded in years, but typically treated as continuous during analysis. What‚Äôs important for a variable to be treated as continuous is our willingness to treat the differences between values as quantitative. For the purposes of this book, continuous variables are also referred to as numeric or quantitative variables. 3.2.1.1 Histograms One of the most effective ways to visualize the distribution (in particular, the density) of a continuous variable is to use a histogram; our first example of a histogram was seen in Figure 2.3 for the variable Sale_Price. Although rather crude estimates, histograms were indispensable tools in the days before computers were mainstream. Histograms are often overlooked, yet they offer an efficient means for communicating important features of continuous variables (e.g., skewness). Histograms are just special bar charts! First, the variable is broken into equal sized intervals, called bins, and the number of observations that fall into each bin is counted (these are called frequencies). A histogram is nothing more than a simple bar chart of these frequencies (where each bar represents a different bin). Histograms quickly signal what the most common observations are for the variable being assessed (the higher the bar the more frequent those values are observed in the data); they also signal the shape (spread and symmetry) of the data by illustrating if the observed values cluster towards one end or the other of the distribution. To get a quick sense of how Sale_Price is distributed, we can generate a histogram estimate of its density using ggplot2‚Äôs geom_histogram() function. ggplot(ames, aes(x = Sale_Price)) + geom_histogram() Figure 3.1: Default histogram of Sale_Price. Figure 3.1 conveys several important features about the distribution of Sale_Price: measures of location (2.2): the most common values of Sale_Price are around $160K (the median); measures of spread (2.3): Sale_Price ranges from near zero to over $700K; skewness: Sale_Price is skewed right (a common feature of financial data); outliers (2.6): there appears to be some abnormally large values of Sale_Price. Some of the analytic techniques we will learn about in later chapters are sensitive to outliers and assume that the data are (at least approximately) symmetric. Histograms, as well may of the graphical displays in this chapter, are invaluable tools for identifying there and other potential problems with our data. FIXME: The footnote in this section does not make sense. By default, geom_histogram() will divide the values of a continuous variable into 30 equally sized bins. Since Sale_Price ranges from $12,789‚Äì$755,000, 30 equally sized bins implies a bin width of \\(\\frac{\\left\\lfloor{\\$755,000 - \\$12,789}\\right\\rfloor}{30} = \\$24,740\\). So in Figure 3.1, the first bar represents the frequency of Sale_Price in the range of (roughly) \\(\\left[\\$12,500, \\ \\$37,500\\right]\\)4, the second bar represents the frequency of Sale_Price in the range of (roughly) \\(\\left[\\$37,500, \\ \\$62,300\\right]\\), and so on. The number of bins (or equivalently, the bin width) in a histogram is a tuning parameter that should be adjusted specifically for each application. Adjusting the bin width is somewhat subjective and needs to be done carefully (usually by eye). If the bin width is ‚Äútoo small‚Äù, the histogram will overfit the data and display too much noise. If the bin width is ‚Äútoo large‚Äù, the histogram will underfit the data and important features (like outliers) will be missed. In geom_histogram(), we can use the binwidth and bins arguments to control the bin width and number of bins, respectively (but only one argument needs to be specified). By adjusting one of these parameters, we can change the crudeness of the histogram estimate of the variable‚Äôs density. For instance, in the default histogram, the bin with the largest frequency ranged from $136,000‚Äì$161,000; however, as demonstrated below, we can often do better by manually adjusting the bin width5. # Histograms with various bin widths # Too crude (i.e., under fitting) p1 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 100000) + ggtitle(&quot;Bin width = $100,000&quot;) # Less crude p2 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 50000) + ggtitle(&quot;Bin width = $50,000&quot;) # Just right? p3 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 5000) + ggtitle(&quot;Bin width = $5,000&quot;) # Too flexible (i.e., over fitting) p4 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(binwidth = 1000) + ggtitle(&quot;Bin width = $1,000&quot;) # Display plots in a grid grid.arrange(p1, p2, p3, p4, ncol = 2) Figure 3.2: Adjusting histogram bin width. The most common value of Sale_Price (i.e., the statistical mode) is $135,000 (this is roughly where the peak of each histogram occurs). R does not have a built-in function for finding the most common value in a set of observations, but we can easily find it using a combination of which.max() and table(). which.max(table(ames$Sale_Price)) ## 135000 ## 270 The mode of a continuous variable is another measure of location (measures-of-location). The mode of a sample is the value that occurs most frequently‚Äîthis also makes the mode a useful descriptive statistic for categorical data as well. If the sample values are not unique, there will exist multiple modes (i.e., the data are said to be multimodal). For continuous data, unlike the sample mean and median, computing the mode is not straightforward (e.g., what is the mode of \\(x = \\left\\{0.001, 3.514, 6.799, \\dots\\right\\}\\)). Fortunately, the modes (or peaks in the density) of a sample are easily seen from density estimates like histograms. Recall that the mean and median of Sale_Price were $180,796.10 and $160,000, respectively. For unimodal distributions (i.e., distributions with only a single peak), the median will often lie between the mean and the mode. For perfectly symmetric distributions, the mean, median, and mode are all the same (though, this rarely, if ever, happens in practice). This is especially true for right skew data like Sale_Price. As discussed in Chapter 1.1, a log transformation is often useful for making right skewed data look more symmetric; taking the square root is also effective for this purpose. While we can simply apply such transformations to the data ourselves, ggplot2 offers several useful functions for plotting data on various scales; in our case, we can use the scale_x_log() and scale_x_sqrt() functions to apply a \\(log_{10}\\) and square root transformation, respectively6. This is demonstrated in the code below which produces Figure ??. In this case, the \\(log_{10}\\) transformation was more effective at making the data appear more symmetric. # Log 10 scale p1 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(bins = 50) + scale_x_log10(labels = dollar, breaks = c(50000, 125000, 300000)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;Sale_Price (log10 scale)&quot;) # Square root scale p2 &lt;- ggplot(ames, aes(Sale_Price)) + geom_histogram(bins = 50) + scale_x_log10(labels = dollar, breaks = c(50000, 125000, 300000)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(&quot;Sale_Price (square root scale)&quot;) # Display both plots side by side grid.arrange(p1, p2, ncol = 2) Figure 3.3: Histogram of Sale_Price using different transformation scales. Left: \\(log_{10}\\) scale. Right: Square root scale. Oftentimes, it will be useful to ‚Äúsmooth the histogram‚Äù by overlaying a kernel density estimate: an alternative to histograms for data assumed to come from a smooth, continuous distribution. To plot a kernel density estimate in ggplot2, we use the geom_density() function. For example, the following code constructs a simple kernel density estimate for Sale_Price. Additionally, we also add a rug representation (i.e., one-dimensional marginal distribution) of Sale_Price to the \\(x\\)-axis using the geom_rug() function. Rug displays give additional insight into the spread of the distribution of the continuous variable of interest; by default, rug displays plot one tick mark for each data point, but with lots of data, it may be more useful to plot certain percentiles (e.g., the deciles of the distribution). Rug displays are uses in various plots throughout this book. # Kernel density estimate p1 &lt;- ggplot(ames, aes(Sale_Price)) + geom_density() # Add a rug representation to the x-axis p2 &lt;- p1 + geom_rug(alpha = 0.1) # Display both graphs side by side grid.arrange(p1, p2, ncol = 2) Figure 3.4: Density plot of sales price. Rug representations can be added to any axis in a plot (as long as that axis represents a continuous variable). In ggplot2, we can specify which sides of the plot we want to display a 1-D marginal distribution using the sides argument. For example, specifying sides = &quot;bl&quot; (the default) will display a rug representation of the data on the bottom and left sides of the plot. In practice, we suggest using a kernel density estimate in conjunction with a histogram‚Äîas they both compliment each other. To overlay a kernel density estimate on top of a histogram, we need to plot the histogram so that the \\(y\\)-axis is on the density scale, rather than the frequency scale (which is typically the default for a histogram). In ggplot2, this can be done by specifying aes(y = ..density..) in the call to geom_histogram()). We can then overlay a kernel density estimate by adding the geom_density() layer. This is displayed in Figure 3.5. ggplot(ames, aes(Sale_Price)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;grey65&quot;) + geom_density(color = &quot;red&quot;) Figure 3.5: Histogram of Sale_Price with a kernel density estimate (red curve). 3.2.1.2 Quantile-quantile plots Quantile-quantile (Q-Q) plots offer a graphical way for assessing how similar data from two (possibly different) continuous distributions are. The most common use of Q-Q plots is in assessing normality; that is, whether or not it is reasonable to assume a variable is (at least approximately) normally distributed; Q-Q plots in this setting are referred to as normal Q-Q plots. Normal Q-Q plots graph the observed quantiles of the observed data against the similar quantiles from a normal distribution‚Äîsuch plots are often used in assessing the normality assumption in ordinary linear regression (Chapter (regression)). Although commonly used to check normality, Q-Q plots can be constructed to graphically check if it is reasonable to assume a continuous variable comes from any parametric distribution! A normal Q-Q plot for Sale_Price is given in the left side of Figure 3.6. This graph plots the sample quantiles of Sale_Price against the theoretical quantiles of a normal distribution (the default for stat_qq()). If Sale-Price is normally distributed, the points on the plot should lie (roughly) on a 45\\(^\\circ\\) line (imagine trying to cover the points with a ‚Äúfat pencil‚Äù). If the observed data deviate from normality, then the plot will display curvature. The nature of this curvature would be informative into how Sale_Price deviates from normality. To produce a Q-Q plot in ggplot2, we can use the stat_qq() function. The following code chunk produces Figure 3.6. # Normal Q-Q plot of Sale_Price p1 &lt;- ggplot(ames, aes(sample = Sale_Price)) + geom_qq() # Normal Q-Q plot of log(Sale_Price) p2 &lt;- ggplot(ames, aes(sample = log(Sale_Price))) + geom_qq() # Display both graphs side by side grid.arrange(p1, p2, ncol = 2) Figure 3.6: Normal Q-Q plots for Sale_Price. Left: Original scale. Right: Log-transformed scale. Notice how the normal Q-Q plot for Sale_Price starts to bend up. This indicates that the distribution of Sale_Price has a heavier right tail than expected from a normal distribution (i.e., Sale_Price is more positively skewed than what would be expected from normally distributed data). The normal Q-Q plot for log(Sale_Price) is a bit more well behaved (aside from the two potential outliers and slight downward curvature in the bottom left of the graph). Histograms (and kernel density estimates), and Q-Q plots are great at visually summarizing the distribution of a continuous variable; especially a variable with many unique values. Unfortunately, histograms are less suitable for continuous data with few unique values, and are not great at displaying outliers (the same goes for Q-Q plots). Fortunately, a simple display of a few descriptive statistics offers a perfect compliment to histograms; this display is referred to as a box plot. 3.2.1.3 Box plots Box plots (Frigge, Hoaglin, and Iglewicz 1989)‚Äîin particular, Tukey‚Äôs box-and-whisker plots (Tukey 1977)‚Äîare an effective way to compare a continuous variable between groups. For a single variable, a box of arbitrary width is drawn around the median from the lower quartile \\(Q_1\\) to the upper quartile \\(Q_3\\); this box, which has length \\(Q_3 - Q_1 = IQR\\), contains the middle 50% of the data. The ‚Äúwhiskers‚Äù are formed by extending lines from the edges of the box out to \\(Q_1 - 1.5 \\times IQR\\) and \\(Q_3 + 1.5 \\times IQR\\). Any observations lying beyond the whiskers are flagged as potential outliers and plotted as individual points. An example of a typical box plot for some skew right data is given in Figure 3.7. Can you easily tell from the box plot that these data are skew right? ü§î Figure 3.7: A typical box plot. A box plot can be constructed in ggplot2 using the geom_boxplot() layer. In the code chunk below, we use geom_boxplot() to construct box plots for both Sale_Price and log10(Sale_Price) (by specifying a \\(\\log\\) scale for the \\(y\\)-axis). The results are displayed in Figure 3.8. Note: we used \\(log_{10}\\) as the scale because ggplot2 does not have a natural log scale function. p1 &lt;- ggplot(ames, aes(x = &quot;var&quot;, y = Sale_Price)) + geom_boxplot() + scale_y_continuous(labels = dollar, breaks = quantile(ames$Sale_Price)) + labs(x = &quot;&quot;, y = &quot;&quot;) + ggtitle(&quot;Box plot for Sale_Price&quot;) p2 &lt;- p1 + scale_y_log10(labels = dollar, breaks = quantile(ames$Sale_Price)) + labs(x = &quot;&quot;, y = &quot;&quot;) + ggtitle(&quot;Box plot for log10(Sale_Price)&quot;) grid.arrange(p1, p2, ncol = 2) Figure 3.8: Box plots for Sale_Price. Left: Original scale. Right: \\(log_{10}\\) scale. A modern alternative to Tukey‚Äôs box plot, called a violin üéª plot, combines box plots with kernel density estimates; in ggplot2 nomenclature, geom_violin() \\(\\approx\\) geom_boxplot() + geom_density(). There are two efficient graphs to get an indication of potential outliers in our data. The classic box plot on the left will identify points beyond the whiskers which are beyond \\(\\$1.5 \\times IQR\\) from the first and third quantile. This illustrates there are several additional observations that we may need to assess as outliers that were not evident in our histogram. However, when looking at a box plot we lose insight into the shape of the distribution. A violin plot on the right provides us a similar chart as the box plot but we lose insight into the quantiles of our data and outliers are not plotted (hence the reason we plot geom_point() prior to geom_violin()). Violin plots will come in handy later when we start to visualize multiple continuous distributions along side each other. # Box plot (log10 scale) p1 &lt;- ggplot(ames, aes(&quot;var&quot;, Sale_Price)) + geom_boxplot(outlier.alpha = 0.25) + scale_y_log10( labels = dollar, breaks = quantile(ames$Sale_Price) ) # Violin plot (log10 scale) p2 &lt;- ggplot(ames, aes(&quot;var&quot;, Sale_Price)) + geom_point() + geom_violin() + scale_y_log10( labels = dollar, breaks = quantile(ames$Sale_Price) ) # DIsplay both plots side by side grid.arrange(p1, p2, ncol = 2) Figure 3.9: Box plot (left) and violin plot (right). The box plot starts to answer the question of what potential outliers exist in our data. Outliers in data can distort predictions and affect their accuracy. Consequently, it‚Äôs important to understand if outliers are present and, if so, which observations are considered outliers. As demonstrated, several plots exist for examining univariate continuous variables. Several examples were provided here but still more exist (i.e. frequency polygon, bean plot, shifted histograms). There is some general advice to follow such as histograms being poor for small data sets, dot plots being poor for large data sets, histograms being poor for identifying outlier cut-offs, box plots being good for outliers but obscuring multimodality. Consequently, it is important to draw a variety of plots. Moreover, it is important to adjust parameters within plots (i.e. bin width, axis transformation for skewed data) to get a comprehensive picture of the variable of concern. In this section, we have introduced several fundamental, but useful, graphics for continuous variables. This is by no mean an exhaustive list and we will see many different types of graphics throughout the book. In practice, it is often helpful (encouraged, in fact) to look at many types of graphics for a particular variable. For continuous variables, it is often useful to look at histograms, box plots, Q-Q plots, and a number of other graphics (depending on the situation). A useful graphical summary for a continuous variable is called the four plot (REFERENCE). A four plot displays a wealth of important information about continuous variables, especially as it relates to typical analytic tasks (e.g., density, correlation, outliers, etc.) p1 &lt;- ggplot(ames, aes(x = Sale_Price)) + geom_histogram(aes(y = ..density..), size = 1.5) + geom_density(color = &quot;purple2&quot;) p2 &lt;- ggplot(ames, aes(x = &quot;&quot;, y = Sale_Price)) + geom_jitter(alpha = 0.1) + geom_boxplot() + coord_flip() p3 &lt;- ggplot(ames, aes(sample = Sale_Price)) + geom_qq(alpha = 0.5) ames$Index &lt;- seq_len(nrow(ames)) # add row number as a new column p4 &lt;- ggplot(ames, aes(x = Index, y = Sale_Price)) + geom_line(alpha = 0.5) grid.arrange(p1, p2, p3, p4, ncol = 2) Figure 3.10: ABC. 3.2.2 Categorical Variables Categorical variables, in contrast to numeric, can only take on a finite number of distinct values. For example, \\(\\left\\{Male, Female\\right\\}\\) or \\(\\left\\{low, medium, high\\right\\}\\). In statistics, categorical variables are often referred to as factors. With categorical variables, as discussed in Chapter 1.1, we are often interested in tabulating frequencies and estimating proportions. Graphically, these are easiest to display via bar charts and dot plots. In general, dot plots are referred to bar charts as they can display mostly the same information, but in a more concise plot. 3.2.2.1 Bar charts In a typical bar chart, each bar represents a different category (e.g., \\(Male\\) or \\(Female\\)) and the height of each bar represents the frequency (or proportion) of observations within each category. By default, the \\(x\\)-axis typically represents categories; however, as we will see, it is often useful to ‚Äúflip‚Äù bar charts horizontally for readability, especially when the number of categories is large. We have already seen bar charts in the form of a histogram‚Äîthe difference there being that the bars (i.e., categories) were created by binning a numeric variable into (roughly) equal sized buckets. If we look at the general zoning classification (MS_Zoning) for each property sold in our ames data set, we see that the majority of all properties fall within one category. We can use ggplot2‚Äôs geom_bar() function to construct simple bar charts. By default, geom_bar() simply counts the observations within each category and displays them in a vertical bar chart. A bar chart for MS_Zoning is displayed in Figure 3.11. ggplot(ames, aes(MS_Zoning)) + geom_bar() + theme(axis.text.x = element_text(angle = 55, hjust = 1)) Figure 3.11: Basic bar chart. The visual order of categories is important for bar charts and dot plots. By default, ggplot2 (and most other graphical packages) plot the categories in alphabetical order‚Äîwhich is not visually appealing. In ggplot2 we can use the reorder() function to easily plot the categories of a variable in a particular order, as demonstrated in Figure (04-bar-chart-02). MS_Zoning is an example of a nominal categorical variable; a categorical variable for which there is no logical ordering of the categories (as opposed to low, medium, and high, for example). To get better clarity of nominal variables we can make some refinements. We can also draw bar charts manually using geom_col(). To do this, we first need to compute the length of each bar. For frequencies, we can use dplyr::count() to tally the number of observations within each category. We can then use dplyr::mutate() to convert frequencies to proportions. With this approach, we can use reorder() to easily reorder the bar categories from most frequent to least (or vice versa). For readability, we can also apply coord_flip() to rotate the bar chart (or any ggplot2 figure) on its side. These refinements are demonstrated in the code chunk below. # Bar chart of frequencies p1 &lt;- ames %&gt;% count(MS_Zoning) %&gt;% ggplot(aes(reorder(MS_Zoning, n), n)) + geom_col() + coord_flip() + # now x becomes y labs(x = &quot;MS_Zoning&quot;, y = &quot;Frequency&quot;) # better labels # Bar chart of proportions p2 &lt;- ames %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% # convert to proportions ggplot(aes(reorder(MS_Zoning, pct), pct)) + geom_col() + coord_flip() + # now x becomes y labs(x = &quot;MS_Zoning&quot;, y = &quot;Relative frequency&quot;) + # better labels scale_y_continuous(labels = scales::percent) # Dispay both plots side by side grid.arrange(p1, p2, ncol = 2) Figure 3.12: Bar chart and dot plots of MS_Zoning. Left: Bar chart of frequencies. Right: Bar chart of relative frequencies (%). Note how dplyr functions can be expressed sequentially using the forward pipe operator %&gt;%, whereas ggplot2 layers have to be added using +! Now we can see that properties zoned as residential low density make up nearly 80% of all observations . We also see that properties zoned as agricultural (A_agr), industrial (I_all), commercial (C_all), and residential high density make up a very small amount of observations. In fact, below we see that these imbalanced category levels each make up less than 1% of all observations. ames %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% arrange(pct) ## # A tibble: 7 x 3 ## MS_Zoning n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A_agr 2 0.000683 ## 2 I_all 2 0.000683 ## 3 C_all 25 0.00853 ## 4 Residential_High_Density 27 0.00922 ## 5 Floating_Village_Residential 139 0.0474 ## 6 Residential_Medium_Density 462 0.158 ## 7 Residential_Low_Density 2273 0.776 Severely imbalanced categories can cause problems in statistical modelling, so it makes sense to sometimes combine the infrequent levels into an other category. An easy way to accomplish this is to use fct_lump()7. Here we use n = 2 to retain the top two most frequent categories/levels, and condense the remaining into an other category. You can see that other still represents less than 10% of all observations. ames %&gt;% mutate(MS_Zoning = forcats::fct_lump(MS_Zoning, n = 2)) %&gt;% count(MS_Zoning) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(reorder(MS_Zoning, pct), pct)) + geom_col() + coord_flip() Figure 3.13: Bar chart with collapsed categorical levels. In some cases, the categories of a categorical variable have a natural ordering (though, the difference between any two categories is not meaningful)‚Äîthese are called ordinal variables. For example, the Kitchen_Qual variable in ames categorizes kitchen quality into five buckets: table(ames$Kitchen_Qual) ## ## Excellent Fair Good Poor Typical ## 205 70 1160 1 1494 ggplot(ames, aes(Kitchen_Qual)) + geom_bar() Figure 3.14: Bar chart inadequately capturing ordinal levels. Here, we might consider ordering the bars using their natural order: Poor &lt; Fair &lt; Typical &lt; Good &lt; Excellent. One way to plot the categories in a particular order is to use forcats::fct_relevel(). From Figure 3.15, it is easier to see that most homes have average to slightly above average quality kitchens. ames %&gt;% mutate(Kitchen_Qual = forcats::fct_relevel( Kitchen_Qual, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) %&gt;% ggplot(aes(Kitchen_Qual)) + geom_bar() Figure 3.15: Bar chart adequately capturing ordinal levels. Often our data will have categorical variables that are numerically encoded (typically as integers). For example, in the ames data set, the month each home was sold (Mo_Sold) was encoded using the integers 1‚Äì12. For visual (and in some cases, modelling), we will want to make sure R treats such variables as categorical. This is easiest to accomplish using as.factor(). The following example demonstrates the difference; the results are displayed in Figure 3.16. # Keeping Mo_Sold as an integer p1 &lt;- ggplot(ames, aes(Mo_Sold)) + geom_bar() + xlab(&quot;Month sold (numeric)&quot;) # Converting Mo_Sold to a factor p2 &lt;- ggplot(ames, aes(as.factor(Mo_Sold))) + geom_bar() + xlab(&quot;Month sold (factor)&quot;) # Display both plots side by side grid.arrange(p1, p2, nrow = 2) Figure 3.16: Bar chart of Mo_Sold. Top: Numeric. Bottom: Factor. 3.2.2.2 Dot plots Basic bar charts are great when the number of categories is small. As the number of categories increases, the bars can become squished together and distract attention from the main insights of the visual. Cleveland dot plots (or just dot plots) and lollipop charts, like bar charts, are useful for visualizing discrete distributions (e.g., tables or the frequencies of different categories) while being more economical in ink. For example, if we can use geom_point() to construct a dot plot of the relative frequencies of home sales across the 28 within the Ames housing data set. The result is displayed on the left side of Figure 3.17. p1 &lt;- ames %&gt;% count(Neighborhood) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(pct, reorder(Neighborhood, pct))) + geom_point() + labs(x = &quot;Relative frequency&quot;, y = &quot;Neighborhood&quot;) Similar to a dot plot, a lollipop chart minimizes the visual ink but uses a line to draw the readers attention to the specific \\(x\\)-axis value of each category. To create a lollipop chart, we use geom_segment() to add lines to the previous plot; we explicitly state that we want the lines to start at x = 0 and extend to the corresponding relative frequency with xend = pct. We also need to include y = Neighborhood and yend = Neighborhood so that we get one line segment for each neighborhood. The result is displayed on the right side of Figure 3.17. p2 &lt;- p + geom_segment(aes(x = 0, xend = pct, y = Neighborhood, yend = Neighborhood), size = 0.15) Figure 3.17: Relative frequency of home sales accross the different neighborhoods. Left: Dot plot. Right: Lollipop chart. 3.2.2.3 Pie charts Don‚Äôt use them. 3.3 Bivariate data Beyond understanding the distribution of each individual variable, we often want to investigate associations and relationships between variables. When visualizing the relationship between two or more variables we are generally interested in identifying potential associations, outliers, clusters, gaps, barriers, and change points. 3.3.1 Scatter plots The easiest way to assess the relationship between two continuous variables is to use a scatter plot, one of the most important statistical graphics. A scatter plot graphs two continuous variables directly against each other (one variable on each axis). In this section, we‚Äôll use the geom_point() function‚Äîggplot2‚Äôs function for producing scatter plots. In the code chunk below, we use geom_point() to obtain a plot of sale price (Sale_Price) versus the square footage of above ground living area (Gr_Liv_Area). To limit the effect of over plotting, a common issue with scatter plot of large data sets, we lower the opacity of the individual points using alpha = 0.38. ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.2) # lower opacity Figure 3.18: Scatter plot of Sale_Price and Gr_Liv_Area. It is fairly evident from Figure 3.18 that there is a generally positive association between Gr_Liv_Area and Sale_Price (this does not imply any causal association between them). Five potential outliers with Gr_liv_Area &gt; 4000 are also apparent. We also gain a general understanding of the joint density of these two variables (e.g., dense regions/clusters of data points indicate reasonable combinations of Gr_liv_Area and Sale_Price.) For example, it is not very likely to find a home that will sell for more than $400K that less than 1.5K (sq. ft.) of above ground living area. The relationship between Gr_Liv_Area and Sale_Price appears to be fairly linear, but the variability in Sale_Price increases with Gr_Liv_Area (probably due to the positive skewness associated with both variables); this non-constant variance is called heteroscedasticiy and we will revisit this topic briefly in section [REFERNCE LINEAR REGRESSION SECTION]. To help guide our eyes in interpreting trends between two variables, we can add parametric and/or non-parametric trend lines. In Figure 3.19, we use geom_smooth() to add two different trend lines: method = &quot;lm&quot; draw a trend line of the form \\(\\beta_0 + \\beta_1\\)Gr_Liv_Area, where the \\(y\\)-intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)) are estimated from the observed data; method = &quot;auto&quot; use the number of observations to choose an appropriate non-parametric smoother (i.e., let the trend be dictated by the observed data). Note that for both trend lines we used se = FALSE to suppress plotting +/-2 standard error bands. Figure 3.19 shows that for homes with less than 2,250 square feet the relationship is fairly linear; however, beyond 2,250 square feet we see strong deviations from linearity. For reference, we also included the same plot, but with both axes on the \\(log_{10}\\) scale. Notice how this transformation helps to alleviate, to some extent, the heteroskedacticy noted earlier. # Better colors for the trend lines set1 &lt;- RColorBrewer::brewer.pal(n = 9, name = &quot;Set1&quot;) # Scatter plot with trend lines p1 &lt;- ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = set1[1]) + geom_smooth(method = &quot;auto&quot;, se = FALSE, color = set1[2]) # Scatter plot with trend lines (log10 scale) p2 &lt;- ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = set1[1]) + geom_smooth(method = &quot;auto&quot;, se = FALSE, color = set1[2]) + scale_x_log10() + scale_y_log10() # Display plots side by side grid.arrange(p1, p2, ncol = 2) Figure 3.19: Illustrating linear versus nonlinear relationships in scatter plots. Although we can overcome the issue of over plotting to some extent by lowering the opacity of each individual point, this is often not enough. We can go a step beyond by adding a 2-D kernel density estimate (KDE) using stat_density_2d(), or grouping points into hexagonal bins and displaying a heat map of the bin counts using geom_hex(). Each of these are illustrated in Figure 3.20 which graphs Sale_Price versus Garage_Area. By incorporating a 2-D KDE (middle) we draw attention to the higher density areas which appear to be located at homes with Garage_Area = 0, and homes where 250 &lt; Garage_Area &lt; 500. Similar observations can be drawn from the hexagonal heat map (right). # Scatter plot p1 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_point(alpha = 0.2) # Scatter plot with 2-D KDE p2 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_point(alpha = 0.1) + stat_density2d(aes(fill = stat(level)), geom = &quot;polygon&quot;) + viridis::scale_fill_viridis(option = &quot;A&quot;) + theme(legend.position = &quot;none&quot;) # heat map of hexagonal bin counts p3 &lt;- ggplot(ames, aes(x = Garage_Area, y = Sale_Price)) + geom_hex(bins = 50, show.legend = FALSE) + viridis::scale_fill_viridis(option = &quot;D&quot;) # &quot;D&quot; is the default # Display plots side by side grid.arrange(p1, p2, p3, ncol = 3) Figure 3.20: Avoiding over plotting in scatter plots. Left: scatter plot with transparent points. Middle: transparent points with 2-D KDE. Right: heat map of hexagonal bin counts. A scatter plot of a continuous variable against a categorical variable is referred to as a strip plot. Strip plots can be useful in practice, but it is generally advisable to use box plots (and there extensions) instead. Below we plot Sale_Price against the number of above ground bedrooms (Bedroom_AbvGr). Due to the size of this data set, the strip plot (top left) suffers from over plotting. We can use geom_jitter() to add a some random variation to points within each category (top right), which allows us to see where heavier concentrations of points exist. Alternatively, we can use box plots and violin plots to compare the distributions of Sale_Price to Bedroom_AbvGr (bottom row). # Convert to an ordered factor ames$Bedroom_AbvGr &lt;- as.ordered(ames$Bedroom_AbvGr) # Strip plot p1 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_point(alpha = 0.2) # Strip plot (with jittering) p2 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_jitter(alpha = 0.2, width = 0.2) # Box plots p3 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_boxplot() # Violin plots p4 &lt;- ggplot(ames, aes(x = Bedroom_AbvGr, y = Sale_Price)) + geom_violin() # Display plots side by side grid.arrange(p1, p2, p3, p4, nrow = 2) Figure 3.21: Comparing a continuous variable accross groups. Top left: scatter plot. Top right: strip plots. Bottom left: box plots. Bottom right: violin plots. When constructing box plots of a continuous variable within different groups of a factor, it is sometimes useful to use notched box plots (at least from a comparative statistical inference perspective). Whenever notch = TRUE, a ‚Äúnotch‚Äù is drawn on both sides of each box. If the notches of any two box plots do not overlap, this is evidence that the medians differ; see ?boxplot for details and a reference. An example is displayed in Figure 3.22 comparing Sale_Price across homes of varying quality (Overall_Qual). ggplot(ames, aes(x = Overall_Qual, y = Sale_Price)) + geom_boxplot(notch = TRUE) + scale_y_continuous(labels = dollar) + coord_flip() Figure 3.22: Notched box plots comparing Sale_Price across homes of varying quality (Overall_Qual). Box plots and violin plots are an effective way to visualize distributional differences in a continuous variable accross groups. A popular alternative is to use what are called ridge plots (formerly known as joy plots). These plots can be constructed easily using the ggridges::geom_density_ridges() function, as shown below; the results are displayed in Figure 3.23. ggplot(ames, aes(x = Sale_Price, y = Overall_Qual)) + ggridges::geom_density_ridges() + scale_x_continuous(labels = dollar) Figure 3.23: Ridge plot comparing Sale_Price across homes of varying quality (Overall_Qual). 3.4 Multivariate data 3.4.0.1 FIXME: This section still needs cleaned up a bit. More often than not, we are interested in visualizing the relationship between two or more variables. In the Ames housing data, for example, we might be interested in visualizing the relationship between Sale_Price and Overall_Qual. The nature of each variable (i.e., continuous or categorical) will often dictate the appropriate type of graphic to use. We start with the simplest case, where we want to visualize the relationship (if any) between two continuous variables. 3.4.1 Facetting Data are usually multivariate by nature, and the many analytic techniques are designed to capture multivariate relationships. Visual exploration should therefore also incorporate this important aspect. Although we have shown how to visualize one or two variables at a time, we can go a step beyond by adding information about additional variables using color, shape, size, etc. In Figure 3.18, we compared Sale_Price with Gr_Liv_Area using a simple scatter plot. We can also include information on other variables by using color, shape, and point size. For instance, we can use a different point shape and color to indicate homes with and without central air conditioning (Central_Air). Figure 3.24 illustrates that there are far more homes with central air; furthermore, the homes without central air tend to have less square footage and lower sale prices. ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price, color = Central_Air, shape = Central_Air)) + geom_point(alpha = 0.5) + # lower the opacity of each point theme(legend.position = &quot;top&quot;) Figure 3.24: Scatter plot of Sale_Price and Gr_Liv_Area colored and shaped by Central_Air. Although we lowered the opacity of each point (using alpha = 0.5), the over crowded plot makes it difficult to distinguish the relationships between homes with and without air conditioning. Another approach is to use facetting (i.e., plot each category in its own panel). In ggplot2, we have two options: facet_wrap() for a 1-D ribbon of 2-D panels, and facet_grid() to form a particular matrix of panels. For this problem, we‚Äôll use the former: ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price, color = Central_Air, shape = Central_Air)) + geom_point(alpha = 0.5) + # lower the opacity of each point facet_wrap( ~ Central_Air) Figure 3.25: Scatter plot of Sale_Price and Gr_Liv_Area colored and shaped by Central_Air. However, as before, when there are many levels in a categorical variable it becomes hard to compare differences by only incorporating color or shape features. An alternative is to create small multiples. Below we compare the relationship between Sale_Price and Gr_Liv_Area and how this relationship differs across the different house styles (House_Style). ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = 0.3) + scale_x_log10() + scale_y_log10(labels = dollar) + facet_wrap(~ House_Style, nrow = 2) + theme_bw() Figure 3.26: Using small multiples to assess three dimensions in a scatter plot. We can start to add several of the features discussed in this chapter to highlight multivariate features. For example, here we assess the relationship between sales price and above ground square footage for homes with and without central air conditioning and across the different housing styles. For each house style and central air category we can see where the values are clustered and how the linear relationship changes. For all home styles, houses with central air have a higher selling price with a steeper slope than those without central air. Also, those plots without density markings and linear lines for the no central air category (red) tell us that there are no more than one observation in these groups; so this identifies gaps across multivariate categories of interest. ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price, color = Central_Air, shape = Central_Air)) + geom_point(alpha = 0.3) + geom_density2d(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + scale_y_log10(labels = dollar) + facet_wrap(~ House_Style, nrow = 2) + ggtitle(&quot;Sale Price vs. Above Ground Sq.Ft&quot;, subtitle = &quot;How does central air and house style influence this relationship?&quot;) + theme_bw() Figure 3.27: Combining multiple attributes. We can also use faceting (i.e., facet_wrap() and facet_grid()) to understand how two or more categorical variables are associated with each other. For example, below we assess the quality of kitchens (Kitchen_Qual) for homes that sold above average (i.e., Sale_Price &gt; mean(Sale_Price)) below average (i.e., Sale_Price &gt; mean(Sale_Price)). Not surprisingly, we see that that sold for above average tended to have higher quality kitchens (Figure 3.28). p &lt;- ames %&gt;% mutate( Group = ifelse( Sale_Price &gt; mean(Sale_Price), yes = &quot;Sold above avg.&quot;, no = &quot;Sold below avg.&quot;), Kitchen_Qual = forcats::fct_relevel( Kitchen_Qual, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) %&gt;% ggplot(aes(Kitchen_Qual)) + geom_bar() + facet_wrap(~ Group) p Figure 3.28: Kitchen quality for homes that sold below average (left panel) and above average (right panel). Figure 3.29 builds onto Figure 3.28 using facet_grid(). In this example, we assess kitchen quality for homes that sold below average and above average across the different neighborhoods. ames %&gt;% mutate( Group = ifelse( Sale_Price &gt; mean(Sale_Price), yes = &quot;Sold above avg.&quot;, no = &quot;Sold below avg.&quot;), Kitchen_Qual = forcats::fct_relevel( Kitchen_Qual, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ) %&gt;% group_by(Neighborhood, Group, Kitchen_Qual) %&gt;% tally() %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(Kitchen_Qual, pct)) + geom_col() + facet_grid(Neighborhood ~ Group) + theme(strip.text.y = element_text(angle = 0, hjust = 0)) Figure 3.29: Kitchen quality for homes that sold below average (left panel) and above average (right panel) across the different neighborhoods. 3.4.1.1 Parallel coordinate plots Parallel coordinate plots (PCPs) are also a great way to visualize continuous variables across multiple variables. In PCPs, a vertical axis is drawn for each variable. Then each observation is represented by drawing a line that connects its values on the different axis, creating a multivariate profile. To create a PCP, we can use the GGally::ggparcoord() function. By default, GGally::ggparcoord() will standardize the variables based on a \\(z\\)-score distribution; however, there are many options for scaling (see ?GGally::ggparcoord). A benefit of using PCPs is that you can visualize observations across both continuous and categorical variables. In the example below we include Overall_Qual, an ordered factor with ten levels &quot;Very Poor&quot; &lt; &quot;Poor&quot; &lt; &quot;Fair&quot; &lt; ... &lt; &quot;Excellent&quot; &lt; &quot;Very Excellent&quot; having integer values of 1‚Äì10. When including a factor variable, GGally::ggparcoord() will use the factor integer levels as their corresponding value, so it is important to appropriately order any factors you want to include. # Variables of interest variables &lt;- c(&quot;Sale_Price&quot;, &quot;Year_Built&quot;, &quot;Year_Remod_Add&quot;, &quot;Overall_Qual&quot;) # Parallel coordinate plot ames %&gt;% select(variables) %&gt;% GGally::ggparcoord(alpha = 0.05, scale = &quot;center&quot;) Figure 3.30: Parallel coordinate plot. The darker bands in the above plot illustrate several features. The observations with higher sales prices tend to be built in more recent years, be remodeled in recent years and be categorized in the top half of the overall quality measures. In contracts, homes with lower sales prices tend to be more out-dated (based on older built and remodel dates) and have lower quality ratings. We also see some homes with exceptionally old build dates that have much newer remodel dates but still have just average quality ratings. We can make this more explicit by adding a new variable to indicate if a sale price is above average. We can then tell GGally::ggparcood() to group by this new variable. Now we clearly see that above average sale prices are related to much newer homes. ames %&gt;% select(variables) %&gt;% mutate(Above_Avg = Sale_Price &gt; mean(Sale_Price)) %&gt;% GGally::ggparcoord( alpha = 0.05, scale = &quot;center&quot;, columns = 1:4, groupColumn = &quot;Above_Avg&quot; ) Figure 3.31: Refined parallel coordinate plot. 3.4.1.2 Mosaic plots Mosaic plots are a graphical method for visualizing data from two or more qualitative variables. In this visual the graphics area is divided up into rectangles proportional in size to the counts of the combinations they represent. ames2 &lt;- ames %&gt;% mutate( Above_Avg = Sale_Price &gt; mean(Sale_Price), Garage_Type = abbreviate(Garage_Type), Garage_Qual = abbreviate(Garage_Qual) ) par(mfrow = c(1, 2)) mosaicplot(Above_Avg ~ Garage_Type, data = ames2, las = 1) mosaicplot(Above_Avg ~ Garage_Type + Garage_Cars, data = ames2, las = 1) Figure 3.32: Mosaic plot. 3.4.1.3 Tree maps Tree maps are also a useful visualization aimed at assessing the hierarchical structure of data. Tree maps are primarily used to assess a numeric value across multiple categories. It can be useful to assess the counts or proportions of a categorical variable nested within other categorical variables. For example, we can use a tree map to visualize the above right mosaic plot that illustrates the number of homes sold above and below average sales price with different garage characteristics. We can see in the tree map that houses with above average prices tend to have attached 2 and 3-car garages. Houses sold below average price have more attached 1-car garages and also have far more detached garages. ames %&gt;% mutate(Above_Below = ifelse(Sale_Price &gt; mean(Sale_Price), &quot;Above Avg&quot;, &quot;Below Avg&quot;)) %&gt;% count(Garage_Type, Garage_Cars, Above_Below) %&gt;% treemap::treemap( index = c(&quot;Above_Below&quot;, &quot;Garage_Type&quot;, &quot;Garage_Cars&quot;), vSize = &quot;n&quot; ) Figure 3.33: Treemaps to illustrate hierarchical structures. 3.4.1.4 Heat maps A heat map is essentially a false color image of a matrix or data set. Heat maps can be extremely useful in identifying clusters of values; a common use is in plotting a correlation matrix (Figure 3.34). In the code chunk below we select all the numeric variables in ames, compute the correlation matrix between them, and visualize the results with a heat map. Bright/dark spots represent clusters of observations with similar correlations. From Figure 3.34, we can see that Sale_Price (3rd row from top) has a relatively weak linear association with variables such as BsmtFin_Sf_1, Bsmt_Unf_SF, Longitude, and Enclosed_Porch. The larger correlations values for Sale_Price align with variables such as Garage_Cars, Garage_Area, and First_Flr_SF, etc. ames %&gt;% select_if(is.numeric) %&gt;% # select all the numeric columns cor() %&gt;% # compute the correlation matrix heatmap( symm = TRUE, # since correlation matrices are symmetric! col = viridis::inferno(nrow(ames)) ) Figure 3.34: heat map where yellow represents highly correlated values and red represents weakly correlated values. 3.4.1.5 Generalized pairs plot When dealing with a small data set (or subset of a large data set), it can be useful to construct a matrix of plots comparing two-way relationships across a number of variables. In the code chunk below we (i) select Sale_Price and all variables names that contain &quot;sf&quot; (i.e., all square footage variables), (ii) scale all variables, and (iii) display scatter plots and correlation values with the GGally::ggpairs() function. The results are displayed in Figure 3.35. ames %&gt;% select(Sale_Price, contains(&quot;sf&quot;)) %&gt;% # select column names containing &quot;sf&quot; purrr::map_df(scale) %&gt;% GGally::ggpairs() Figure 3.35: Pairwise scatter plot. 3.5 Data quality Data quality is an important issue for any project involving analyzing data. Data quality issues deserves an entire book in its own right, and a good reference is the The Quartz guide to bad data (‚ÄúThe Quartz Guide to Bad Data,‚Äù n.d.). In this section, we discuss one topic of particular importance: visualizing missing data. It is important to understand the distribution of missing values (i.e., NA) is any data set. So far, we have been using a pre-processed version of the Ames housing data set (via the AmesHousing::make_ames() function). However, if we use the raw Ames housing data (via AmesHousing::ames_raw), there are actually 13,997 missing values‚Äîthere is at least one missing values in each row of the original data! sum(is.na(AmesHousing::ames_raw)) ## [1] 13997 It is important to understand the distribution of missing values in a data set in order to determine 1) if any variable(s) should be eliminated prior to analysis, or 2) if any values need to be imputed9. Heat maps are an efficient way to visualize the distribution of missing values for small- to medium-sized data sets. The code is.na(&lt;data-frame-name&gt;) will return a matrix of the same dimension as the given dsta frame, but each cell will contain either TRUE (if the corresponding value is missing) or FALSE (if the corresponding value is not missing). To construct such a plot, we can use R‚Äô2 built-in heatmap() or image() functions, or ggplot2‚Äôs geom_raster() function, among others; we use geom_raster() below. This allows us to easily see where the majority of missing values occur (i.e., in the variables Alley, Fireplace Qual, Pool QC, Fence, and Misc Feature). Due to their high frequency of missingness, these variables would likely need to be removed prior to statiscial analysis, or imputed (i.e., filled in with intelligent guesses). We can also spot obvious patterns of missingness. For example, missing values appear to occur within the same observations across all garage variables. AmesHousing::ames_raw %&gt;% is.na() %&gt;% reshape2::melt() %&gt;% ggplot(aes(Var2, Var1, fill=value)) + geom_raster() + coord_flip() + scale_fill_grey(name = &quot;&quot;, labels = c(&quot;Present&quot;, &quot;Missing&quot;)) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(axis.text.y = element_text(size = 4)) Figure 3.36: Heat map of ,issing values in the raw Ames housing data. Digging a little deeper into these variables, we might notice that Garage_Cars and Garage_Area contain the value 0 whenever the other Garage_xx variables have missing values (i.e. a value of NA). This might be because they did not have a way to identify houses with no garages when the data were originally collected; and therefore, all houses with no garage were identified by including nothing. Since this missingness is informative, it would be appropriate to impute NA with a new category level (e.g., &quot;None&quot;) for these garage variables. Circumstances like this tend to only arise upon careful descriptive and visual examination of the data! AmesHousing::ames_raw %&gt;% filter(is.na(`Garage Type`)) %&gt;% select(contains(&quot;garage&quot;)) ## # A tibble: 157 x 7 ## `Garage Type` `Garage Yr Blt` `Garage Finish` ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 &lt;NA&gt; NA &lt;NA&gt; ## 2 &lt;NA&gt; NA &lt;NA&gt; ## 3 &lt;NA&gt; NA &lt;NA&gt; ## 4 &lt;NA&gt; NA &lt;NA&gt; ## 5 &lt;NA&gt; NA &lt;NA&gt; ## 6 &lt;NA&gt; NA &lt;NA&gt; ## 7 &lt;NA&gt; NA &lt;NA&gt; ## 8 &lt;NA&gt; NA &lt;NA&gt; ## 9 &lt;NA&gt; NA &lt;NA&gt; ## 10 &lt;NA&gt; NA &lt;NA&gt; ## # ... with 147 more rows, and 4 more variables: ## # `Garage Cars` &lt;int&gt;, `Garage Area` &lt;int&gt;, `Garage ## # Qual` &lt;chr&gt;, `Garage Cond` &lt;chr&gt; The visna() function in R package extracat (Pilhoefer 2018) allows for easy visualization of missing data patterns. We illustrate this functionality below using the raw Ames housing data (Figure @ref(fig: 04-missingness-02)). The columns of the heat map represent the 82 variables of the raw data and the rows represent the observations. By default, missing values (i.e., NA) are inidcated via a blue cell. The variables and patterns have been ordered by the number of NAs on both rows and columns (i.e., sort = &quot;b&quot;). The bars beneath the columns show the proportion of NAs by variable and the bars on the right show the relative frequency of NAs. extracat::visna(AmesHousing::ames_raw, sort = &quot;b&quot;) Figure 3.37: Visualizing missing patterns Data can be missing for different reasons. Perhaps the values was never recroded (or lost in trabnslation), or it was recorded an error (a common feature of data enetered by hand). Regardless, it is important to identify and attempt to understand how missing values are distributed across a data set as it can provide insight into how to deal with these observations. 3.6 Further reading List some good books, like Cleveland (1993)! 3.7 Exercises Coming soon! Can use this to introduce additional plots/geoms! References "],
["inference.html", "Chapter 4 Statistical Inference 4.1 The frequentist approach 4.2 One- and two-sample t-tests 4.3 Tests involving more than two means: ANOVA models 4.4 Testing for association in contingency tables 4.5 Nonparametric tests 4.6 The nonparametric bootstrap 4.7 Further reading 4.8 Exercises", " Chapter 4 Statistical Inference ‚ÄúTo consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.‚Äù ‚Äî Sir Ronald Fisher Suppose you are moving to Ames, Iowa and are considering buying a home. How would you know whether or not the house you are considering is averagely priced, significantly more expensive, or significantly less expensive? With all of the data available on the web, it is possible to gather data selling prices of similar homes in the area. From this data, which we call a reference distribution, it can be determined whether or not the price of a particular home is on par with similar homes in the neighborhood. Say, for example, the price of a new home for sale in Ames, Iowa is $610,000. Using historical data, we can compare this price against a reference distribution. This is illustrated in the code chunk below using the ames data frame. Sale_Price &lt;- AmesHousing::make_ames()$Sale_Price (610000 - mean(Sale_Price)) / sd(Sale_Price) # compute a z-score ## [1] 5.373 So a house costing $610,000 is more than five standard deviations beyond the mean of all the houses sold between the years 2006 and 2010‚Äîof course, a more fair comparison would only involve houses with similar features (e.g., a fireplace, finished basement, same neighborhood, etc.). Classical statistical inference (e.g., significance testing) is a similar process. An investigator or analyst considers the result from a particular experiment and wants to know whether or not the result is statistically significant10 (e.g., due to the varying experimental conditions), or due to chance alone. In order to make this conclusion, a relative reference set is required that characterizes the outcome if the varying experimental factors truly had no impact on the result. The observed outcome can then be compared to this reference distribution and the statistical significance of the result can be quantified. This approach to statistical inference is called the frequentist approach‚Äîin contrast to Bayesian inference which is not discussed in this book. 4.1 The frequentist approach The most common methods in statistical inference are based on the frequentist approach to probability. Many of the common statistical tests, like the one-sample \\(t\\)-test, follow the same paradigm: compute a test statistic associated with the population attribute of interest (e.g., the mean), determine it‚Äôs sampling distribution, and use the sampling distribution to compute a \\(p\\)-value, construct a confidence interval, etc. The sampling distribution of a statistic (e.g., a test statistic), based on a sample of size \\(n\\), is the distribution obtained after taking every possible sample of size \\(n\\) from the population of interest and computing the sample statistic for each; see, for example, Figure 4.1. Figure 4.1: Frequentist approach to sampling and sampling distributions. In some cases, the sampling distribution of the statistic is known, provided certain assumptions are met (like independent observations and normality). For example, consider a random sample \\(x_1, x_2, \\dots, x_n\\) from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma ^ 2\\). As it turns out, the statistic \\[\\begin{equation} z_{obs} = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}, \\tag{4.1} \\end{equation}\\] has a standard normal distribution. From this sampling distribution, we can formulate a confidence for the true mean \\(\\mu\\), or test specific hypotheses. In practice, \\(\\sigma\\) is unknown and is estimated using the sample standard deviation, \\(s\\). Replacing \\(\\sigma\\) with \\(s\\) in Equation (4.1) results in another statistic, called the \\(t\\)-statistic, and is given by \\[\\begin{equation} t_{obs} = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}. \\tag{4.2} \\end{equation}\\] Student (1908) showed that \\(t_{obs}\\) follows a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. In more complicated examples, the sampling distribution of a statistic is not known, or is rather complicated (e.g., the correlation coefficient or the ratio of two means), but can be simulated through a process called the booststrap, which we discuss in ??. 4.1.1 The central limit theorem One of the most common goals in classical statistical inference is to make inference regarding the mean of a population: \\[ H_0: \\mu = \\mu_0 \\quad vs. \\quad H_1: \\mu \\ne \\mu_0, \\] where \\(\\mu\\) is the true population mean and \\(\\mu_0\\) is some hypothetical value. Assume we have a random sample \\(x_1, x_2, \\dots, x_n\\) from the population of interest. If the data are normally distributed, we can test this hypothesis using a standard \\(t\\)-test (discussed later). If the data are not normally distributed, then the central limit theorem (CLM) tells us that the sampling distribution of the sample mean (or more simply the sample total) will be approximately normal for sufficiently large \\(n\\). How large does \\(n\\) need to be? The answer depends on how far the true distribution deviates from normality! A common rule of thumb, though not always adequate, is that \\(n &gt; 30\\) is sufficient to invoke the CLM. For instance, as we saw in Chapters 2‚Äì3, the distribution of Sale_Price is quite skewed to the right. However, the sampling distribution of the mean from this population will be approximately normal provided \\(n\\) is sufficiently large. For example, we simulated 10^{4} sample means from Sale_Price based on sample of various sizes. The resulting sampling distributions are displayed in Figure 4.2. Clearly, the sampling distribution becomes more bell-shaped and normal looking as the sample size increases; for this population, \\(n = 30\\) seems sufficient. Figure 4.2: Sampling distribution of mean sale price based on samples of size \\(n = 5\\) (top left), \\(n = 10\\) (top right), \\(n = 30\\) (bottom left), and \\(n = 100\\) (bottom right). The classical tests and procedures discussed in this chapter assume that we are sampling from populations that are infinitely large. In most cases, however, the populations from which we obtain samples are finite. 4.1.2 Hypothesis testing Univariate statistical tests of hypotheses usually concern a single parameter, say \\(\\theta\\). For instance, \\(\\theta\\) could be the mean of a single population (i.e., \\(\\theta = \\mu\\)), or the difference between the means of two populations (i.e., \\(\\theta = \\mu_1 - \\mu_2\\)). The null hypothesis, denoted \\(H_0\\), represents the status quo of \\(\\theta\\) and the alternative hypothesis, denoted \\(H_1\\) or \\(H_a\\), represents the research hypothesis regarding \\(\\theta\\). For instance, in comparing the means of two populations, we may be interested in testing \\[ H_0: \\mu_1 - \\mu_2 = \\delta_0 \\quad \\text{vs.} \\quad H_1: \\mu_1 - \\mu_2 \\ne \\delta_0, \\] where \\(\\delta_0\\) is some hypothetical value (usually \\(0\\) signifying no difference in the means of the two populations). To carry out such tests, we require an estimate of \\(\\theta\\), say \\(\\widehat{\\theta}\\), and its corresponding sampling distribution. 4.1.3 One-sided versus two-sided tests The previous hypothesis test involved a two-sided alternative (i.e., \\(H_1: \\mu_1 - \\mu_2 \\ne \\delta_0\\)). Such a test is called a two-sided test. It is possible, though less common, to use a one-sided alternative of the form \\[ H_1: \\theta &lt; \\theta_0 \\quad \\text{or} \\quad H_1: \\theta &gt; \\theta_0 \\] A word of caution regarding one-sided alternatives is to avoid them! These are more common in experimental studies where a priori information is available suggesting that the population attribute of interest is either less than or greater than some hypothetical value. Although the proceeding discussions apply specifically to two-sided tests, the methodology can easily be amended to accommodate one-sided tests. 4.1.4 Type I and type II errors Statistical significance testing relies on the presumption of innocence. That is, we fail to reject the null hypothesis unless the data provide sufficient evidence to say otherwise. For example, in the US criminal justice system, the defendant is assumed innocent until proven guilty: \\[ H_0: \\text{Defendant is innocent} \\quad \\text{vs.} \\quad H_1: \\text{Defendant is guilty} \\] Whenever we conduct a statistical test of hypothesis, a decision is made regarding the null hypothesis. Since this is a binary decision, there are four possible outcomes, two of which are errors: Convict the defendant when the defendant is guilty (a good decision) Convict the defendant when the defendant is innocent (a bad decision) Fail to convict the defendant when the defendant is guilty (a bad decision) Fail to convict the defendant when the defendant is innocent (a good decision) Which decision is worst? Naturally, it would be worse to convict an innocent person than to let a guilty person go free. We call the first type of error a type I error, and the second a type II error. Furthermore, we denote the probability of making a type I error as \\(\\alpha\\) and the probability of making a type II error as \\(\\beta\\). The classic approach to statistical testing fixes the probability of making a type I error ahead of time (e.g., \\(\\alpha = 0.05\\)), we then do our best to reduce the risk of making a type II error (e.g., collecting sufficient sample size). In order to carry out a test with the goal of mitigating the probability of making a type II error we would conduct a power analysis, which we do not discuss here‚Äîthe classic reference is Cohen (1988). INSERT TABLE HERE 4.1.5 \\(p\\)-values There are three equivalent approaches to conducting hypothesis tests: The rejection region approach (the least informative). The \\(p\\)-value approach. The confidence interval approach. The latter two are the most informative as they provide information beyond our decision to simply reject or fail to reject a null hypothesis. This section discusses \\(p\\)-values. \\(p\\)-values provide a measure of evidence in favor of or against the null hypothesis. The \\(p\\)-value, denoted \\(p\\), can be thought of as the observed significance level. We would reject the null hypothesis at the \\(\\alpha\\) level of significance whenever \\(p &lt; \\alpha\\). Table 4.1 provides a general guideline for interpreting \\(p\\)-values. Table 4.1: p-values. Result Interpretation \\(p \\le 0.01\\) Very strong evidence against the null \\(0.01 &lt; p \\le 0.05\\) Strong evidence against the null \\(0.05 &lt; p \\le 0.10\\) Moderate evidence against the null \\(0.10 &lt; p \\le 0.20\\) Weak evidence against the null \\(p &gt; 0.20\\) No evidence against the null To put another way, \\(p\\)-values tell us the smallest value of \\(\\alpha\\) that would result in rejecting the null hypothesis. Keep in mind, however, that it is highly unethical to change \\(\\alpha\\) after comparing it to the \\(p\\)-value in order to change the resulting decision of the test‚Äîthe significance level should be stated before the data are inspected, or even collected, and never be changed thereafter. To compute a \\(p\\)-value, we need to be able to compute probabilities from the sampling distribution of the test statistic under the assumption that the null hypothesis is true. Most statistical tests built into R, however, compute \\(p\\)-values that are provided in the output. 4.1.6 Confidence intervals Confidence intervals assign a range of plausible values to the population attribute of interest. A traditional \\(100\\left(1 - \\alpha\\right)\\)% confidence interval for a population parameter \\(\\theta\\) has the form \\[\\begin{equation} \\widehat{\\theta} \\pm \\gamma_{1 - \\alpha / 2} \\widehat{SE}\\left(\\widehat{\\theta}\\right), \\tag{4.3} \\end{equation}\\] where \\(\\widehat{\\theta}\\) is an appropriate estimate of \\(\\theta\\), \\(\\gamma_{1 - \\alpha / 2}\\) is the \\(1 - \\alpha / 2\\) quantile from an appropriate reference distribution, and \\(\\widehat{SE}\\left(\\widehat{\\theta}\\right)\\) is the estimated standard error of \\(\\widehat{\\theta}\\). Confidence intervals of the form (4.3) are commonly used in practice, but are not always accurate‚Äîthey assume that the sampling distribution of \\(\\widehat{\\theta}\\) is symmetric. Later in this chapter, we discuss the nonparametric bootstrap, a simulation-based approach to estimating \\(\\widehat{SE}\\left(\\widehat{\\theta}\\right)\\) and computing confidence intervals that does not assume a theoretical sampling distribution for \\(\\widehat{\\theta}\\). 4.2 One- and two-sample t-tests One of the most common goals in classical statistical inference is to make inference regarding the mean of a single population (\\(\\theta = \\mu\\)) or the difference in means between two populations (\\(\\theta = \\mu_1 - \\mu_2\\)). And the corresponding test has the form \\[ H_0: \\theta = \\theta_0 \\quad \\text{vs.} \\quad H_1: \\theta \\ne \\theta_0, \\] where \\(\\theta_0\\) is the hypothesized value of the mean or difference in means. 4.2.1 One-sample t-test Assume we have a random sample \\(x_1, x_2, \\dots, x_n\\) from the population of interest with sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\). If the data are normally distributed, we can test this hypothesis using a standard \\(t\\)-test. If the data are not normally distributed, then the central limit theorem (CLM) tells us that the sampling distribution of the sample mean will be approximately normal for sufficiently large \\(n\\). How large does \\(n\\) need to be? The answer depends on how far the true distribution deviates from normality! A common rule of thumb, though not always sufficient, is that \\(n &gt; 30\\) is required to invoke the CLM. The test statistic for the one-sample \\(t\\)-test is \\[ t_{obs} = \\frac{\\bar{x} - \\theta_0}{s / \\sqrt{n}}. \\] If the null hypothesis is true, then \\(t_{obs}\\) comes from a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. We would reject the null hypothesis if \\(|t_{obs}| &gt; t_{1 - \\alpha / 2, n - 1}\\), where \\(t_{1 - \\alpha / 2, n - 1}\\) is the \\(1 - \\alpha / 2\\) quantile from a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. A \\(100\\left(1 - \\alpha\\right)\\)% confidence interval for true mean is given by \\[\\begin{equation} \\bar{x} \\pm t_{1 - \\alpha / 2, n - 1} \\times \\frac{s}{\\sqrt{n}} \\tag{4.4} \\end{equation}\\] Correspondingly, we would reject the null hypothesis whenever the hypothesized value \\(\\theta_0\\) is not contained within (4.4). A \\(p\\)-value for the test can also be computes as \\(p = 2 \\times Pr\\left(T_{n - 1} &gt; |t_{obs}|\\right)\\), where \\(T_{n - 1}\\) is a random variable following a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. In other words, the \\(p\\)-value is the area under the curve of a \\(t\\)-distribution with \\(n - 1\\) degrees of freedom to the right \\(t_{obs}\\); see Figure 4.3. Figure 4.3: \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. The shaded area corresponds to the \\(p\\)-value of the test. To illustrate, the one-sample \\(t\\)-test, we‚Äôll use the ames data set. In Chapters 2‚Äì3, we provided both numerical and visual descriptions of Sale_Price. Below, we use R‚Äôs built-in t.test function to obtain a 95% confidence interval for the true mean selling price based on a random sample of size \\(n = 50\\). set.seed(1551) # for reproducibility sp50 &lt;- sample(Sale_Price, size = 50, replace = FALSE) t.test(sp50, alternative = &quot;two.sided&quot;, conf.level = 0.95) ## ## One Sample t-test ## ## data: sp50 ## t = 21, df = 49, p-value &lt;2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 158237 192246 ## sample estimates: ## mean of x ## 175241 Based on the output, a 95% confidence interval for the mean selling price, based on a sample of size \\(n = 50\\) is \\(\\left(1.5824\\times 10^{5}, 1.9225\\times 10^{5}\\right)\\). By default, the t.test function uses \\(\\theta_0 = 0\\). To specify a different value, use the mu argument: t.test(sp50, alternative = &quot;two.sided&quot;, conf.level = 0.95, mu = 600000) ## ## One Sample t-test ## ## data: sp50 ## t = -50, df = 49, p-value &lt;2e-16 ## alternative hypothesis: true mean is not equal to 6e+05 ## 95 percent confidence interval: ## 158237 192246 ## sample estimates: ## mean of x ## 175241 The resulting confidence interval is the same, but the corresponding \\(p\\)-value now correspond to the testing whether or not the population mean significantly differs from the value $\\(600,000\\); in this case, we would fail to reject the null hypothesis at the \\(0.05\\) level of significance. 4.2.2 Two-sample t-test Assume we have a random sample \\(x_1, x_2, \\dots, x_n\\) from one population of interest with sample mean \\(\\bar{x}\\) and sample standard deviation \\(s_x\\) and another random sample \\(y_1, y_2, \\dots, y_n\\) from a second population of interest with sample mean \\(\\bar{y}\\) and sample standard deviation \\(s_y\\). For the two-sample \\(t\\)-test, \\(\\theta = \\mu_x - \\mu_y\\) and \\(\\theta_0\\) is often \\(0\\) (i.e., no difference between the population means). Of course, no two means are exactly equal! What we really care about is whether or not the true difference is small enough to say that the two means are practically the same. The more data we have, the smaller a true difference we are able to detect. In performing a two-sample \\(t\\)-test, we have to make an assumption regarding the variance of the two populations. The assumption we make here determines which two-sample \\(t\\)-test will be used: Pooled variance \\(t\\)-test (\\(\\sigma_1 ^ 2 = \\sigma_2 ^ 2\\)) Welch‚Äôs two-sample \\(t\\)-test (\\(\\sigma_1 ^ 2 \\ne \\sigma_2 ^ 2\\)). Since the variances of two populations are not typically equal in practice, we discuss Welch‚Äôs two-sample \\(t\\)-test. (Even when the population variance are equal, Welch‚Äôs \\(t\\)-test can still provide satisfactory results.) The test statistic corresponding to Welch‚Äôs \\(t\\)-test is given by \\[ t_{obs} = \\frac{\\bar{x} - \\bar{y}}{\\sqrt{s_1 ^ 2 / n_1 + s_2 ^ 2 / n_2}} \\] The trouble with Welch‚Äôs \\(t\\)-test is that the \\(t_obs\\) does not come from a \\(t\\)-distribution, but can be approximated by a \\(t\\)-distribution with \\(v\\) degrees of freedom, where \\(v\\) is given by the Satterthwaite approximation: \\[ v = TBD. \\] Fortunately, the degrees of freedom is computed automatically by the t.test function. Confidence intervals and \\(p\\)-values can be computed in a manner analogous to the one-sample \\(t\\)-test. For instance, a \\(100\\left(1 - \\alpha\\right)\\)% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\[ \\bar{x}_1 + \\bar{x}_2 \\pm t_{1 - \\alpha / 2, v} \\times \\sqrt{s_1 ^ 2 / n_1 + s_2 ^ 2 / n_2}, \\] where \\(t_{1 - \\alpha / 2, v}\\) is the \\(1 - \\alpha / 2\\) quantile from a \\(t\\)-distribution with \\(v\\) degrees of freedom. A/B test example 4.3 Tests involving more than two means: ANOVA models TBD. 4.4 Testing for association in contingency tables TBD. 4.5 Nonparametric tests TBD. 4.6 The nonparametric bootstrap The statistical tests discussed so far in this chapter assume a theoretical sampling distribution for the corresponding test statistic \\(\\widehat{\\theta}\\), which requires certain assumptions like large sample sizes (when appealing to the CLT) or normality of the population from which the sample was obtained. These assumptions are often difficult to meet in practice. The bootstrap technique (Efron 1979) estimates the sampling \\(\\widehat{\\theta}\\) by direct simulation. In general, bootstrap methods fall into one of two categories, parametric and nonparametric. In this section, we briefly introduce the nonparametric bootstrap. A thorough introduction to the bootstrap and its use in R is provided in Davison and Hinkley (1997). Suppose we have a sample of data \\(\\boldsymbol{x} = \\left\\{x_1, x_2, \\dots, x_n\\right\\}\\) from some population of interest. We can estimate a particular population attribute \\(\\theta\\) using a statistic that is a function of the sample, say \\(\\widehat{\\theta} = t\\left(\\boldsymbol{x}\\right)\\). In order to make inference regarding \\(\\theta\\), we need to know the complete sampling distribution of \\(\\widehat{\\theta}\\). The nonparametric bootstrap constructs the sampling distribution of \\(\\widehat{\\theta}\\) by sampling with replacement from the original sample; that is, treating the sample as if it were the population and making repeated resamples from it, each time recomputing the statistic of interest (see Figure 4.4). This is analogous to the frequentist approach displayed in Figure 4.1. A single bootstrap sample \\[ \\boldsymbol{x} ^ \\star = \\left\\{x_1 ^ \\star, x_2 ^ \\star, \\dots, x_n ^ \\star\\right\\}, \\] where \\(x_i ^ \\star\\) \\(\\left(i = 1, 2, \\dots, n\\right)\\) is drawn from the original sample \\(\\boldsymbol{x}\\) with replacement. Since samples are drawn with replacement, each bootstrap sample will contain duplicate values. In fact, on average, \\(1 - e ^ {-1} \\approx 63.21\\)% of the original sample ends up in any particular bootstrap sample. The original observations not contained in a particular bootstrap sample are considered out-of-bag and will have important implications when discussing random forests in Chapter ?. Figure 4.4: Bootstrap distirbution. With each bootstrap sample, we can compute a bootstrap replicate of the statistic \\(\\widehat{\\theta}\\) \\[ \\widehat{\\theta} ^ \\star = t\\left(\\boldsymbol{x} ^ \\star\\right). \\] Given a large number, say \\(R\\), of bootstrap replicates \\(\\widehat{\\theta}_1 ^ \\star, \\widehat{\\theta}_2 ^ \\star, \\dots, \\widehat{\\theta}_R ^ \\star\\) we can form an estimated sampling distribution for the original statistic \\(\\widehat{\\theta}\\). For example, a useful estimate of the standard error of \\(\\widehat{\\theta}\\) is given by \\[ \\widehat{SE}\\left(\\widehat{\\theta}\\right) ^ \\star = \\sqrt{\\frac{1}{R - 1}\\sum_{i = 1} ^ R\\left(\\widehat{\\theta}_i ^ \\star - \\bar{\\widehat{\\theta} ^ \\star}\\right)}, \\] where \\[ \\bar{\\widehat{\\theta} ^ \\star} = \\frac{1}{R}\\sum_{i = 1} ^ R \\widehat{\\theta}_i ^ \\star \\] is the sample mean of the \\(R\\) bootstrap replicates of \\(\\widehat{\\theta}\\). Given a vector of values in R, a random sample with replacement can be obtained using the sample function, for example x &lt;- 1:10 set.seed(2233) # for reproducibility sample(x, replace = TRUE) ## [1] 2 4 2 6 9 10 8 5 10 4 sample(x, replace = TRUE) ## [1] 5 7 10 4 9 10 5 4 4 9 Notice how some values from the original sample get repeated in each bootstrap sample. For example, 4 shows up three times in the second bootstrap sample. To illustrate, we can bootstrap our sample of \\(n = 50\\) values of Sale_Price to form a bootstrap estimate of the sampling distribution for the mean sale price. This is shown in Figure @ref{fig:bootstrap-distribution-ames} and was produced using the code chunk below. set.seed(1551) # for reproducibility x &lt;- sample(Sale_Price, size = 50, replace = FALSE) # an SRS of size n = 50 bootreps &lt;- plyr::rdply(.n = 10000, .expr = mean(sample(x, replace = TRUE))) ggplot(bootreps, aes(x = V1)) + geom_histogram(bins = 30, color = &quot;white&quot;) + xlab(&quot;Bootstrap repliacte&quot;) Figure 4.5: Bootstrap distirbution of mean sale price based on a random sample of size 50 using \\(R = 10,000\\) bootstrap samples. Compare this to the true sampling distributions of the mean selling price based on various sample sizes illustrated in Figure ?. 4.6.1 Bootstrap confidence intervals Confidence intervals can be obtained directly from the bootstrap distribution of \\(\\widehat{\\theta}\\). For example, to obtain an approximate \\(100\\left(1 - \\alpha\\right)\\) confidence interval for \\(\\theta\\), we can use the \\(\\alpha / 2\\) and \\(1 - \\alpha / 2\\) quantiles from the bootstrap distribution. For the above example, an approximate 95% confidence interval for the mean sale price, we get quantile(bootreps$V1, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 159295 191915 This is called the percentile bootstrap interval. Compare this to the results from the \\(t\\)-test procedure, which gave \\(\\left(1.5824\\times 10^{5}, 1.9225\\times 10^{5}\\right)\\). So how many bootstrap samples are sufficient? The answer, of course, depends on the inferential objectives. Previous studies have shown that far less bootstrap replicates are required when estimating standard errors (e.g., 200) while more are required for computing confidence intervals (e.g., \\(\\ge 1000\\)). With the speed of modern computers, however, the number of bootstrap replicates should be made as large possible within reason! 4.7 Further reading TBD. 4.8 Exercises TBD. References "],
["unsupervised.html", "Chapter 5 Unsupervised learning 5.1 Prerequisites 5.2 Principal Components Analysis 5.3 Cluster Analysis", " Chapter 5 Unsupervised learning Unsupervised learning, includes a set of statistical tools to better understand n observations that contain a set of features (\\(x_1, x_2, \\dots, x_p\\)) but do not contain a response variable (Y). In essence, unsupervised learning is concerned with identifying groups in a data set. The groups may be defined by the rows (i.e., clustering) or the columns (i.e., dimension reduction); however, the motive in each case is quite different. The goal of clustering is to segment observations into similar groups based on the observed variables. For example, to divide consumers into different homogeneous groups, a process known as market segmentation. In dimension reduction, we are often concerned with reducing the number of variables in a data set. For example, classical regression models break down in the presence of highly correlated features. Principal components analysis is a technique that reduces the feature set to a potentially smaller set of uncorrelated variables. These variables are often used as the input variables to simpler modelling techniques like multiple linear regression (Section 7.3). Figure 5.1: Clustering identifies groupings among the observations (left). Dimension reduction identifies groupings among the features (right). Unsupervised learning is often performed as part of an exploratory data analysis. However, the exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Furthermore, it can be hard to assess the quality of results obtained from unsupervised learning methods. The reason for this is simple. If we fit a predictive model using a supervised learning technique (i.e. linear regression), then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don‚Äôt know the true answer‚Äîthe problem is unsupervised. Examples of how unsupervised methods can be used: A marketing firm can divide consumers into different homogeneous groups so that tailored marketing strategies can be developed and deployed for each segment. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose what search results to display to a particular individual based on the click histories of other individuals with similar search patterns. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease. These questions, and many more, can be addressed with unsupervised learning. This chapter covers the unsupervised learning techniques more commonly applied for clustering and dimension reduction purposes which includes: Principal components analysis K-means cluster analysis Hierarchical cluster analysis Alternative approaches for mixed data 5.1 Prerequisites For this section we will use the following packages: library(tidyverse) # data manipulation library(cluster) # clustering algorithms library(factoextra) # clustering algorithms &amp; visualization To perform these unsupervised techniques in R, generally, the data should be prepared as follows: Rows are observations (individuals) and columns are variables (also known as tidy per Wickham and others (2014)). Any missing values in the data must be removed or estimated. Typically, the data must all be numeric values; however, in section ?? we discuss alternative approaches that can be applied to mixed (numeric and categorical) data. Numeric data must be standardized (i.e. centered and scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one. We‚Äôll continue using the Ames housing data throughout this chapter; however, for the initial sections we‚Äôll only use the numeric variables. Furthermore, we‚Äôll remove the sales price variable which results in 34 of the original variables. What results are all numeric variables that describe various features of 2930 homes. Our objective will be to identify various groupings among these variables and observations. ames &lt;- AmesHousing::make_ames() %&gt;% select_if(is.numeric) %&gt;% select(-Sale_Price) dim(ames) ## [1] 2930 34 # remaining numeric variables describing homes names(ames) ## [1] &quot;Lot_Frontage&quot; &quot;Lot_Area&quot; ## [3] &quot;Year_Built&quot; &quot;Year_Remod_Add&quot; ## [5] &quot;Mas_Vnr_Area&quot; &quot;BsmtFin_SF_1&quot; ## [7] &quot;BsmtFin_SF_2&quot; &quot;Bsmt_Unf_SF&quot; ## [9] &quot;Total_Bsmt_SF&quot; &quot;First_Flr_SF&quot; ## [11] &quot;Second_Flr_SF&quot; &quot;Low_Qual_Fin_SF&quot; ## [13] &quot;Gr_Liv_Area&quot; &quot;Bsmt_Full_Bath&quot; ## [15] &quot;Bsmt_Half_Bath&quot; &quot;Full_Bath&quot; ## [17] &quot;Half_Bath&quot; &quot;Bedroom_AbvGr&quot; ## [19] &quot;Kitchen_AbvGr&quot; &quot;TotRms_AbvGrd&quot; ## [21] &quot;Fireplaces&quot; &quot;Garage_Cars&quot; ## [23] &quot;Garage_Area&quot; &quot;Wood_Deck_SF&quot; ## [25] &quot;Open_Porch_SF&quot; &quot;Enclosed_Porch&quot; ## [27] &quot;Three_season_porch&quot; &quot;Screen_Porch&quot; ## [29] &quot;Pool_Area&quot; &quot;Misc_Val&quot; ## [31] &quot;Mo_Sold&quot; &quot;Year_Sold&quot; ## [33] &quot;Longitude&quot; &quot;Latitude&quot; To prepare our data for these techniques, let‚Äôs make sure our data complies with the 3 requirements mentioned above. Our data is already set up in the proper tidy fashion where each row is an individual observation and each column is an individual variable. And as you can see, there are no missing values in the data. # how many missing values are in the data sum(is.na(ames)) ## [1] 0 It is usually beneficial for each variable to be centered at zero due to the fact that it makes comparing each principal component to the mean or the dissimilarity distances for cluster analysis straightforward. This also eliminates potential problems with magnitude differences of each variable. For example, the variance of Year_Built is 914, while the variance of First_Flr_SF is 1,535,789. The Year_Built variable isn‚Äôt necessarily more variable, it‚Äôs simply on a different scale relative to First_Flr_SF. However, due to the math behind PCA and clustering algorithms, the larger magnitude variables will bias the results. However, keep in mind that there may be instances where scaling is not desirable. apply(ames[1:10], 2, var) ## Lot_Frontage Lot_Area Year_Built ## 1.122e+03 6.209e+07 9.148e+02 ## Year_Remod_Add Mas_Vnr_Area BsmtFin_SF_1 ## 4.352e+02 3.191e+04 4.988e+00 ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF ## 2.861e+04 1.932e+05 1.945e+05 ## First_Flr_SF ## 1.536e+05 As we don‚Äôt want our unsupervised techniques to depend on an arbitrary variable unit, we start by standardizing the data using the R function scale. However, scale only works on variables coded as doubles; hence, we need to coerce any integer variables to double. ames_scale &lt;- ames %&gt;% mutate_all(as.double) %&gt;% scale() # check that the mean value for each variable is centered at zero # I only show the first 4 for brevity summary(ames_scale)[, 1:4] ## Lot_Frontage Lot_Area Year_Built ## Min. :-1.721 Min. :-1.123 Min. :-3.285 ## 1st Qu.:-0.437 1st Qu.:-0.344 1st Qu.:-0.574 ## Median : 0.160 Median :-0.090 Median : 0.054 ## Mean : 0.000 Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.608 3rd Qu.: 0.179 3rd Qu.: 0.980 ## Max. : 7.623 Max. :26.027 Max. : 1.278 ## Year_Remod_Add ## Min. :-1.643 ## 1st Qu.:-0.924 ## Median : 0.419 ## Mean : 0.000 ## 3rd Qu.: 0.946 ## Max. : 1.234 PCA and clustering algorithms are influenced by the magnitude of each variable; therefore, the results obtained when we perform these algorithms will also depend on whether the variables have been individually scaled. 5.2 Principal Components Analysis Principal components analysis (PCA) reduces the dimensionality of the feature set, allowing most of the variability to be explained using fewer variables than the original data set. Among our 34 numeric variables within the Ames data set, 16 variables have moderate correlation ( \\(\\geq 0.30\\)) with at least one other variable. Figure 5.2: Top 10 variables containing the strongest correlation with at least one other variable. Multicollinearity such as this can cause problems in some supervised models. Moreover, often we want to simply explain common attributes of a data set in a lower dimensionality than the original data. Within the Ames data, total ground level square footage (Gr_Liv_Area) and total number of rooms above ground (TotRms_AbvGrd) have a correlation of 0.81. These two variables largely capture the same information - living space - of a house. In fact, there are multiple variables in the Ames data that represent living space and are highly correlated. Consequently, it can be useful to represent these highly correlated variables in a lower dimension such as ‚Äúliving space‚Äù. One option includes examining pairwise scatter plots for each variable against every other variable and identifying co-variation. Unfortunately, this is tedious and becomes excessive quickly even with a small number of variables (given \\(p\\) variables there are \\(p(p-1)/2\\) scatterplot combinations. For example, since our Ames data has 35 numeric variables, we would need to examine \\(35(35-1)/2 = 595\\) scatterplots! Clearly, a better method must exist to represent our data in a smaller dimension. PCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much of the variation as possible. The idea is that each of the n observations lives in p-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the p features and we can take these linear combinations of the measurements and reduce the number of plots necessary for visual analysis while retaining most of the information present in the data. 5.2.1 Finding principal components The first principal component of a data set \\(X_1\\), \\(X_2\\), ‚Ä¶, \\(X_p\\) is the linear combination of the features \\[\\begin{equation} \\tag{5.1} Z_{1} = \\phi_{11}X_{1} + \\phi_{21}X_{2} + ... + \\phi_{p1}X_{p}, \\end{equation}\\] that has the largest variance and where \\(\\phi_1\\) is the first principal component loading vector, with elements \\(\\phi_{12}, \\phi_{22},\\dots,\\phi_{p2}\\). The \\(\\phi\\) are normalized, which means that \\(\\sum_{j=1}^{p}{\\phi_{j1}^{2}} = 1\\). After the first principal component \\(Z_1\\) of the features has been determined, we can find the second principal component \\(Z_2\\). The second principal component is the linear combination of \\(X_1,\\dots , X_p\\) that has maximal variance out of all linear combinations that are uncorrelated with \\(Z_1\\). The second principal component scores \\(z_{12}, z_{22}, \\dots, z_{n2}\\) take the form \\[\\begin{equation} \\tag{5.2} Z_{2} = \\phi_{12}X_{1} + \\phi_{22}X_{2} + ... + \\phi_{p2}X_{p} \\end{equation}\\] This proceeds until all principal components are computed. The elements \\(\\phi_{11}, ..., \\phi_{p1}\\) in Eq. 1 are the loadings of the first principal component. To calculate these loadings, we must find the \\(\\phi\\) vector that maximizes the variance. It can be shown using techniques from linear algebra that the eigenvector corresponding to the largest eigenvalue of the covariance matrix is the set of loadings that explains the greatest proportion of the variability. An illustration provides a more intuitive grasp of principal components. Within our Ames housing data, first floor square footage and above ground square footage have a 0.56 correlation, we can explain the covariation of these variables in two dimensions (principal component 1 and principal component 2). We see that the greatest co-variation falls along the first principal component, which is simply the line that minimizes the total squared distance from each point to its orthogonal projection onto the line. Consequently, we can explain the vast majority (93% to be exact) of variability among first floor square footage and above ground square footage simply with the first principal component. Figure 5.3: Principal components of two living area variables. We can extend this to three variables, assessing the relationship between first floor square footage, above ground square footage, and total number of rooms above ground. The first two principal component directions span the plane that best fits the data. It minimizes the sum of squared distances from each point to the plan. As more dimension are added, these visuals are not as intuitive but we‚Äôll see shortly how we can use PCA to extract and visualize informative information. Figure 5.4: Principal components of three living area variables variables. 5.2.2 Performing PCA in R R has several built-in functions (along with numerous add-on packages) that simplifies performing PCA. One of these built-in functions is prcomp. With prcomp we can perform PCA calculations quickly. By default, the prcomp function centers the variables to have mean zero. By using the argument scale = TRUE, we can scale the variables to have standard deviation one; however, since we already standardized our data we‚Äôll remove this option. The output from prcomp contains a number of items. # perform PCA pca_result &lt;- prcomp(ames_scale, scale = FALSE) # various output provided by the model names(pca_result) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; ## [5] &quot;x&quot; The rotation matrix provides the principal component loadings. There are 35 distinct principal components for our data. This is to be expected because you can have the same number of components as you have variables. However, shortly I‚Äôll show you how to understand how much each component explains our data. As for the principal component loadings - remember, the loadings represent \\(\\phi_{12}, \\phi_{22},\\dots,\\phi_{p2}\\) in Equation (??). Thus, these loadings represent coefficients in which it illustrates each variables influence on the principal component. By default, loadings (aka eigenvectors) in R point in the negative direction. For this example, we‚Äôd prefer them to point in the positive direction because it leads to more logical insights. To use the positive-pointing vector, we multiply the default loadings by -1. # convert loadings to positive pca_result$rotation &lt;- -pca_result$rotation # look at the first 5 principal component loadings and the first 5 rows pca_result$rotation[1:5, 1:5] ## PC1 PC2 PC3 PC4 ## Lot_Frontage 0.09618 -0.01628 0.21698 -0.10323 ## Lot_Area 0.13045 0.03500 0.25828 0.13446 ## Year_Built 0.24156 0.21729 -0.32987 -0.12749 ## Year_Remod_Add 0.22212 0.10844 -0.30413 -0.13136 ## Mas_Vnr_Area 0.21338 0.07667 0.02667 0.02696 ## PC5 ## Lot_Frontage 0.01746 ## Lot_Area -0.01150 ## Year_Built -0.07034 ## Year_Remod_Add -0.07856 ## Mas_Vnr_Area 0.16102 We can visualize the level of contribution (relative size of the loadings) each variable has on principal components 1 (left) and 2 (right). From the results below we can see that the first principal component (PC1) roughly corresponds to the main living space and the garage. The second component (PC2) is also affected by living space but appears to consist of several secondary living areas (i.e. second floor, basement). p1 &lt;- fviz_contrib(pca_result, choice = &quot;var&quot;, axes = 1) p2 &lt;- fviz_contrib(pca_result, choice = &quot;var&quot;, axes = 2) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 5.5: Level of contribution each variable has on principal components 1 (left) and 2 (right). We can also obtain the principal components scores from our results as these are stored in the x list item of our results. However, we also want to make a sign adjustment to our scores to point them in the positive direction. # sign adjustment pca_result$x &lt;- -pca_result$x # look at scores for the first five observations for PC1 and PC2 pca_result$x[1:5, 1:2] ## PC1 PC2 ## [1,] 1.1839 1.4308 ## [2,] -2.2839 0.8561 ## [3,] -0.2968 0.6984 ## [4,] 2.7066 0.8782 ## [5,] 0.9716 -0.3285 The principal components scores simply places a standardized score for each observation for each principal component. Thus, above we see that the first observation has a score of 1.18 for PC1. This just states that based on this houses attributes (at least for the numeric variables we are assessing), this house is about 1 standard deviation above the average value for PC1 across all the homes. Since PC1 appears to represent main living space and garage space it appears that this house is about 1 standard deviation more than the average of these attributes compared to all other homes. We can also visualize the contribution of each observation on a particular PC with fviz_contrib by changing choice = &quot;ind&quot;. This basically takes the absoluate values of the scores and plots the percent of total scores for each state. However, this is only useful when we are dealing with a small amount of observations (50 or less). 5.2.3 Selecting the Number of Principal Components So far we have computed principal component attributes and gained a little understanding of what the results initially tell us. However, a primary goal is to use PCA for data reduction. In essence, we want to come out of PCA with less components than variables and with these components telling us as much variation as possible about our data. But how do we decide how many principal components to keep? Do we keep the first four principal components or the first 16? There are three primary approaches in helping to make this decision: Eigenvalue criterion Proportion of variance explained criterion Scree plot criterion 5.2.3.1 Eigenvalue criterion The sum of the eigenvalues is equal to the number of variables entered into the PCA; however, the eigenvalues will range from greater than one to near zero. An eigenvalue of 1 means that the principal component would explain about one variable‚Äôs worth of the variability. The rationale for using the eigenvalue criterion is that each component should explain at least one variable‚Äôs worth of the variability, and therefore, the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained. prcomp automatically computes the standard deviations of the principal components, which is equal to the square roots of the eigenvalues, and stores these values in the pca_result$sdev list item. Therefore, we can compute the eigenvalues easily and identify principal components where the sum of eigenvalues is greater than or equal to 1. Consequently, using this criteria would have us retain the first 11 principal components. # compute eigenvalues eigen &lt;- pca_result$sdev^2 # sum of all eigenvalues equals number of variables sum(eigen) ## [1] 34 # find all PCs where the sum of eigenvalues is greater than or equal to 1 which(eigen &gt;= 1) ## [1] 1 2 3 4 5 6 7 8 9 10 11 Figure 5.6: Eigenvalue criterion keeps all principal components where the sum of the eigenvalues are above or equal to a value of one. 5.2.3.2 Proportion of variance explained criterion The proportion of variance explained (PVE) provides us a technical way to identify the optimal number of principal components to keep based on the total variability that we would like to account for. Mathematically, the PVE for the mth principal component is calculated as: \\[PVE = \\frac{{\\sum_{i=1}^{n}(\\sum_{j=1}^{p}{\\phi_{jm}x_{ij}})^{2}}}{\\sum_{j=1}^{p}\\sum_{i=1}^{n}{x_{ij}^{2}}} \\tag{3}\\] It can be shown that the PVE of the mth principal component can be more simply calculated by taking the mth eigenvalue and dividing it by the number of principal components (or, equivalently, the sum of the eigenvalues). We can create a vector of PVEs for each principal component: # compute the PVE of each principal component PVE &lt;- eigen / sum(eigen) round(PVE, 2) ## [1] 0.18 0.09 0.06 0.06 0.04 0.04 0.03 0.03 0.03 0.03 ## [11] 0.03 0.03 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.02 ## [21] 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01 ## [31] 0.00 0.00 0.00 0.00 The first principal component in our example therefore explains 18% of the variability, and the second principal component explains 9%. Together, the first two principal components explain 27% of the variability. Thus, if an analyst desires to choose the number of principal components that explains at least 75% of the variability in our original data then they would choose the first 16 components. # how many PCs required to explain at least 75% of total variability min(which(cumsum(PVE) &gt;= .75)) ## [1] 16 Figure 5.7: PVE criterion keeps all principal components that are above or equal to a pre-specified threshold of total variability explained. What amount of variability is reasonable? This varies from the problem being addressed and the data being used. However, when the principal components are being used for descriptive purposes only, such as customer profiling, then the proportion of variability explained may be lower than otherwise. When the principal components are to be used as replacements for the original variables, and used for further inference in models downstream, then the PVE should be as much as can conveniently be achieved, given any constraints. 5.2.3.3 Scree plot criterion A scree plot shows the eigenvalues or PVE for each individual principal component. Most scree plots look broadly similiar in shape, starting high on the left, falling rather quickly, and then flattening out at some point. This is because the first component usually explains much of the variability, the next few components explain a moderate amount, and the latter components only explain a small amount of the variability. The scree plot criterion selects all components just before the line flattens out, which is four in our example. fviz_screeplot(pca_result, ncp = 34) Figure 5.8: Scree plot criterion keeps all principal components before the line flattens out. So how many principal components should we use in this example? The frank answer is that there is no single method for determining how many components to use. In this case, differing criteria suggest to retain 4, 11, and 16 (based on a 75% requirement) components. The number you go with depends on your end objective and analytic workflow. If I were merely trying to profile houses I would probably use 4, if I were performing dimension reduction to feed into a downstream predictive model I would likely retain 11 or 16. 5.2.4 Extracting additional insights As previously identified, 27% of the variation in our data can be captured in the first two components: PC1 roughly corresponds to the main living space and the garage. PC2 is also affected by living space but appears to consist of several secondary living areas (i.e. second floor, basement). We can visualize this further with the following multivariate plot. This plot provides the directional influence each variable has on the principal components. The center point represents no influence on PC1 (x-axis) or PC2 (y-axis). Variables that are darker and further to the right of the center vertical line have a strong position influence on PC1 (i.e. Gr_Liv_Area, TotRms_Abv_Grd). Like-wise, variables that are lighter and closer to the center horizontal line have a small influence on PC2 (i.e. Longitude, Porch_SF). fviz_pca_var(pca_result, alpha.var = &quot;contrib&quot;) Figure 5.9: Variable contributions to the first and second principal components. Check out the axes argument (?fviz_pca_var) to compare different principal components in a pairwise fashion. Furthermore, we can see where each observation aligns along these components. This allows us to identify observations that have high values for one or more attributes that influence PC1 and PC2. For example, observations 2181 and 1499 likely have higher values for main living space and garage attributes. Whereas observation 2195 likely has lower values of secondary living space (likely a single story home or does not have a basement). fviz_pca_ind(pca_result, alpha.ind = .3) Figure 5.10: Individual household observations along the first and second principal components. 5.2.5 PCA with mixed data Typical textbook examples of PCA include only numeric data as demonstrated above. However, most real life data sets contain a mixture of numeric and categorical variables. The original Ames housing data set contains 35 numeric variables and 46 categorical variables. Consequently, only focusing on the numeric variables required us to remove over half of our features. Rather, than remove this (likely important) information, we can retain it and still perform PCA. However, the approach we apply differs depending on if we are merely seeking inference on housing attributes or if we plan to use the PCA output for downstream modeling. 5.2.5.1 PCA for inference When performing data mining where the principal components are being used for descriptive purposes only, such as customer profiling, we can convert our categorical variables to numeric information. First, any ordinal variables can be numerically coded in an ordinal fashion. For example, the Overall_Qual variable measures the overall quality of a home across 10 levels (very poor to very excellent). We can recode these variables numerically from 1-10 which now puts them on a continuous dimension. Nominal categorical variables; however, do not contain any natural ordering. One alternative is to one-hot encode these variables to convert them to binary 0/1 values. This significantly expands the number of variables in our data set. # full ames data set --&gt; recode ordinal variables to numeric ames_full &lt;- AmesHousing::make_ames() %&gt;% mutate_if(str_detect(names(.), &quot;Qual|Cond|QC|Qu&quot;), as.numeric) # one-hot encode --&gt; retain only the features and not sale price full_rank &lt;- caret::dummyVars(Sale_Price ~ ., data = ames_full, fullRank = TRUE) ames_1hot &lt;- predict(full_rank, ames_full) # new dimensions dim(ames_1hot) ## [1] 2930 240 Now that all our variables are represented numerically, we can perform PCA as we did in the previous sections. Using the scree plot criterion suggests to retain eight principal components, which explains 50% of the variability across all 240 variables. # apply PCA to one-hot encoded data pca_one_hot &lt;- prcomp(ames_1hot, scale = TRUE) # sign adjustment to loadings and scores pca_one_hot$rotation &lt;- -pca_one_hot$rotation pca_one_hot$x &lt;- -pca_one_hot$x # scree plot fviz_screeplot(pca_result, ncp = 20) Figure 5.11: Scree plot showing the amount of variance explained for each of the first 20 principal components. 5.2.5.2 PCA for downstream modeling When the principal components are to be used as replacements for the original variables, and used for further inference in models downstream, then we want to be a little more particular about how we change the data. Many models (i.e. tree-based) perform quite well when the categorical variables are untransformed. Consequently, often our motives to perform PCA is to reduce the dimension of numerical variables and minimize multicollinearity so that we can apply models that are sensitive to multicollinearity (i.e. linear regression models, neural networks). The caret package provides a function that allows you to perform many preprocessing steps to a set of features. In the following example, I center, scale, and apply PCA to the Ames data. The output of preProcess lists the number of variables centered, scaled, and PCA applied to (34 variables). These represent the 34 numeric variables. It also states that 46 categorical variables were ignored since these preprocessing steps cannot be applied to non-numeric variables. Lastly, it states that 26 principal components were retained to capture the variability explained threshold specified (95%). # get feature set ames_full &lt;- AmesHousing::make_ames() features &lt;- subset(ames_full, select = -Sale_Price) # preprocess data preprocess &lt;- caret::preProcess( x = features, method = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;), thresh = 0.95 ) preprocess ## Created from 2930 samples and 80 variables ## ## Pre-processing: ## - centered (34) ## - ignored (46) ## - principal component signal extraction (34) ## - scaled (34) ## ## PCA needed 26 components to capture 95 percent of the variance You can adjust the thresh argument or also use numComp to explicitly specify the number of varibales to retain. Next, we apply the preProcess object to the training data to return a transformed feature set with the specified preprocessing steps. This new transformed_features data frame contains the original 46 categorical variables and the 26 principal components retained (72 total variables). This preprocessed data can now be fed into any future models, which you will learn about in the predictive analytics section. # create transformed feature set transformed_features &lt;- predict(preprocess, features) dim(transformed_features) ## [1] 2930 72 5.3 Cluster Analysis Clustering is a broad set of techniques for finding subgroups of observations within a data set. When we cluster observations, we want observations in the same group to be similar and observations in different groups to be dissimilar. Because there isn‚Äôt a response variable, this is an unsupervised method, which implies that it seeks to find relationships between the n observations without being trained by a response variable. Clustering allows us to identify which observations are alike, and potentially categorize them therein. Within the cluster analysis domain, there are several clustering techniques we can apply - we will focus on the more common applications. 5.3.1 Clustering distance measures The classification of observations into groups requires some methods for computing the distance of the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. There are many methods to calculate this distance information; the choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters. The classical methods for distance measures are Euclidean and Manhattan distances, which are defined as follow: Euclidean distance: \\[ d_{euc}(x,y) = \\sqrt{\\sum^n_{i=1}(x_i - y_i)^2} \\tag{1}\\] Manhattan distance: \\[ d_{man}(x,y) = \\sum^n_{i=1}|(x_i - y_i)| \\tag{2}\\] Where, x and y are two vectors of length n. Other dissimilarity measures exist such as correlation-based distances, which is widely used for gene expression data analyses. Correlation-based distance is defined by subtracting the correlation coefficient from 1. Different types of correlation methods can be used such as: Pearson correlation distance: \\[d_{cor}(x, y) = 1 - \\frac{\\sum^n_{i=1}(x_i-\\bar x)(y_i - \\bar y)}{\\sqrt{\\sum^n_{i=1}(x_i-\\bar x)^2\\sum^n_{i=1}(y_i - \\bar y)^2}} \\tag{3}\\] Spearman correlation distance: The spearman correlation method computes the correlation between the rank of x and the rank of y variables. \\[d_{spear}(x, y) = 1 - \\frac{\\sum^n_{i=1}(x^\\prime_i-\\bar x^\\prime)(y^\\prime_i - \\bar y^\\prime)}{\\sqrt{\\sum^n_{i=1}(x^\\prime_i-\\bar x^\\prime)^2\\sum^n_{i=1}(y^\\prime_i - \\bar y^\\prime)^2}} \\tag{4}\\] Where \\(x^\\prime_i = rank(x_i)\\) and \\(y^\\prime_i = rank(y_i)\\). Kendall correlation distance: Kendall correlation method measures the correspondence between the ranking of x and y variables. The total number of possible pairings of x with y observations is n(n ‚àí 1)/2, where n is the size of x and y. Begin by ordering the pairs by the x values. If x and y are correlated, then they would have the same relative rank orders. Now, for each \\(y_i\\), count the number of \\(y_j &gt; y_i\\) (concordant pairs (c)) and the number of \\(y_j &lt; y_i\\) (discordant pairs (d)). Kendall correlation distance is defined as follow: \\[d_{kend}(x,y) = 1 - \\frac{n_c - n_d}{\\frac{1}{2}n(n - 1)} \\tag{5}\\] The choice of distance measures is very important, as it has a strong influence on the clustering results. For most common clustering software, the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options. Within R it is simple to compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package. The following figure plots the Euclidean distances between the first 50 homes. This starts to illustrate which observations have large dissimilarities (red) versus those that appear to be fairly similar (blue). get_dist: for computing a distance matrix between the rows of a data matrix. The default distance computed is the Euclidean; however, get_dist also supports distanced described in equations 2-5 above plus others. fviz_dist: for visualizing a distance matrix # we continue using our scaled ames data set distance &lt;- get_dist(ames_scale[1:50, ], method = &quot;euclidean&quot;) fviz_dist( distance, gradient = list(low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;) ) Figure 5.12: Distance matrix plot for first 50 observations. Check out the different distance measures that get_dist accepts with ?get_dist. Use method = &quot;pearson&quot; and see how the distance matrix visualization changes. 5.3.2 K-means clustering K-means clustering is the most commonly used clustering algorithm for partitioning observations into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity). In k-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster. 5.3.2.1 Defining clusters The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid: \\[W(C_k) = \\sum_{x_i \\in C_k}(x_i - \\mu_k)^2 \\tag{6}\\] where: \\(x_i\\) is a data point belonging to the cluster \\(C_k\\) \\(\\mu_k\\) is the mean value of the points assigned to the cluster \\(C_k\\) Each observation (\\(x_i\\)) is assigned to a given cluster such that the sum of squares (SS) distance of the observation to their assigned cluster centers (\\(\\mu_k\\)) is minimized. We define the total within-cluster variation as follows: \\[tot.withiness = \\sum^k_{k=1}W(C_k) = \\sum^k_{k=1}\\sum_{x_i \\in C_k}(x_i - \\mu_k)^2 \\tag{7} \\] The total within-cluster sum of square measures the compactness (i.e goodness) of the clustering and we want it to be as small as possible. 5.3.2.2 K-means algorithm The first step when using k-means clustering is to indicate the number of clusters (k) that will be generated in the final solution. The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also known as cluster means or centroids. Next, each of the remaining objects is assigned to it‚Äôs closest centroid, where closest is defined using the Euclidean distance between the object and the cluster mean. This step is called ‚Äúcluster assignment step‚Äù. After the assignment step, the algorithm computes the new mean value of each cluster. The term cluster ‚Äúcentroid update‚Äù is used to design this step. Now that the centers have been recalculated, every observation is checked again to see if it might be closer to a different cluster. All the objects are reassigned again using the updated cluster means. The cluster assignment and centroid update steps are iteratively repeated until the cluster assignments stop changing (i.e until convergence is achieved). That is, the clusters formed in the current iteration are the same as those obtained in the previous iteration. K-means algorithm can be summarized as follows: Specify the number of clusters (K) to be created (by the analyst) Select randomly k objects from the data set as the initial cluster centers or means Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a Kth cluster is a vector of length p containing the means of all variables for the observations in the kth cluster; p is the number of variables. Iteratively minimize the total within sum of square. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations. 5.3.2.3 Computing k-means clustering in R We can compute k-means in R with the kmeans function. Here will group the data into two clusters (centers = 2). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 25 will generate 25 initial configurations. This approach is often recommended. k2 &lt;- kmeans(ames_scale, centers = 2, nstart = 25) The output of kmeans is a list with several bits of information. The most important being: cluster: A vector of integers (from 1:k) indicating the cluster to which each point is allocated. centers: A matrix of cluster centers. totss: The total sum of squares. withinss: Vector of within-cluster sum of squares, one component per cluster. tot.withinss: Total within-cluster sum of squares, i.e. sum(withinss). betweenss: The between-cluster sum of squares, i.e. \\(totss-tot.withinss\\). size: The number of points in each cluster. If we print the results we‚Äôll see that our groupings resulted in 2 cluster sizes of 1397 and 1533. We can also extract the cluster centers for each variable and the cluster assignment for each observation. str(k2) ## List of 9 ## $ cluster : int [1:2930] 1 2 2 1 1 1 1 1 1 1 ... ## $ centers : num [1:2, 1:34] 0.118 -0.108 0.201 -0.183 0.665 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;1&quot; &quot;2&quot; ## .. ..$ : chr [1:34] &quot;Lot_Frontage&quot; &quot;Lot_Area&quot; &quot;Year_Built&quot; &quot;Year_Remod_Add&quot; ... ## $ totss : num 99586 ## $ withinss : num [1:2] 47995 39746 ## $ tot.withinss: num 87741 ## $ betweenss : num 11845 ## $ size : int [1:2] 1397 1533 ## $ iter : int 1 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the largest amount of variance. So this chart shows that our observations are being clustered primarily based on having above or below average values of dimension 2 (recall from the PCA section that the second principal component - or x axis - largely represented secondary living spaces). fviz_cluster(k2, data = ames_scale, geom = &quot;point&quot;, alpha = .4) You can also visualize clusters against specific variables by assigning choose.vars within fviz_cluster. Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure: k3 &lt;- kmeans(ames_scale, centers = 3, nstart = 25) k4 &lt;- kmeans(ames_scale, centers = 4, nstart = 25) k5 &lt;- kmeans(ames_scale, centers = 5, nstart = 25) # plots to compare p1 &lt;- fviz_cluster(k2, geom = &quot;point&quot;, data = ames_scale, alpha = .4) + ggtitle(&quot;k = 2&quot;) p2 &lt;- fviz_cluster(k3, geom = &quot;point&quot;, data = ames_scale, alpha = .4) + ggtitle(&quot;k = 3&quot;) p3 &lt;- fviz_cluster(k4, geom = &quot;point&quot;, data = ames_scale, alpha = .4) + ggtitle(&quot;k = 4&quot;) p4 &lt;- fviz_cluster(k5, geom = &quot;point&quot;, data = ames_scale, alpha = .4) + ggtitle(&quot;k = 5&quot;) library(gridExtra) grid.arrange(p1, p2, p3, p4, nrow = 2) Although visually assessing the different k cluster outputs tells us where true dilineations occur (or do not occur) between clusters, it does not tell us what the optimal number of clusters is. 5.3.2.4 Determining optimal clusters As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters. To aid the analyst, the following explains the three most popular methods for determining the optimal clusters, which includes: Elbow method Silhouette method Gap statistic 5.3.2.4.1 Elbow method Recall that, the basic idea behind cluster partitioning methods, such as k-means clustering, is to define clusters such that the total within-cluster variation is minimized: \\[ minimize\\Bigg(\\sum^k_{k=1}W(C_k)\\Bigg) \\tag{8}\\] where \\(C_k\\) is the \\(k^{th}\\) cluster and \\(W(C_k)\\) is the within-cluster variation. The total within-cluster sum of square (wss) measures the compactness of the clustering and we want it to be as small as possible. Thus, we can use the following algorithm to define the optimal clusters: Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 20 clusters. For each k, calculate the total within-cluster sum of square (wss). Plot the curve of wss according to the number of clusters k. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters. Fortunately, this process to compute the ‚ÄúElbow method‚Äù has been wrapped up in a single function (fviz_nbclust). Unfortunately, the results are unclear where the elbow actually is? Do we select 3 clusters? 7 clusters? 18? set.seed(123) fviz_nbclust(ames_scale, kmeans, method = &quot;wss&quot;, k.max = 20) 5.3.2.4.2 Average silhouette method In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.11 Similar to the elbow method, this process to compute the ‚Äúaverage silhoutte method‚Äù has been wrapped up in a single function (fviz_nbclust): set.seed(123) fviz_nbclust(ames_scale, kmeans, method = &quot;silhouette&quot;, k.max = 20) 5.3.2.5 Gap statistic method The gap statistic has been published by R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable (\\(x_i\\)) in the data set we compute its range \\([min(x_i), max(x_j)]\\) and generate values for the n points uniformly from the interval min to max. For the observed data and the the reference data, the total intracluster variation is computed using different values of k. The gap statistic for a given k is defined as follow: \\[ Gap_n(k) = E^*_n{log(W_k)} - log(W_k) \\tag{9}\\] Where \\(E^*_n\\) denotes the expectation under a sample size n from the reference distribution. \\(E^*_n\\) is defined via bootstrapping (B) by generating B copies of the reference datasets and, by computing the average \\(log(W^*_k)\\). The gap statistic measures the deviation of the observed \\(W_k\\) value from its expected value under the null hypothesis. The estimate of the optimal clusters (\\(\\hat k\\)) will be the value that maximizes \\(Gap_n(k)\\). This means that the clustering structure is far away from the uniform distribution of points. In short, the algorithm involves the following steps: Cluster the observed data, varying the number of clusters from \\(k=1, \\dots, k_{max}\\), and compute the corresponding \\(W_k\\). Generate B reference data sets and cluster each of them with varying number of clusters \\(k=1, \\dots, k_{max}\\). Compute the estimated gap statistics presented in eq. 9. Let \\(\\bar w = (1/B) \\sum_b log(W^*_{kb})\\), compute the standard deviation \\(sd(k) = \\sqrt{(1/b)\\sum_b(log(W^*_{kb})- \\bar w)^2}\\) and define \\(s_k = sd_k \\times \\sqrt{1 + 1/B}\\). Choose the number of clusters as the smallest k such that \\(Gap(k) \\geq Gap(k+1) - s_{k+1}\\). We can visualize the results with fviz_gap_stat which suggests four clusters as the optimal number of clusters. set.seed(123) fviz_nbclust(ames_scale, kmeans, method = &quot;gap_stat&quot;, k.max = 20, verbose = FALSE) It is often the case, as above, where each metric suggests a different number of preferred clusters. This is part of the challege of clustering and, often, the decision about the number of clusters is partly driven by qualitative assessment and domain knowledge. In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. 5.3.2.6 Extracting results Once you‚Äôve identified the preferred number of clusters, we rerun the analysis with the selected k (8 in this example). We can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level. Here, we can see that homes assigned to cluster 2 tend to be older, one story homes (or with a minimal second floor) whereas homes in cluster 7 tend to be newer (relatively speaking), two story homes with lots of space. # re-run kmeans set.seed(123) final &lt;- kmeans(ames_scale, 8, nstart = 25) # perform descriptive analysis at the cluster level ames %&gt;% mutate(Cluster = final$cluster) %&gt;% group_by(Cluster) %&gt;% summarise_at(vars(Year_Built, Second_Flr_SF, TotRms_AbvGrd, Garage_Area), &quot;mean&quot;) ## # A tibble: 8 x 5 ## Cluster Year_Built Second_Flr_SF TotRms_AbvGrd ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1928. 402. 8.38 ## 2 2 1969. 40.3 5.36 ## 3 3 1997. 5.95 6.48 ## 4 4 1939. 310. 5.84 ## 5 5 1950. 441. 8.61 ## 6 6 1992. 821. 7.05 ## 7 7 1990. 994. 8.99 ## 8 8 1971 691. 8.36 ## # ... with 1 more variable: Garage_Area &lt;dbl&gt; 5.3.2.7 Additional comments K-means clustering is a very simple and fast algorithm. Furthermore, it can efficiently deal with very large data sets. However, there are some weaknesses of the k-means approach. One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of clusters. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram. An additional disadvantage of K-means is that it‚Äôs sensitive to outliers and different results can occur if you change the ordering of your data. The Partitioning Around Medoids (PAM) clustering approach is less sensititive to outliers and provides a robust alternative to k-means to deal with these situations. The next two sections illustrate these clustering approaches. 5.3.3 Hierarchical clustering Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. It does not require us to pre-specify the number of clusters to be generated as is required by the k-means approach. Furthermore, hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram. Figure 5.13: Illustrative dendrogram. 5.3.3.1 Hierarchical Clustering Algorithms Hierarchical clustering can be divided into two main types: agglomerative and divisive. Agglomerative clustering: Also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are a member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram. Divisive hierarchical clustering: It‚Äôs also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster (see figure below). Note that agglomerative clustering is good at identifying small clusters. Divisive hierarchical clustering is good at identifying large clusters. Figure 5.14: AGNES (bottom-up) versus DIANA (top-down) clustering. As we learned in the last section, we measure the (dis)similarity of observations using distance measures (i.e. Euclidean distance, Manhattan distance, etc.) In R, the Euclidean distance is used by default to measure the dissimilarity between each pair of observations. However, a bigger question is: How do we measure the dissimilarity between two clusters of observations? A number of different cluster agglomeration methods (i.e, linkage methods) have been developed to answer to this question. The most common types methods are: Maximum or complete linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters. Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, ‚Äúloose‚Äù clusters. Mean or average linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters. Centroid linkage clustering: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2. Ward‚Äôs minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged. We can see the differences these approaches in the following dendrograms: Figure 5.15: Differing hierarchical clustering outputs based on similarity measures. The important thing to remember is there are multiple ways to define clusters when performing hierarchical cluster analysis. 5.3.3.2 Hierarchical Clustering with R There are different functions available in R for computing hierarchical clustering. The commonly used functions are: hclust [in stats package] and agnes [in cluster package] for agglomerative hierarchical clustering (HC) diana [in cluster package] for divisive HC 5.3.3.2.1 Agglomerative Hierarchical Clustering We can perform agglomerative HC with hclust. First we compute the dissimilarity values with dist and then feed these values into hclust and specify the agglomeration method to be used (i.e. ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù, ‚Äúsingle‚Äù, ‚Äúward.D‚Äù). # for reproducibility set.seed(123) # Dissimilarity matrix d &lt;- dist(ames_scale, method = &quot;euclidean&quot;) # Hierarchical clustering using Complete Linkage hc1 &lt;- hclust(d, method = &quot;complete&quot; ) You could plot the dendrogram with plot(hc1, cex = 0.6, hang = -1); however, due to the number of observations the output is not discernable. Alternatively, we can use the agnes function. This function behaves similar to hclust; however, with the agnes function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure). # for reproducibility set.seed(123) # Compute maximum or &quot;complete linkage clustering with agnes hc2 &lt;- agnes(ames_scale, method = &quot;complete&quot;) # Agglomerative coefficient hc2$ac ## [1] 0.9268 This allows us to find certain hierarchical clustering methods that can identify stronger clustering structures. Here we see that Ward‚Äôs method identifies the strongest clustering structure of the four methods assessed. # methods to assess m &lt;- c( &quot;average&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;ward&quot;) names(m) &lt;- c( &quot;average&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;ward&quot;) # function to compute coefficient ac &lt;- function(x) { agnes(ames_scale, method = x)$ac } # get agglomerative coefficient for each linkage method map_dbl(m, ac) ## average single complete ward ## 0.9139 0.8713 0.9268 0.9767 5.3.3.2.2 Divisive Hierarchical Clustering The R function diana provided by the cluster package allows us to perform divisive hierarchical clustering. diana works similar to agnes; however, there is no method to provide. As before, a divisive coefficient closer to one suggests stronger group distinctions. Consequently, it appears that an agglomerative approach with Ward‚Äôs linkage provides the optimal results. # compute divisive hierarchical clustering hc4 &lt;- diana(ames_scale) # Divise coefficient; amount of clustering structure found hc4$dc ## [1] 0.9191 5.3.3.3 Determining optimal clusters Similar to how we determined optimal clusters with k-means clustering, we can execute similar approaches for hierarchical clustering: hc_ward &lt;- hclust(d, method = &quot;ward.D2&quot; ) p1 &lt;- fviz_nbclust(ames_scale, FUN = hcut, method = &quot;wss&quot;, k.max = 10) p2 &lt;- fviz_nbclust(ames_scale, FUN = hcut, method = &quot;silhouette&quot;, k.max = 10) gap_stat &lt;- clusGap(ames_scale, FUN = hcut, nstart = 25, K.max = 10, B = 20) p3 &lt;- fviz_gap_stat(gap_stat) gridExtra::grid.arrange(p1, p2, p3, nrow = 1) 5.3.3.4 Working with Dendrograms The nice thing about hierarchical clustering is that it provides a complete dendrogram illustrating the relationships between groupings in our data. In the dendrogram displayed below, each leaf corresponds to one observation (aka an individual house). As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height. hc5 &lt;- hclust(d, method = &quot;ward.D2&quot; ) dend_plot &lt;- fviz_dend(hc5) dend_data &lt;- attr(dend_plot, &quot;dendrogram&quot;) dend_cuts &lt;- cut(dend_data, h = 8) fviz_dend(dend_cuts$lower[[2]]) Figure 5.16: A subsection of the dendrogram for illustrative purposes. The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity. The height of the cut to the dendrogram controls the number of clusters obtained. It plays the same role as the k in k-means clustering. In order to identify sub-groups (i.e. clusters), we can cut the dendrogram with cutree. Here, we cut our agglomerative hierarchical clustering model into 8 clusters based on the silhouette results in the previous section. We can see that the concentration of observations are in clusters 1-3. # Ward&#39;s method hc5 &lt;- hclust(d, method = &quot;ward.D2&quot; ) # Cut tree into 4 groups sub_grp &lt;- cutree(hc5, k = 8) # Number of members in each cluster table(sub_grp) ## sub_grp ## 1 2 3 4 5 6 7 8 ## 1363 567 650 36 123 156 24 11 We can plot the entire dendrogram with fviz_dend and highlight the 8 clusters with k = 8. # plot full dendogram fviz_dend( hc5, k = 8, horiz = TRUE, rect = TRUE, rect_fill = TRUE, rect_border = &quot;jco&quot;, k_colors = &quot;jco&quot;, cex = 0.1 ) Figure 5.17: The complete dendogram highlighting all 8 clusters. However, due to the size of our data, the dendrogram is not legible. Consequently, we may want to zoom into one particular cluster. This allows us to see which observations are most similiar within a particular cluster. There is no easy way to get the exact height required to capture all 8 clusters. This is largely trial and error by using different heights until the output of dend_cuts matches the cluster totals identified previously. dend_plot &lt;- fviz_dend(hc5) # create full dendogram dend_data &lt;- attr(dend_plot, &quot;dendrogram&quot;) # extract plot info dend_cuts &lt;- cut(dend_data, h = 70.5) # cut the dendogram at designated height # create sub dendrogram plots p1 &lt;- fviz_dend(dend_cuts$lower[[1]]) p2 &lt;- fviz_dend(dend_cuts$lower[[1]]) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 5.18: A subsection of the dendrogram highlighting cluster 7. 5.3.4 Clustering with mixed data As with PCA, typical textbook examples of clustering include only numeric data as demonstrated above. However, most real life data sets contain a mixture of numeric and categorical variables and whether an observation is similar to another observation should depend on both types of variables. There are a few options for performing clustering with mixed data and we‚Äôll demonstrate on the full ames data set (minus the response variable Sale_Price). To perform k-means clustering on mixed data we can convert any ordinal categorical variables to numeric and one-hot encode the remaining nominal categorical variables. # full ames data set --&gt; recode ordinal variables to numeric ames_full &lt;- AmesHousing::make_ames() %&gt;% mutate_if(str_detect(names(.), &quot;Qual|Cond|QC|Qu&quot;), as.numeric) # one-hot encode --&gt; retain only the features and not sale price full_rank &lt;- caret::dummyVars(Sale_Price ~ ., data = ames_full, fullRank = TRUE) ames_1hot &lt;- predict(full_rank, ames_full) # scale data ames_1hot_scaled &lt;- scale(ames_1hot) # new dimensions dim(ames_1hot_scaled) ## [1] 2930 240 Now that all our variables are represented numerically, we can perform k-means or hierarchical clustering as we did in the previous sections. Using the Silhouette statistic criterion, it appears the optimal number of clusters across all variables are 2. set.seed(123) fviz_nbclust( ames_1hot_scaled, kmeans, method = &quot;silhouette&quot;, k.max = 20, verbose = FALSE ) Figure 5.19: Suggested number of clusters for one-hot encoded Ames data using k-means clustering and the Silhouette criterion. Unfortunately, as the number of features expand, performance of k-means tends to break down and both k-means and hierarchical approaches become slow. It starts to break down typically because your data becomes very sparse (lots of 0s and 1s from one-hot encoding; however, standardizing your data resolves this). Also, as you add more information you are likely to introduce more outliers and since k-means uses the mean, it is not robust to outliers. An alternative to this is to use partitioning around mediods (PAM), which has the same algorithmic steps as k-means but uses the median rather than the mean; making it more robust to outliers. To perform PAM clustering use pam() instead of kmeans(). If you compare k-means and PAM clustering results for a given criterion and experience common results then that is good indication that outliers are not skewing your k-mean results (as is the case here). fviz_nbclust( ames_1hot_scaled, pam, method = &quot;silhouette&quot;, k.max = 20, verbose = FALSE ) Figure 5.20: Suggested number of clusters for one-hot encoded Ames data using PAM clustering and the Silhouette criterion. As your data set becomes larger both hierarchical, k-means, and PAM clustering become slower. An alternative is clustering large applications (CLARA), which performs the same algorithmic process as PAM; however, instead of finding the medioids for the entire data set it considers a small sample size and applies k-means or PAM. CLARA performs the following algorithmic steps: Randomly split the data set into multiple subsets with fixed size. Compute PAM algorithm on each subset and choose the corresponding k mediods. Assign each observation of the entire data set to the closest mediod. Calculate the mean (or sum) of the dissimilarities of the observations to their closest mediod. This is used as a measure of the goodness of fit of the clustering. Retain the sub-data set for which the mean (or sum) is minimal. To perform CLARA clustering use clara() instead of pam() and kmeans(). We see that our results are very similiar using CLARA as with PAM or k-means but using clara took 1/10 of the time! fviz_nbclust( ames_1hot_scaled, clara, method = &quot;silhouette&quot;, k.max = 20, verbose = FALSE ) Figure 5.21: Suggested number of clusters for one-hot encoded Ames data using CLARA clustering and the Silhouette criterion. An additional option is to use the Gower distance. So far, all our examples have used the Euclidean distance, the most popular distance metric used. The Euclidean distance and all the other distance metrics discussed at the beginning of the clustering chapter require numeric data to perform the calculations; this is why clustering typically requires all inputs to be numeric. However, the Gower distance metric allows for mixed data types and, for each variable type, a particular distance calculation that works well for that type is used and scaled to fall between 0 and 1. The metrics used for each data type include: quantitative (interval): range-normalized Manhattan distance, ordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties, nominal: variables with k categories are first converted into k binary columns (one-hot encoded) and then the Dice coefficient is used. To compute the dice metric, the algorithm looks across all one-hot encoded categorical variables and scores them as: a - number of dummies 1 for both individuals b - number of dummies 1 for this and 0 for that c - number of dummies 0 for this and 1 for that d - number of dummies 0 for both and then uses the following formula: \\[ D = \\frac{2a}{2a + b + c} \\] We can use the daisy function to create a Gower distance matrix of our data. # original data minus Sale_Price ames_full &lt;- AmesHousing::make_ames() %&gt;% select(-Sale_Price) # compute Gower distance for original data gower_dst &lt;- daisy(ames_full, metric = &quot;gower&quot;) We can now use the resulting distance matrix and feed it into any clustering algorithm that accepts a distance matrix. This primarily includes pam(), diana(), and agnes() (kmeans() and clara() do not accept distance matrices as inputs). pam_gower &lt;- pam(x = gower_dst, k = 8, diss = TRUE) diana_gower &lt;- diana(x = gower_dst, diss = TRUE) agnes_gower &lt;- agnes(x = gower_dst, diss = TRUE) Another technique that you can apply is Non-Negative Matrix Factorization; however, we do not cover this algorithm in this book. References "],
["fundamentalconcepts.html", "Chapter 6 Fundamental concepts 6.1 Regression problems 6.2 Classification problems 6.3 Algorithm Comparison Guide 6.4 General modeling process", " Chapter 6 Fundamental concepts Predictive analytics continues to grow in importance for many organizations across nearly all domains. A predictive model is used for tasks that involve the prediction of a given output using other variables and their values (features) in the data set. Or as stated by Kuhn and Johnson (2013), predictive modeling is ‚Äúthe process of developing a mathematical tool or model that generates an accurate prediction‚Äù (p. 2). The learning algorithm in a predictive attempts to discover and model the relationship among the target response (the variable being predicted) and the other features (aka predictor variables). Examples of predictive modeling include: using customer attributes to predict the probability of the customer churning in the next 6 weeks, using home attributes to predict the sales price, using employee attributes to predict the likelihood of attrition, using patient attributes and symptoms to predict the risk of readmission, using production attributes to predict time to market. Each of these examples have a defined learning task. They each intend to use attributes (\\(X\\)) to predict an outcome measurement (\\(Y\\)). Throughout this section we will use various terms interchangeably for: \\(X\\): ‚Äúpredictor variables‚Äù, ‚Äúindependent variables‚Äù, ‚Äúattributes‚Äù, ‚Äúfeatures‚Äù, ‚Äúpredictors‚Äù \\(Y\\): ‚Äútarget variable‚Äù, ‚Äúdependent variable‚Äù, ‚Äúresponse‚Äù, ‚Äúoutcome measurement‚Äù The predictive modeling examples above describe what is known as supervised learning. The supervision refers to the fact that the target values provide a supervisory role, which indicates to the learner the task it needs to learn. Specifically, given a set of data, the learning algorithm attempts to optimize a function (the algorithmic steps) to find the combination of feature values that results in a predicted value that is as close to the actual target output as possible. In supervised learning, the training data you feed the algorithm includes the desired solutions. Consequently, the solutions can be used to help supervise the training process to find the optimal algorithm parameters. Supervised learning problems revolve around two primary themes: regression and classification. 6.1 Regression problems When the objective of our supervised learning is to predict a numeric outcome, we refer to this as a regression problem (not to be confused with linear regression modeling). Regression problems revolve around predicting output that falls on a continuous numeric spectrum. In the examples above predicting home sales prices and time to market reflect a regression problem because the output is numeric and continuous. This means, given the combination of predictor values, the response value could fall anywhere along the continuous spectrum. Figure 6.1 illustrates average home sales prices as a function of two home features: year built and total square footage. Depending on the combination of these two features, the expected home sales price could fall anywhere along the plane. Figure 6.1: Average home sales price as a function of year built and total square footage. 6.2 Classification problems When the objective of our supervised learning is to predict a categorical response, we refer to this as a classification problem. Classification problems most commonly revolve around predicting a binary or multinomial response measure such as: did a customer redeem a coupon (yes/no, 1/0), did a customer churn (yes/no, 1/0), did a customer click on our online ad (yes/no, 1/0), classifying customer reviews: binary: positive vs negative multinomial: extremely negative to extremely positive on a 0-5 Likert scale However, when we apply predictive models for classification problems, rather than predict a particular class (i.e. ‚Äúyes‚Äù or ‚Äúno‚Äù), we often predict the probability of a particular class (i.e. yes: .65, no: .35). Then the class with the highest probability becomes the predicted class. Consequently, even though we are performing a classification problem, we are still predicting a numeric output (probability). However, the essence of the problem still makes it a classification problem. 6.3 Algorithm Comparison Guide TODO: do we want something along these lines here? Although there are supervised learning algorithms that can be applied to regression problems but not classification and vice versa, the supervised predictive models we cover in this book can be applied to both.12 These algorithms have become the most popular predictive analytic techniques in recent years. Although the chapters that follow will go into detail on each algorithm, the following provides a quick reference guide that compares and contrasts some of their features. Characteristics Generalized Linear Models (GLM) Regularized GLM Multivariate Adaptive Regression Splines Random Forest Gradient Boosting Machine Deep Learning Captures non-linear relationships Allows n &lt; p Provides automatic feature selection Handles missing values No feature pre-processing required Robust to outliers Easy to tune NA Computational speed Predictive power 6.4 General modeling process Predictive modeling is a very iterative process. If performed and interpreted correctly, we can have great confidence in our outcomes. If not, the results will be useless. Approaching predictive modeling correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning hyperparameters, and assessing model performance (Figure 6.2). Before introducing specific algorithms, this section introduces concepts that are commonly required in the supervised predictive modeling process and that you‚Äôll see briskly covered in each chapter. Figure 6.2: General predictive modeling process. 6.4.1 Prerequisites This section leverages the following packages. library(rsample) library(caret) library(dplyr) To illustrate some of the concepts, we will use the Ames Housing data and employee attrition data introduced in Section 1.4. # ames data ames &lt;- AmesHousing::make_ames() # attrition data churn &lt;- rsample::attrition 6.4.2 Data splitting 6.4.2.1 Spending our data wisely A major goal of the predictive modeling process is to find an algorithm \\(f(x)\\) that most accurately predicts future values (\\(y\\)) based on a set of inputs (\\(x\\)). In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we ‚Äúspend‚Äù our data will help us understand how well our algorithm generalizes to unseen data. To provide an accurate understanding of the generalizability of our final optimal model, we split our data into training and test data sets: Training Set: these data are used to train our algorithms and tune hyper-parameters. Test Set: having chosen a final model, these data are used to estimate its prediction error (generalization error). These data should not be used during model training! Figure 6.3: Splitting data into training and test sets. Given a fixed amount of data, typical recommendations for splitting your data into training-testing splits include 60% (training) - 40% (testing), 70%-30%, or 80%-20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep in mind that as your overall data set gets smaller, spending too much in training (\\(&gt;80\\%\\)) won‚Äôt allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting), sometimes too much spent in testing (\\(&gt;40\\%\\)) won‚Äôt allow us to get a good assessment of model parameters In today‚Äôs data-rich environment, typically, we are not lacking in the quantity of observations, so a 70-30 split is often sufficient. The two most common ways of splitting data include simple random sampling and stratified sampling. 6.4.2.2 Simple random sampling The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the percentage of data represented in your response variable (\\(y\\)). There are multiple ways to split our data. Here we show four options to produce a 70-30 split (note that setting the seed value allows you to reproduce your randomized splits): Sampling is a random process so setting the random number generator with a common seed allows for reproducible results. Throughout this book we will use the number 123 often for reproducibility but the number itself has no special meaning. # base R set.seed(123) index_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7)) train_1 &lt;- ames[index_1, ] test_1 &lt;- ames[-index_1, ] # caret package set.seed(123) index_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) train_2 &lt;- ames[index_2, ] test_2 &lt;- ames[-index_2, ] # rsample package set.seed(123) split_1 &lt;- initial_split(ames, prop = 0.7) train_3 &lt;- training(split_1) test_3 &lt;- testing(split_1) Since this sampling approach will randomly sample across the distribution of \\(y\\) (Sale_Price in our example), you will typically result in a similar distribution between your training and test sets as illustrated below. Figure 6.4: Distribution comparison between the training (black) test (red) sets. 6.4.2.3 Stratified sampling However, if we want to explicitly control our sampling so that our training and test sets have similar \\(y\\) distributions, we can use stratified sampling. This is more common with classification problems where the reponse variable may be imbalanced (90% of observations with response ‚ÄúYes‚Äù and 10% with response ‚ÄúNo‚Äù). However, we can also apply to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality. With a continuous response variable, stratified sampling will break \\(y\\) down into quantiles and randomly sample from each quantile. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets. The easiest way to perform stratified sampling on a response variable is to use the rsample package, where you specify the response variable to stratafy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes: 16%). By enforcing stratified sampling both our training and testing sets have approximately equal response distributions. # orginal response distribution table(churn$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8388 0.1612 # stratified sampling with the rsample package set.seed(123) split_strat &lt;- initial_split(churn, prop = 0.7, strata = &quot;Attrition&quot;) train_strat &lt;- training(split_strat) test_strat &lt;- testing(split_strat) # consistent response ratio between train &amp; test table(train_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8388 0.1612 table(test_strat$Attrition) %&gt;% prop.table() ## ## No Yes ## 0.8386 0.1614 6.4.3 Feature engineering Feature engineering generally refers to the process of adding, deleting, and transforming the variables to be applied to your predictive modeling algorithms. Feature engineering is a significant process and requires you to spend substantial time understanding your data‚Ä¶or as Leo Breiman said ‚Äúlive with your data before you plunge into modeling‚Äù (Breiman and others 2001). Although this section primarily focuses on applying predictive modeling algorithms, feature engineering can make or break an algorithm‚Äôs predictive ability. We will not cover all the potential ways of implementing feature engineering; however, we will cover a few fundamental pre-processing tasks that can significantly improve modeling performance. To learn more about feature engineering check out Feature Engineering for Machine Learning by Zheng and Casari (2018) and Max Kuhn‚Äôs upcoming book Feature Engineering and Selection: A Practical Approach for Predictive Models. 6.4.3.1 Response Transformation Although not a requirement, normalizing the distribution of the response variable by using a transformation can lead to a big improvement, especially for parametric models. As we saw in Figure 6.4, our response variable Sale_Price is right skewed. To normalize, we have a few options: Option 1: normalize with a log transformation as discussed in 2.3.1. This will transform most right skewed distributions to be approximately normal. # log transformation train_log_y &lt;- log(train_1$Sale_Price) test_log_y &lt;- log(test_1$Sale_Price) If your reponse has negative values then a log transformation will produce NaNs. If these negative values are small (between -0.99 and 0) then you can apply log1p, which adds 1 to the value prior to applying a log transformation. If your data consists of negative equal to or less than -1, use the Yeo Johnson transformation mentioned next. log(-.5) ## [1] NaN log1p(-.5) ## [1] -0.6931 Option 2: use a Box Cox transformation. A Box Cox transformation is more flexible than a log transformation and will find the transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution. At the core of the Box Cox transformation is an exponent, lambda (\\(\\lambda\\)), which varies from -5 to 5. All values of \\(\\lambda\\) are considered and the optimal value for the given data is selected; The ‚Äúoptimal value‚Äù is the one which results in the best approximation of a normal distribution curve. The transformation of Y has the form: \\[ \\begin{equation} y(\\lambda) = \\begin{cases} \\frac{y^\\lambda-1}{\\lambda}, &amp; \\text{if}\\ \\lambda \\neq 0 \\\\ \\log y, &amp; \\text{if}\\ \\lambda = 0. \\end{cases} \\end{equation} \\] Be sure to compute the lambda on the training set and apply that same lambda to both the training and test set to minimize data leakage. # Box Cox transformation lambda &lt;- forecast::BoxCox.lambda(train_1$Sale_Price) train_bc_y &lt;- forecast::BoxCox(train_1$Sale_Price, lambda) test_bc_y &lt;- forecast::BoxCox(test_1$Sale_Price, lambda) We can see that in this example, the log transformation and Box Cox transformation both do about equally well in transforming our reponse variable to be normally distributed. Figure 6.5: Response variable transformations. Note that when you model with a transformed response variable, your predictions will also be in the transformed value. You will likely want to re-transform your predicted values back to their normal state so that decision-makers can interpret the results. The following code can do this for you: # log transform a value y &lt;- log(10) # re-transforming the log-transformed value exp(y) ## [1] 10 # Box Cox transform a value y &lt;- forecast::BoxCox(10, lambda) # Inverse Box Cox function inv_box_cox &lt;- function(x, lambda) { if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda) } # re-transforming the Box Cox-transformed value inv_box_cox(y, lambda) ## [1] 10 ## attr(,&quot;lambda&quot;) ## [1] -0.3068 If your response has negative values, you can use the Yeo-Johnson transformation. To apply, use car::powerTransform to identify the lambda, car::yjPower to apply the transformation, and VGAM::yeo.johnson to apply the transformation and/or the inverse transformation. 6.4.3.2 Predictor Transformation 6.4.3.2.1 One-hot encoding Many models require all predictor variables to be numeric. Consequently, we need to transform any categorical variables into numeric representations so that these algorithms can compute. Some packages automate this process (i.e. h2o, glm, caret) while others do not (i.e. glmnet, keras). Furthermore, there are many ways to encode categorical variables as numeric representations (i.e. one-hot, ordinal, binary, sum, Helmert). The most common is referred to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding variable x in the following: id x 1 a 2 c 3 b 4 c 5 c 6 a 7 b 8 c results in the following representation: id x.a x.b x.c 1 1 0 0 2 0 0 1 3 0 1 0 4 0 0 1 5 0 0 1 6 1 0 0 7 0 1 0 8 0 0 1 This is called less than full rank encoding where we retain all variables for each level of x. However, this creates perfect collinearity which causes problems with some predictive modeling algorithms (i.e. generalized regression models, neural networks). Alternatively, we can create full-rank one-hot encoding by dropping one of the levels (level a has been dropped): id x.b x.c 1 0 0 2 0 1 3 1 0 4 0 1 5 0 1 6 0 0 7 1 0 8 0 1 If you needed to manually implement one-hot encoding yourself you can with caret::dummyVars. Sometimes you may have a feature level with very few observations and all these observations show up in the test set but not the training set. The benefit of using dummyVars on the full data set and then applying the result to both the train and test data sets is that it will guarantee that the same features are represented in both the train and test data. # full rank one-hot encode - recommended for generalized linear models and # neural networks full_rank &lt;- dummyVars( ~ ., data = ames, fullRank = TRUE) train_oh &lt;- predict(full_rank, train_1) test_oh &lt;- predict(full_rank, test_1) # less than full rank --&gt; dummy encoding dummy &lt;- dummyVars( ~ ., data = ames, fullRank = FALSE) train_oh &lt;- predict(dummy, train_1) test_oh &lt;- predict(dummy, test_1) Since one-hot encoding adds new features it can significantly increase the dimensionality of our data. If you have a data set with many categorical variables and those categorical variables in turn have many unique levels, the number of features can explode. In these cases you may want to explore ordinal encoding of your data. 6.4.3.3 Standardizing Some models (i.e. generalized linear models, regularized models, neural networks) require that the predictor variables have the same units. Centering and scaling can be used for this purpose and is often referred to as standardizing the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Some packages have built-in arguments (i.e. glmnet, caret) to standardize and some do not (i.e. glm, keras). If you need to manually standardize your variables you can use the preProcess function provided by the caret package. For example, here we center and scale our Ames predictor variables. It is important that you standardize the test data based on the training mean and variance values of each feature. This minimizes data leakage. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;center&quot;, &quot;scale&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 6.4.3.4 Alternative Feature Transformation There are some alternative transformations that you can perform: Normalizing the predictor variables with a Box Cox transformation can improve parametric model performance. Collapsing highly correlated variables with PCA can reduce the number of features and increase the stability of generalize linear models. However, this reduces the amount of information at your disposal and we show you how to use regularization as a better alternative to PCA. Removing near-zero or zero variance variables. Variables with vary little variance tend to not improve model performance and can be removed. preProcess provides many other transformation options which you can read more about here. For example, the following normalizes predictors with a Box Cox transformation, center and scales continuous variables, performs principal component analysis to reduce the predictor dimensions, and removes predictors with near zero variance. # identify only the predictor variables features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # pre-process estimation based on training features pre_process &lt;- preProcess( x = train_1[, features], method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;, &quot;nzv&quot;) ) # apply to both training &amp; test train_x &lt;- predict(pre_process, train_1[, features]) test_x &lt;- predict(pre_process, test_1[, features]) 6.4.4 Basic model formulation There are many packages to perform predictive modeling and there are almost always more than one to perform each algorithm (i.e. there are over 20 packages to perform random forests). There are pros and cons to each package; some may be more computationally efficient while others may have more hyperparameter tuning options. Future chapters will expose you to many of the packages and algorithms that perform and scale best to most organization‚Äôs problems and data sets. Just realize there are more ways than one to skin a üôÄ. For example, these three functions will all produce the same linear regression model output. lm.lm &lt;- lm(Sale_Price ~ ., data = train_1) lm.glm &lt;- glm(Sale_Price ~ ., data = train_1, family = gaussian) lm.caret &lt;- train(Sale_Price ~ ., data = train_1, method = &quot;lm&quot;) One thing you will notice throughout this section is that we can specify our model formulation in different ways. In the above examples we use the model formulation (Sale_Price ~ . which says explain Sale_Price based on all features) approach. An alternative approach you will see throughout this section is the matrix formulation approach. Matrix formulation requires that we separate our response variable from our features. For example, in the regularization chaper we‚Äôll use glmnet which requires our features (x) and response (y) variable to be specified separately: # get feature names features &lt;- setdiff(names(train_1), &quot;Sale_Price&quot;) # create feature and response set train_x &lt;- train_1[, features] train_y &lt;- train_1$Sale_Price # example of matrix formulation glmnet.m1 &lt;- glmnet(x = train_x, y = train_y) 6.4.5 Model tuning Hyperparameters control the level of model complexity. Some algorithms have many tuning parameters while others have only one or two. Tuning can be a good thing as it allows us to transform our model to better align with patterns within our data. For example, Figure 6.6 shows how the more flexible model aligns more closely to the data than the fixed linear model. Figure 6.6: Tuning allows for more flexible patterns to be fit. However, highly tunable models can also be dangerous because they allow us to overfit our model to the training data, which will not generalize well to future unseen data. Figure 6.7: Highly tunable models can overfit if we are not careful. Throughout this section we will demonstrate how to tune the different parameters for each model. One way to perform hyperparameter tuning is to fiddle with hyperparameters manually until you find a great combination of hyperparameter values that result in high predictive accuracy. However, this would be very tedious work. An alternative approach is to perform a grid search. A grid search is an automated approach to searching across many combinations of hyperparameter values. Throughout this guide you will be exposed to different approaches to performing grid searches. 6.4.6 Cross Validation for Generalization Our goal is to not only find a model that performs well on training data but to find one that performs well on future unseen data. So although we can tune our model to reduce some error metric to near zero on our training data, this may not generalize well to future unseen data. Consequently, our goal is to find a model and its hyperparameters that will minimize error on held-out data. Let‚Äôs go back to this image‚Ä¶ Figure 6.8: Bias versus variance. The model on the left is considered rigid and consistent. If we provided it a new training sample with slightly different values, the model would not change much, if at all. Although it is consistent, the model does not accurately capture the underlying relationship. This is considered a model with high bias. The model on the right is far more inconsistent. Even with small changes to our training sample, this model would likely change significantly. This is considered a model with high variance. The model in the middle balances the two and, likely, will minimize the error on future unseen data compared to the high bias and high variance models. This is our goal. TODO: Create our own illustration Figure 6.9: Bias-variance tradeoff. To find the model that balances the bias-variance tradeoff, we search for a model that minimizes a k-fold cross-validation error metric. Figure 6.10 illustrates k-fold cross-validation, which is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. The model is fit on \\(k-1\\) folds and then the held-out validation fold is used to compute the error. This procedure is repeated k times; each time, a different group of observations is treated as the validation set. This process results in k estimates of the test error (\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_k\\)). Thus, the k-fold CV estimate is computed by averaging these values, which provides us with an approximation of the error to expect on unseen data. Figure 6.10: Illustration of the k-fold cross validation process. Many of the algorithms we cover in this guide have built-in cross validation capabilities. One typically uses a 5 or 10 fold CV (\\(k = 5\\) or \\(k = 10\\)). For example, glmnet implements CV with the nfolds argument: # example of 10 fold CV in h2o example.cv &lt;- cv.glmnet( x = train_x, y = train_y, nfolds = 10 ) 6.4.7 Model evaluation This leads us to our next topic, evaluating performance. Historically, the performance of a predictive model was largely based on goodness-of-fit tests and assessment of residuals. Unfortunately, misleading conclusions may follow from predictive models that pass these kind of assessments (Breiman and others 2001). Today, it has become widely accepted that a more sound approach to assessing model performance is to assess the predictive accuracy via loss functions. Loss functions are metrics that compare the predicted values to the actual value (often referred to as the error or residual). There are many loss functions to choose when assessing the performance of a predictive model; each providing a unique understanding of the predictive accuracy and differing between regression and classification models. The most common include: 6.4.7.1 Regression models MSE: Mean squared error is the average of the squared error (\\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\)). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. Objective: minimize RMSE: Root mean squared error. This simply takes the square root of the MSE metric (\\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\)) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize Deviance: Short for mean residual deviance. In essence, it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). If the response variable distribution is gaussian, then it is equal to MSE. When not, it usually gives a more useful estimate of error. Objective: minimize MAE: Mean absolute error. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values (\\(MAE = \\frac{1}{n} \\sum^n_{i=1}(\\vert y_i - \\hat y_i \\vert)\\)). Objective: minimize RMSLE: Root mean squared logarithmic error. Similiar to RMSE but it performs a log() on the actual and predicted values prior to computing the difference (\\(RMSLE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(log(y_i + 1) - log(\\hat y_i + 1))^2}\\)). When your response variable has a wide range of values, large repsonse values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. Objective: minimize \\(R^2\\): This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower \\(R^2\\) than the other. You should not place too much emphasis on this metric. Objective: maximize Most models we assess in this guide will report most, if not all, of these metrics. We will emphasize MSE and RMSE but its good to realize that certain situations warrant emphasis on some more than others. 6.4.7.2 Classification models Misclassification: This is the overall error. For example, say you are predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class high, 6 of class medium, and 4 of class low, then you misclassified 13 out of 90 observations resulting in a 14% misclassification rate. Objective: minimize Mean per class error: This is the average error rate for each class. For the above example, this would be the mean of \\(\\frac{3}{25}, \\frac{6}{30}, \\frac{4}{35}\\), which is 12%. If your classes are balanced this will be identical to misclassification. Objective: minimize MSE: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probabilty of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the \\(MSE = 0.09^2 = 0.0081\\), if it is B \\(MSE = 0.93^2 = 0.8649\\), if it is C \\(MSE = 0.98^2 = 0.9604\\). The squared component results in large differences in probabilities for the true class having larger penalties. Objective: minimize Cross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. Objective: minimize Gini index: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. Objective: minimize When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does). Figure 6.11: Confusion matrix. We can extract different levels of performance from these measures. For example, given the classification matrix below we can assess the following: Accuracy: Overall, how often is the classifier correct? Opposite of misclassification above. Example: \\(\\frac{TP + TN}{total} = \\frac{100+50}{165} = 0.91\\). Objective: maximize Precision: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: \\(\\frac{TP}{TP + FP} = \\frac{100}{100+10} = 0.91\\). Objective: maximize Sensitivity (aka recall): How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: \\(\\frac{TP}{TP + FN} = \\frac{100}{100+5} = 0.95\\). Objective: maximize Specificity: How accurately does the classifier classify actual non-events? Example: \\(\\frac{TN}{TN + FP} = \\frac{50}{50+10} = 0.83\\). Objective: maximize Figure 6.12: Example confusion matrix. AUC: Area under the curve. A good classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. Objective: maximize Figure 6.13: ROC curve. 6.4.8 Interpreting predictive models In his seminal 2001 paper (Breiman and others 2001), Leo Breiman popularized the phrase: ‚Äúthe multiplicity of good models.‚Äù The phrase means that for the same set of input variables and prediction targets, complex predictive modeling algorithms can produce multiple accurate models with very similar, but not the exact same, internal architectures. Figure 6.14 is a depiction of a non-convex error surface that is representative of the error function for a predictive model with two inputs ‚Äî say, a customer‚Äôs income and a customer‚Äôs age, and an output, such as the same customer‚Äôs probability of redeeming a coupon. This non-convex error surface with no obvious global minimum implies there are many different ways complex predictive models could learn to weigh a customer‚Äôs income and age to make a good decision about if they are likely to redeem a coupon. Each of these different weightings would create a different function for making coupon redemption (and therefore marketing) decisions, and each of these different functions would have different explanations. Figure 6.14: Non-convex error surface with many local minimas. All of this is an obstacle to analysts, as they can experience very similar predictions from different models based on the same feature set. However, these models will have very different logic and structure leading to different interpretations. Consequently, practitioners should understand how to interpret different types of models. Throughout this section we will provide you with the a variety of ways to interpret your predictive models so that you understand what is driving model and prediction performance. This will allow you to be more effective and efficient in applying and understanding mutliple good models. References "],
["linear-regression.html", "Chapter 7 Linear regression 7.1 Prerequisites 7.2 Simple linear regression 7.3 Multiple linear regression 7.4 Assessing Model Accuracy 7.5 Model concerns 7.6 Principal component regression 7.7 Partial least squares 7.8 Feature Interpretation 7.9 Final thoughts 7.10 Learning more", " Chapter 7 Linear regression Linear regression is a very simple approach for supervised learning, has been around for a long time, and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches: as we will see in later chapters, many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated. This chapter introduces linear regression with an emphasis on predictive purposes rather than inferential purposes (see Faraway (2016b) for discussion of linear regression in R with an inferential emphasis). 7.1 Prerequisites For this section we will use the following packages: library(tidyverse) # data manipulation &amp; visualization library(rsample) # data splitting library(caret) # regression modeling library(modelr) # provides easy pipeline modeling functions library(broom) # helps to tidy up model outputs library(vip) # variable importance To illustrate linear regression concepts we will use the Ames, IA housing data, where our intent is to predict Sale_Price. As discussed in the Data splitting section 6.4.2, we‚Äôll set aside 30% of our data as a test set to assess our generalizability error. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) train &lt;- training(ames_split) test &lt;- testing(ames_split) 7.2 Simple linear regression Simple linear regression lives up to its name: it is a very straightforward approach for predicting a continuous quantitative response \\(Y\\) on the basis of a single predictor variable \\(X\\). It assumes that there is approximately a linear relationship between \\(X\\) and \\(Y\\). Consider our housing data, suppose we wish to model the linear relationship between the year the house was built (Year_Built) and sale price (Sale_Price). We can write this as Equation (7.1) and we often state this as regressing Y onto X. \\[\\begin{equation} \\tag{7.1} Y = \\beta_0 + \\beta_1X + \\epsilon, \\end{equation}\\] where \\(Y\\) represents Sale_Price, \\(X\\) represents Year_Built, \\(\\beta_0\\) and \\(\\beta_1\\) represent two unknown constants (commonly referred to as coefficients or parameters) that represent the intercept and slope terms in the linear model, and \\(\\epsilon\\) is a mean-zero random error term. To estimate the coefficient parameters (\\(\\beta_i\\)), the linear regression algorithm will identify the best-fit linear relationship that fits the data well. There are multiple ways to measure ‚Äúbest-fit‚Äù but the most common involves minimizing the least squares criterion also referred to as ordinary least squares (OLS). The OLS criterion identifies the ‚Äúbest-fit‚Äù line by minimizing the error between the best-fit line (the predicted sales price) and the actual sale price values. To perform an OLS regression model in R we can use lm. # fit model model1 &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = train) Figure 7.1 illustrates this linear model with the best fit linear line. The individual dots represent the actual sales price, and the vertical grey lines represent the individual errors for each observation. The OLS criterion identifies the best-fit line that minimizes the residual sum of squared errors (RSS), which follows Equation (7.2) \\[\\begin{equation} \\tag{7.2} RSS = \\sum^n_{i=1}(y_i - f(x_i))^2, \\end{equation}\\] where \\(y_i\\) is the i\\(^{th}\\) value of the explanatory variable, and \\(f(x_i)\\) is the predicted value of \\(y_i\\) (also termed (\\(\\hat y_i\\))). The left plot of 7.1 illustrates this model‚Äôs fit across homes that have between 1000-2000 square feet of living above ground whereas the right plot shows the model‚Äôs fit across the entire training set. Figure 7.1: The least squares fit for regressing sale price onto above ground square footage for the the Ames housing data. The least squares criterion finds the ‚Äòbest-fit‚Äô line that minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (blue line). Left: model fit for homes with 1000-2000 square feet. Right: model fit across all observations. To identify the coefficients that minimize the RSS, OLS selects the optimal estimated coefficients (\\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\)). Its been proven that the coefficients that minimize the RSS are: \\[\\begin{align} \\tag{7.3} \\hat \\beta_1 = \\frac{\\sum^n_{i=1}(x_i - \\bar x)(y_i - \\bar y)}{\\sum^n_{i=1}(x_i - \\bar x)^2}, \\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x \\end{align}\\] where \\(\\bar y = \\frac{1}{n}\\sum^n_{i=1}y_i\\) and \\(\\bar x = \\frac{1}{n}\\sum^n_{i=1}x_i\\) are the sample means. summary allows us to access the coeffients for our model along with other model results. For this simple model, our coefficients are \\(\\hat \\beta_0 = 17797.0728\\) and \\(\\hat \\beta_1 = 108.0344\\). In other words, according to this approximation, an additional one square foot of above ground living space of a house is associated with approximately an additional $108 in selling price. This ease in interpreting the relationship between the sale price and square footage with a single number is what makes linear regression such a intuitive and popular modeling tool. summary(model1) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -467327 -30799 -1432 22339 338467 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17797.07 3916.11 4.54 5.8e-06 *** ## Gr_Liv_Area 108.03 2.47 43.74 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56800 on 2052 degrees of freedom ## Multiple R-squared: 0.483, Adjusted R-squared: 0.482 ## F-statistic: 1.91e+03 on 1 and 2052 DF, p-value: &lt;2e-16 It‚Äôs also important to understand if these coefficients are statistically significant. In other words, can we state these coefficients are statistically different then 0? To do that we can start by assessing the standard error (SE). The SE for \\(\\beta_0\\) and \\(\\beta_1\\) are computed with: \\[\\begin{align} \\tag{7.4} SE(\\beta_0)^2 = \\sigma^2\\bigg[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\bigg], \\quad SE(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\end{align}\\] where \\(\\sigma^2 = Var(\\epsilon)\\). We see that our model results provide the SE (\\(SE_{\\beta_1}=2.47\\)). We can use the SE to compute the 95% confidence interval for the coefficients: \\[\\begin{equation} \\tag{7.5} \\beta_1 \\pm 2 \\cdot SE(\\beta_1) \\end{equation}\\] To get this information in R we can simply use: confint(model1) ## 2.5 % 97.5 % ## (Intercept) 10117.1 25477.0 ## Gr_Liv_Area 103.2 112.9 Our results show us that our 95% confidence interval for \\(\\beta_1\\) (Gr_Liv_Area) is [103.191, 112.8779]. Thus, since zero is not in this interval we can conclude that as the Gr_Liv_Area increases by one square foot we can expect Sale_Price to increase by approximately $103‚Äì$113. This is also supported by the t-statistic provided by our results, which are computed by \\[\\begin{equation} \\tag{7.6} t=\\frac{\\beta_1 - 0}{SE(\\beta_1)} \\end{equation}\\] which measures the number of standard deviations that \\(\\beta_1\\) is away from 0. Thus a large t-statistic such as ours will produce a small p-value (a small p-value indicates that it is unlikely to observe such a substantial association between the predictor variable and the response due to chance). Thus, we can conclude that a relationship between Gr_Liv_Area and Sale_Price exists. 7.3 Multiple linear regression However, in practice we often have more than one predictor. For example, in the Ames housing data, we may wish to understand if above ground square footage (Gr_Liv_Area) and the year the house was built (Year_Built) are related to sales price (Sale_Price). We can extend the simple linear regression model so that it can directly accommodate multiple predictors; this is referred to as multiple linear regression and is represented by Equation (7.7) and illustrated in Figure 7.2. \\[\\begin{equation} \\tag{7.7} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon, \\end{equation}\\] where now \\(X_1\\) represents Gr_Liv_Area and \\(X_2\\) represents Year_Built. Figure 7.2: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The ‚Äòbest-fit‚Äô plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane). This model in R is built by simply adding the Year_Built variable to our original model. The below results illustrate that the best-fit plane identified in Figure 7.2 resulted in \\(\\hat \\beta_1 = 91.73\\) and \\(\\hat \\beta_2 = 1098\\). In other words, according to this approximation, an additional one square foot of above ground square footage is now associated with approximately an additional $92 in selling price when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of $1,098 in selling price when holding the main floor square footage constant. # fit model model2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) summary(model2) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -440347 -26137 -2859 18098 310897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.12e+06 6.91e+04 -30.7 &lt;2e-16 *** ## Gr_Liv_Area 9.17e+01 2.11e+00 43.6 &lt;2e-16 *** ## Year_Built 1.10e+03 3.54e+01 31.0 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 46900 on 2051 degrees of freedom ## Multiple R-squared: 0.648, Adjusted R-squared: 0.647 ## F-statistic: 1.88e+03 on 2 and 2051 DF, p-value: &lt;2e-16 We can continue to add predictors to our multiple regression and generalize the multiple regression model to Equation (7.8) where we have p distinct predictors. \\[\\begin{equation} \\tag{7.8} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon, \\end{equation}\\] Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. The below creates a third model where we use all features in our data set to predict Sale_Price. The dot in Sale_Price ~ . signals to regress Sale_Price onto all other variables in your data set. model3 &lt;- lm(Sale_Price ~ ., data = train) glance(model3) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.941 0.931 20719. 96.9 0 ## # ... with 6 more variables: df &lt;int&gt;, logLik &lt;dbl&gt;, ## # AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt; 7.4 Assessing Model Accuracy We‚Äôve fit three models, but the question remains, which model is ‚Äúbest‚Äù. To get a good assessment of this accuracy, we want to use cross-validation as discussed in Section 6.4.6. We can use the caret::train function to apply a linear model (method = &quot;lm&quot;). The benefit of caret is that it provides built-in cross validation capabilities whereas the lm function does not. The following shows an average root mean square error (RMSE) of 56873 across our 10 cross validation folds. How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $56,873 off from the actual sale price. # reproducible CV results set.seed(123) # use caret package to train 10-fold cross-validated model cv_model1 &lt;- train( Sale_Price ~ Gr_Liv_Area, data = train, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) cv_model1 ## Linear Regression ## ## 2054 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 1849, 1849, 1849, 1848, 1849, 1849, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 56873 0.4884 38678 ## ## Tuning parameter &#39;intercept&#39; was held constant at ## a value of TRUE We can perform cross validation on the other two models. Extracting the results for each model we see that by adding more information via more predictors, we are able to improve the out-of-sample cross validation performance metrics. Specifically, our average prediction RMSE reduces from $56,872 down to $41,438 for our full model. set.seed(123) cv_model2 &lt;- train( Sale_Price ~ Gr_Liv_Area + Year_Built, data = train, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) set.seed(123) cv_model3 &lt;- train( Sale_Price ~ ., data = train, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # extract out of sample performance measures summary(resamples(list( model1 = cv_model1, model2 = cv_model2, model3 = cv_model3 ))) ## ## Call: ## summary.resamples(object = resamples(list(model1 ## = cv_model1, model2 = cv_model2, model3 = cv_model3))) ## ## Models: model1, model2, model3 ## Number of resamples: 10 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## model1 35890 36624 38445 38678 40566 41819 0 ## model2 29243 30263 31202 31401 31895 34505 0 ## model3 14944 15852 17626 17701 18883 21980 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## model1 49534 53294 56878 56873 58485 66579 0 ## model2 39812 43132 46900 46886 49341 55657 0 ## model3 21305 24403 46475 41438 53958 63247 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## model1 0.3126 0.3966 0.5180 0.4884 0.5607 0.6351 ## model2 0.5060 0.5996 0.6664 0.6492 0.7090 0.7416 ## model3 0.5456 0.6429 0.7124 0.7524 0.9058 0.9250 ## NA&#39;s ## model1 0 ## model2 0 ## model3 0 7.5 Model concerns As previously stated, linear regression has been a popular modeling tool due to the ease of interpreting the coefficients. However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model. Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results. 1. Linear relationship: Linear regression assumes a linear relationship between the predictor and the response variable. When a linear relationship does not hold then the coefficient estimate makes a flawed assumption that a constant relationship holds. However, as discussed in Chapter 1.1, non-linear relationships can be made linear (or near-linear) by applying power transformations to the response and/or predictor. For example, Figure 7.3 illustrates the relationship between sale price and the year a home was built. The left plot illustrates the non-linear relationship that exists. However, we can achieve a near-linear relationship by log transforming sale price; although some non-linearity still exists for older homes. p1 &lt;- ggplot(train, aes(Year_Built, Sale_Price)) + geom_point(size = 1, alpha = .4) + geom_smooth(se = FALSE) + scale_y_continuous(&quot;Sale price&quot;, labels = scales::dollar) + xlab(&quot;Year built&quot;) + ggtitle(&quot;Non-transformed variables with a \\nnon-linear relationship.&quot;) p2 &lt;- ggplot(train, aes(Year_Built, Sale_Price)) + geom_point(size = 1, alpha = .4) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_y_log10(&quot;Sale price&quot;, labels = scales::dollar, breaks = seq(0, 400000, by = 100000)) + xlab(&quot;Year built&quot;) + ggtitle(&quot;Transforming variables can provide a \\nnear-linear relationship.&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 7.3: Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s). 2. Constant variance among residuals: Linear regression assumes the variance among error terms (\\(\\epsilon_1, \\epsilon_2, ..., \\epsilon_p\\)) are constant (also referred to as homoscedasticity). When residuals are not constant, the p-values and confidence intervals of the coefficients are invalid resulting in invalid prediction estimates and confidence intervals. Similar to the linear relationships assumption, non-constant variance can often be resolved with variable transformations or by including additional predictors. For example, Figure 7.4 shows residuals across predicted values for our linear regression models model1 and model3. model1 displays a classic violation of constant variance with cone-shaped residuals. However, model3 appears to have near-constant variance. The broom::augment function is an easy way to add model results to each observation (i.e. predicted values, residuals). df1 &lt;- augment(cv_model1$finalModel, train) p1 &lt;- ggplot(df1, aes(.fitted, .resid)) + geom_point(size = 1, alpha = .4) + xlab(&quot;Predicted values&quot;) + ylab(&quot;Residuals&quot;) + ggtitle(&quot;Model 1&quot;, subtitle = &quot;Sale_Price ~ Gr_Liv_Area&quot;) df2 &lt;- augment(cv_model3$finalModel, train) p2 &lt;- ggplot(df2, aes(.fitted, .resid)) + geom_point(size = 1, alpha = .4) + xlab(&quot;Predicted values&quot;) + ylab(&quot;Residuals&quot;) + ggtitle(&quot;Model 3&quot;, subtitle = &quot;Sale_Price ~ .&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 7.4: Linear regression assumes constant variance among the residuals. model1 (left) shows definitive signs of heteroskedasticity whereas model3 (right) appears to have constant variance. 3. No autocorrelation: Linear regression assumes the error terms are also independent and uncorrelated. If in fact, there is correlation among the error terms, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be. For example, the left plot in Figure 7.5 displays the residuals (y-axis) to the observation ID (x-axis) for model1. A clear pattern exists suggesting that information about \\(\\epsilon_1\\) provides information about \\(\\epsilon_2\\). This pattern is a result of the data being ordered by neighborhood, which we have not accounted for in this model. Consequently, the residuals for homes in the same neighborhood are correlated (homes within a neighborhood are typically the same size and can often contain similar features). Since the Neighborhood predictor is included in model3 (right plot), our errors are no longer correlated. df1 &lt;- mutate(df1, id = row_number()) df2 &lt;- mutate(df2, id = row_number()) p1 &lt;- ggplot(df1, aes(id, .resid)) + geom_point(size = 1, alpha = .4) + xlab(&quot;Row ID&quot;) + ylab(&quot;Residuals&quot;) + ggtitle(&quot;Model 1&quot;, subtitle = &quot;Correlated residuals.&quot;) p2 &lt;- ggplot(df2, aes(id, .resid)) + geom_point(size = 1, alpha = .4) + xlab(&quot;Row ID&quot;) + ylab(&quot;Residuals&quot;) + ggtitle(&quot;Model 3&quot;, subtitle = &quot;Uncorrelated residuals.&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 7.5: Linear regression assumes uncorrelated errors. The residuals in model1 (left) have a distinct pattern suggesting that information about \\(\\epsilon_1\\) provides information about \\(\\epsilon_2\\). Whereas residuals in model3 have no signs of autocorrelation. 4. More observations than predictors: Although not an issue with the Ames housing data, when the number of features exceed the number of observations (\\(p &gt; n\\)), the OLS solution matrix is not invertible. This causes significant issues because it means the least-squares estimates are not unique. In fact, there are an infinite set of solutions available so we lose our ability to meaningfully interpret coefficients. Consequently, to resolve this issue an analyst can remove variables until \\(p &lt; n\\) and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn and Johnson 2013, 26:43‚Äì47), it can be cumbersome and prone to errors. Alternatively, we will introduce regularized regression in Chapter ?? which provides you an alternative linear regression technique when \\(p &gt; n\\). 5. No or little multicollinearity: Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This, obviously, leads to inaccurate interpretation of coefficients and identifying influential predictors. For example, in our data, Garage_Area and Garage_Cars are two variables that have a correlation of 0.89 and both variables are strongly correlated to our response variable (Sale_Price). Looking at our full model where both of these variables are included, we see that Garage_Area is found to be statistically significant but Garage_Cars is not. # fit with two strongly correlated variables summary(cv_model3) %&gt;% tidy() %&gt;% filter(term %in% c(&quot;Garage_Area&quot;, &quot;Garage_Cars&quot;)) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Garage_Cars 2131. 1761. 1.21 0.226 ## 2 Garage_Area 19.5 5.88 3.31 0.000939 However, if we refit the full model without Garage_Area, the coefficient estimate for Garage_Cars increases three fold and becomes statistically significant. # model without Garage_Area set.seed(123) mod_wo_Garage_Area &lt;- train( Sale_Price ~ ., data = select(train, -Garage_Area), method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) summary(mod_wo_Garage_Area) %&gt;% tidy() %&gt;% filter(term == &quot;Garage_Cars&quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Garage_Cars 6342. 1222. 5.19 0.000000236 This reflects the instability in the linear regression model caused by the between-predictor relationships and this instability gets propagated directly to the model predictions. Considering 16 of our 34 numeric predictors have medium to strong correlation (Section 5.2), the biased coefficients of these predictors are likely restricting the predictive accuracy of our model. How can we control for this problem? One option is to manually remove one of the offending predictors. However, when the number of predictors is large such as in our case, this becomes difficult. Moreover, relationships between predictors can become complex and involve many predictors. In these cases, manual removal of specific predictors may not be possible. Consequently, the following sections offers two simple extensions of linear regression where dimension reduction is applied prior to performing linear regression. Chapter ?? offers a modified regression approach that helps to deal with the problem. And future chapters provide alternative methods that are not effected by multicollinearity. 7.6 Principal component regression As discussed in Chapter 5, principal components analysis can be used to represent correlated variables in a lower dimension and the resulting components can be used as predictors in the linear regression model. This two-step process is known as principal component regression (PCR) (Massy 1965). Performing PCR with caret is an easy extension from our previous model. We simply change the method to ‚Äúpcr‚Äù within train to perform PCA on all our numeric predictors prior to applying the multiple regression. Often, we can greatly improve performance by only using a small subset of all principal components as predictors. Consequently, you can think of the number of principal components as a tuning parameter (see Section 6.4.5). The following performs cross validated PCR with \\(1, 2, \\dots, 20\\) principal components, and Figure 7.6 illustrates the cross-validated RMSE. You can see a significant drop in prediction error using just five principal components followed by a gradual decrease. Using 17 principal components provided the lowest RMSE of $35,769.99 (see cv_model_pcr for a comparison of the cross-validated results). Per Section (???)(pca), don‚Äôt forget to center and scale your predictors, which you can do by incorporating the preProcess argument. # perform 10-fold cross validation on a PCR model tuning the number of # principal components to use as predictors from 1-20 set.seed(123) cv_model_pcr &lt;- train( Sale_Price ~ ., data = train, method = &quot;pcr&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), preProcess = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), tuneLength = 20 ) # model with lowest RMSE cv_model_pcr$bestTune ## ncomp ## 17 17 # plot cross-validated RMSE plot(cv_model_pcr) Figure 7.6: The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components. By controlling for multicollinearity with PCR, we saw significant improvement in our predictive accuary (reducing out-of-sample RMSE from 41438 down to 35770). However, since PCR is a two step process, the PCA step does not consider any aspects of the response when it selects the components. Consequently, the new predictors produced by the PCA step are not designed to maximize the relationship with the response. Instead, it simply seeks to reduce the variability present throughout the predictor space. If that variability happens to be related to the response variability, then PCR has a good chance to identify a predictive relationship, as in our case. If, however, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exists (i.e. we may actually experience a decrease in our predictive accuracy). Thus, an alternative approach to reduce the impact of multicollinearity is partial least squares. 7.7 Partial least squares Partial least squares (PLS) can be viewed as a supervised dimension reduction procedure (Kuhn and Johnson 2013). Similar to PCR this technique also constructs a set of linear combinations of the inputs for regression, but unlike PCR it uses the response variable to aid the construction of the principal components. Thus, we can think of PLS as a supervised dimension reduction procedure that finds new features that not only appromxate the old features well, but also that are related to the response. TODO: IMAGE of PCR vs PLS Referring back to Equation (5.1), PLS will compute the first principal (\\(z_1\\)) by setting each \\(\\phi_{j1}\\) to the coefficient from a simple linear regression model of \\(y\\) onto that respective \\(x_j\\). One can show that this coefficient is proportional to the correlation between \\(y\\) and \\(x_j\\). Hence, in computing \\(z_1 = \\sum^p_{j=1} \\phi_{j1}x_j\\), PLS places the highest weight on the variables that are most strongly related to the response. To compute the second principal (\\(z_2\\)), we first regress each variable on \\(z_1\\). The residuals from this regression captures the remaining signal that has not been explained by the first principal. We substitute these residual values for the predictor values in Equation (5.2). This process continues until all \\(m\\) components have been computed and then we use OLS to regress the response on \\(z_1, \\dots, z_m\\). See Friedman, Hastie, and Tibshirani (2001) and Geladi and Kowalski (1986) for a thorough discussion of PLS. Similar to PCR, we can easily fit a PLS model by changing the method argument in caret::train. As with PCR, the number of principal components to use is a tuning parameter that is determined by the model that maximize predictive accuracy (minimizes RMSE in this case). The following performs cross validated PLS with \\(1, 2, \\dots, 20\\) principal components, and Figure 7.7 illustrates the cross-validated RMSE. You can see a greater drop in prediction error than PCR. Using PLS with \\(m = 10\\) principal components provided the lowest RMSE of $31,522.47. # perform 10-fold cross validation on a PLS model tuning the number of # principal components to use as predictors from 1-20 set.seed(123) cv_model_pls &lt;- train( Sale_Price ~ ., data = train, method = &quot;pls&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), preProcess = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), tuneLength = 20 ) # model with lowest RMSE cv_model_pls$bestTune ## ncomp ## 10 10 # plot cross-validated RMSE plot(cv_model_pls) Figure 7.7: The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components. 7.8 Feature Interpretation Once we‚Äôve found the model that minimizes the predictive accuracy, our next goal is to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a monotonic linear relationship between the predictor variables and the response. The linear relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the chapter, this constant change is provided by the given coefficient for a predictor. The monotonic relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables? Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the t-statistic (Equation (7.6)) for each model parameter used. For a PLS model, variable importance is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the RSS across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the RSS. We can use vip::vip to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). Figure 7.8 illustrates that the top 4 most important variables are Gr_liv_Area, First_Flr_SF, Garage_Area, and Garage_Cars respectively. vip(cv_model_pls, num_features = 20) Figure 7.8: Top 20 most important variables for the PLS model. As stated earlier, linear regression models assume a monotonic linear relationship. To illustrate this, we can apply partial dependence plots (PDPs). PDPs plot the change in the average predicted value (\\(\\hat y\\)) as specified feature(s) vary over their marginal distribution. As you will see in later chapters, PDPs become more useful when non-linear relationships are present. However, PDPs of linear models help illustrate how a fixed change in \\(x_i\\) relates to a fixed linear change in \\(\\hat y_i\\). The pdp package (Greenwell 2017) provides convenient functions for computing and plotting PDPs. For example, the following code chunk would plot the PDP for the Gr_Liv_Area predictor. pdp::partial(cv_model_pls, pred.var = ‚ÄúGr_Liv_Area‚Äù, grid.resolution = 20) %&gt;% autoplot() All four of the most important predictors have a positive relationship with sale price; however, we see that the slope (\\(\\beta_i\\)) is steepest for the most important predictor and gradually decreases for lessor important variables. Figure 7.9: Partial dependence plots for the first four most important variables. 7.9 Final thoughts Linear regression is a great starting point in learning more advanced predictive analytic approaches because, in its simplest form, it is very intuitive and easy to interpret. Training a linear regression model is very easy and computationally efficient. However, due to the many assumptions required, the disadvantages of linear regression often outweigh their benefits. In our example, we saw how multicollinearity was interferring with predictive accuracy. By controlling multicollinearity with PCR and PLS we were able to improve predictive accuracy. Later chapters will build on the concepts illustrated in this chapter and will compare cross-validated performance results to identify the best predictive model. The following summarizes some of the advantages and disadvantages discussed regarding linear regression. FIXME: refine this section Advantages: Normal linear regression has no hyperparameters to tune and PCR and PLS have only one hyperparameter to tune; making these methods very simple to train. Computationally efficient - relatively fast compared to other algorithms in this book and does not require large memory. Easy to interpret results. Disadvantages: Makes strong asssumptions about the data. Does not handle missing data - must impute or remove observations with missing values. Not robust to outliers as they can still bias the coefficients. Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion). Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks). Most large data sets violate one of the several assumptions made for linear regression to hold, which cause instability in the modeling results. 7.10 Learning more This will get you up and running with linear regression. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more: An Introduction to Statistical Learning Applied Predictive Modeling Elements of Statistical Learning References "],
["logistic-regression.html", "Chapter 8 Logistic regression 8.1 Prerequisites 8.2 Why logistic regression 8.3 Simple logistic regression 8.4 Multiple logistic regression 8.5 Assessing model accuracy 8.6 Feature interpretation 8.7 Final thoughts 8.8 Learning more", " Chapter 8 Logistic regression Linear regression is used to approximate the relationship between a continuous response variable and a set of predictor variables. However, when the response variable is categorical rather than continuous, linear regression is not appropriate. Fortunately, analysts can turn to an analogous method, logistic regression, which is similar to linear regression in many ways. This chapter explores the use of logistic regression for binary response variables. Logistic regression can be expanded for multinomial problems (see Faraway (2016a) for discussion of multinomial logistic regression in R); however, that goes beyond our intent here. 8.1 Prerequisites For this section we will use the following packages: library(tidyverse) # data manipulation &amp; visualization library(rsample) # data splitting library(caret) # logistic regression modeling library(vip) # variable importance To illustrate logistic regression concepts we will use the employee attrition data, where our intent is to predict the Attrition response variable (‚ÄúYes‚Äù|‚Äúno‚Äù). As in the previous chapter, we‚Äôll set aside 30% of our data as a test set to assess our generalizability error. df &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) # Create training (70%) and test (30%) sets for the rsample::attrition data. # Use set.seed for reproducibility set.seed(123) churn_split &lt;- initial_split(df, prop = .7, strata = &quot;Attrition&quot;) train &lt;- training(churn_split) test &lt;- testing(churn_split) 8.2 Why logistic regression To provide a clear motivation of logistic regression, assume we have credit card default data for customers and we want to understand if the credit card balance the customer has is an indicator of whether or not the customer will default on their credit card. To classify a customer as a high- vs. low-risk defaulter based on their balance we could use linear regression; however, the left plot in Figure 8.1 illustrates how linear regression would predict the probability of defaulting. Unfortunately, for balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1. Contrast this with the logistic regression line (right plot) that is nonlinear (sigmoidal-shaped). Figure 8.1: Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1. To avoid the inadequecies of the linear model fit on a binary response, we must model the probability of our response using a function that gives outputs between 0 and 1 for all values of \\(X\\). Many functions meet this description. In logistic regression, we use the logistic function, which is defined in Equation (8.1) and produces the S-curve in the right plot above. \\[\\begin{equation} \\tag{8.1} p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\end{equation}\\] The \\(\\beta_i\\) parameters represent the coefficients as in linear regression and \\(p(x)\\) may be interpreted as the probability that the positive class (default in the above example) is present. The minimum for \\(p(x)\\) is obtained at \\(\\text{lim}_{a \\rightarrow -\\infty} \\big[ \\frac{e^a}{1+e^a} \\big] = 0\\), and the maximium for \\(p(x)\\) is obtained at \\(\\text{lim}_{a \\rightarrow \\infty} \\big[ \\frac{e^a}{1+e^a} \\big] = 1\\) which restricts the output probabilities to 0-1. Furthermore, a useful transformation for logistic regression is the logit transformation with follows: \\[\\begin{equation} \\tag{8.2} g(X) = \\text{ln} \\bigg[ \\frac{p(x)}{1 - p(x)} \\bigg] = \\beta_0 + \\beta_1x \\end{equation}\\] The logit transformation exhibits several attractive properties of the linear regression model such as its linearity and interpretability, which we will come back to shortly. 8.3 Simple logistic regression We will fit two logistic regression models in order to predict the probability of an employee attriting. The first predicts the probability of attrition based on their monthly income (MonthlyIncome) and the second is based on whether or not the employee works overtime (OverTime). The glm function fits generalized linear models, a class of models that includes logistic regression. The syntax of the glm function is similar to that of lm, except that we must pass the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model. model1 &lt;- glm(Attrition ~ MonthlyIncome, family = &quot;binomial&quot;, data = train) model2 &lt;- glm(Attrition ~ OverTime, family = &quot;binomial&quot;, data = train) In the background glm, uses maximum likelihood to fit the model. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted probability \\(\\hat p(x_i)\\) of attrition for each employee corresponds as closely as possible to the employee‚Äôs observed attrition status. In other words, we try to find \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) such that plugging these estimates into the model for \\(p(x)\\) (Equation (8.1)) yields a number close to one for all employees who attrited, and a number close to zero for all employees who did not. This intuition can be formalized using a mathematical equation called a likelihood function: \\[\\begin{equation} \\tag{8.3} \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}p(x_i) \\prod_{i&#39;:y_i&#39;=0}(1-p(x_i&#39;)) \\end{equation}\\] The estimates \\(\\beta_0\\) and \\(\\beta_1\\) are chosen to maximize this likelihood function. Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we will examine in future chapters. What results is the predicted probability of attrition. Figure 8.2 illustrates the predicted probablities for the two models. Figure 8.2: Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, model1 predicts a decreased probability of attrition and if employees work overtime model2 predicts an increased probability. The below table shows the coefficient estimates and related information that result from fitting a logistic regression model in order to predict the probability of Attrition = Yes for our two models. Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a log-odds scale. Thus, we see that the MonthlyIncome \\(\\hat \\beta_1 =\\) -1.144610^{-4}. This indicates that an increase in MonthlyIncome is associated with a decrease in the probability of attrition. To be precise, a one-unit increase in MonthlyIncome is associated with a decrease in the log odds of attrition by -1.144610^{-4} units. Similarly for model2, an employee that works OverTime has an increase of 1.3076 logg odds of attrition. tidy(model1) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.984 0.152 -6.47 9.62e-11 ## 2 MonthlyIncome -0.000114 0.0000244 -4.69 2.74e- 6 tidy(model2) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.14 0.120 -17.9 2.21e-71 ## 2 OverTimeYes 1.31 0.175 7.47 8.03e-14 Taking an exponential transformation of these coefficients converts them from log odds to odds. Furthermore, we can convert odds to a probability with \\(\\text{probability} = \\frac{odds}{1 + odds}\\) Thus, for every one dollar increase in MonthlyIncome, the odds of an employee attriting decreases slightly, represented by a slightly less than 50% probability. Whereas an employee that works OverTime has nearly 4-1 odds of attriting over an employee that does not work OverTime, represented by an increased probability of 78.7%. # convert to odds exp(coef(model1)) ## (Intercept) MonthlyIncome ## 0.3740 0.9999 exp(coef(model2)) ## (Intercept) OverTimeYes ## 0.1178 3.6974 # convert to probability exp(coef(model1)) / (1 + exp(coef(model1))) ## (Intercept) MonthlyIncome ## 0.2722 0.5000 exp(coef(model2)) / (1 + exp(coef(model2))) ## (Intercept) OverTimeYes ## 0.1054 0.7871 Many aspects of the coefficient output are similar to those discussed in the linear regression output. For example, we can measure the confidence intervals and accuracy of the coefficient estimates by computing their standard errors. For instance, both models‚Äôs \\(\\hat \\beta_1\\) have a p-value &lt; 0.05 suggesting a strong probability that a relationship between these predictors and the probability of attrition exists. We can also use the standard errors to get confidence intervals as we did in the linear regression tutorial: confint(model1) ## 2.5 % 97.5 % ## (Intercept) -1.2812812 -0.6848677 ## MonthlyIncome -0.0001648 -0.0000689 confint(model2) ## 2.5 % 97.5 % ## (Intercept) -2.3808 -1.911 ## OverTimeYes 0.9653 1.652 8.4 Multiple logistic regression We can also extend our model as seen in Eq. 1 so that we can predict a binary response using multiple predictors where \\(X = (X_1,\\dots, X_p)\\) are p predictors: \\[\\begin{equation} \\tag{8.4} p(X) = \\frac{e^{\\beta_0 + \\beta_1X + \\cdots + \\beta_pX_p }}{1 + e^{\\beta_0 + \\beta_1X + \\cdots + \\beta_pX_p}} \\end{equation}\\] Let‚Äôs go ahead and fit a model that predicts the probability of Attrition based on the MonthlyIncome and OverTime. Our results show that both features are statistically significant and Figure 8.3 illustrates common trends between MonthlyIncome and Attrition; however, working OverTime tends to nearly double the probability of attrition. model3 &lt;- glm(Attrition ~ MonthlyIncome + OverTime, family = &quot;binomial&quot;, data = train) tidy(model3) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.44 0.173 -8.32 9.00e-17 ## 2 MonthlyIncome -0.000124 0.0000254 -4.88 1.06e- 6 ## 3 OverTimeYes 1.36 0.179 7.61 2.75e-14 Figure 8.3: Predicted probability of attrition based on monthly income and whether or not employees work overtime. 8.5 Assessing model accuracy With a basic understanding of logistic regression under our belt, similar to linear regression our concern now shifts to how well do our models predict. As in the last chapter, we will use caret::train and fit three 10-fold cross validated logistic regression models. Extracting the accuracy measures, we see that both cv_model1 and cv_model2 had an average accuracy of 83.89%. However, cv_model3 which used all predictor variables in our data achieved an average accuracy rate of 86.3%. set.seed(123) cv_model1 &lt;- train( Attrition ~ MonthlyIncome, data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) set.seed(123) cv_model2 &lt;- train( Attrition ~ MonthlyIncome + OverTime, data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) set.seed(123) cv_model3 &lt;- train( Attrition ~ ., data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # extract out of sample performance measures summary(resamples(list( model1 = cv_model1, model2 = cv_model2, model3 = cv_model3 )))$statistics$Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## model1 0.8350 0.8353 0.8365 0.8389 0.8431 0.8447 ## model2 0.8350 0.8353 0.8365 0.8389 0.8431 0.8447 ## model3 0.8058 0.8389 0.8586 0.8632 0.8949 0.9135 ## NA&#39;s ## model1 0 ## model2 0 ## model3 0 We can get greater understanding of our model‚Äôs performance by assessing the confusion matrix (see section 6.4.7). We can use train::confusionMatrix to compute a confusion matrix. We need to supply our model‚Äôs predicted class and the actuals from our trainin data. Our confusion matrix provides a host of information. Particularly, we can see that although we do well predicting cases of non-attrition (note the high specificity), our model does particularly poor predicting actual cases of attrition (note the low sensitivity). By default the predict function predicts the response class for a caret model; however, you can change the type argument to predict the probabilities (see ?predict.train). # predict class pred_class &lt;- predict(cv_model3, train) # create confusion matrix confusionMatrix(relevel(pred_class, ref = &quot;Yes&quot;), relevel(train$Attrition, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 79 33 ## No 87 831 ## ## Accuracy : 0.883 ## 95% CI : (0.862, 0.902) ## No Information Rate : 0.839 ## P-Value [Acc &gt; NIR] : 3.05e-05 ## ## Kappa : 0.504 ## Mcnemar&#39;s Test P-Value : 1.31e-06 ## ## Sensitivity : 0.4759 ## Specificity : 0.9618 ## Pos Pred Value : 0.7054 ## Neg Pred Value : 0.9052 ## Prevalence : 0.1612 ## Detection Rate : 0.0767 ## Detection Prevalence : 0.1087 ## Balanced Accuracy : 0.7189 ## ## &#39;Positive&#39; Class : Yes ## One thing to point out, in the confusion matrix above you will note the metric No Information Rate: 0.8388. This represents the ratio of non-attrition versus attrition in our trainin data (table(train$Attrition) %&gt;% prop.table()). Consequently, if we simply predicted ‚ÄúNo‚Äù for every employee we would still get an accuracy rate of 83.88%. Therefore, our goal is to maximize our accuracy rate over and above this no information benchmark while also trying to balance sensitivity and specificity. To understand how well we are achieving this we can visualize the ROC curve (section 6.4.7). If we compare our simple model (cv_model1) to our full model cv_model3, we can see that we the lift achieved with the more accurate model. library(ROCR) # create predicted probabilities m1_prob &lt;- predict(cv_model1, train, type = &quot;prob&quot;)$Yes m3_prob &lt;- predict(cv_model3, train, type = &quot;prob&quot;)$Yes # compute AUC metrics for cv_model1 and cv_model3 perf1 &lt;- prediction(m1_prob, train$Attrition) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) perf2 &lt;- prediction(m3_prob, train$Attrition) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # plot both ROC curves for cv_model1 and cv_model3 plot(perf1, col = &quot;black&quot;, lty = 2) plot(perf2, add = TRUE, col = &quot;blue&quot;) legend(.8, .2, legend = c(&quot;cv_model1&quot;, &quot;cv_model3&quot;), col = c(&quot;black&quot;, &quot;blue&quot;), lty = 2:1, cex = 0.6) Figure 8.4: ROC curve for cv_model1 and cv_model3. The increase in the AUC represents the ‚Äòlift‚Äô that we achieve with cv_model3. Similar to linear regression, we can perform a PLS logistic regression to assess if reducing the dimension of our numeric predictors helps to achieve improved accuracy. There are 16 numeric features in our data set so the following performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1-16. The optimal model uses 14 principal components, which is not reducing the dimension by much. However, the mean accuracy of 0.866 was only marginally better than the average CV accuracy of cv_model3 (0.863), likely within the margin of error. # perform 10-fold cross validation on a PLS model tuning the number of # principal components to use as predictors from 1-20 set.seed(123) cv_model_pls &lt;- train( Attrition ~ ., data = train, method = &quot;pls&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), preProcess = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), tuneLength = 16 ) # model with lowest RMSE cv_model_pls$bestTune ## ncomp ## 14 14 # plot cross-validated RMSE plot(cv_model_pls) Figure 8.5: The 10-fold cross valdation RMSE obtained using PLS with 1-16 principal components. 8.6 Feature interpretation Similar to linear regression, once our preferred logistic regression model is identified, next we need to interpret how the features are influencing the results. As with normal linear regression models, variable importance for logistic regression models are computed with the absolute value of the t-statistic for each model parameter is used. Using vip we can extract our top 20 influential variables. Figure 8.6 illustrates that OverTime is the most influential followed by JobSatisfaction, NumCompaniesWorked, and EnvironmentSatisfaction. vip(cv_model3, num_features = 20) Figure 8.6: Top 20 most important variables for the PLS model. Similar to linear regression, logistic regression assumes a monotonic linear relationship. However, the linear relationship is in the form of a log-odds probability; therefore, the regular probability relationship will have a curvilinear effect. This is illustrated in Figure 8.7 by the change in predicted probability of attrition associated with the marginal change in the number of companies an employee has work for (NumCompaniesWorked). Employees that have experienced more employment changes tend to have a high probability of making another future change. Furthermore, the partial dependence plots for the three top categorical predictors (OverTime, JobSatisfaction, and EnvironmentSatisfaction) illustrate the change in predicted probability of attrition based on the employee‚Äôs status for each predictor. See the supplemental material at [https://github.com/koalaverse/abar](https://github.com/koalaverse/abar] for the code to produce the following plots. Figure 8.7: Partial dependence plots for the first four most important variables. We can see how the predicted probability of attrition changes for each value of the influential predictors. 8.7 Final thoughts Logistic regression is a natural starting point for learning predictive models for classification purposes due to its similarity to linear regression. Later chapters will build on the concepts illustrated in this chapter and will compare cross-validated performance results to identify the best predictive model for our employee attrition problem. The following summarizes some of the advantages and disadvantages discussed regarding logistic regression. FIXME: refine this section Advantages: Disadvantages: 8.8 Learning more This will get you up and running with logistic regression. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more: An Introduction to Statistical Learning Applied Predictive Modeling Elements of Statistical Learning References "],
["regularized-regression.html", "Chapter 9 Regularized regression 9.1 Prerequisites 9.2 Why Regularize 9.3 Implementation 9.4 Tuning 9.5 Feature interpretation 9.6 Attrition data 9.7 Final thoughts 9.8 Learning more", " Chapter 9 Regularized regression Generalized linear models (GLMs) such as ordinary least squares regression and logistic regression are simple and fundamental approaches for supervised learning. Moreover, when the assumptions required by GLMs are met, the coefficients produced are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in today‚Äôs world, data sets being analyzed typically have a large amount of features. As the number of features grow, our GLM assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our coefficients, which can reduce the variance and decrease out of sample error. 9.1 Prerequisites This chapter leverages the following packages. Most of these packages are playing a supporting role while the main emphasis will be on the glmnet package (Friedman, Hastie, and Tibshirani 2010). library(rsample) # data splitting library(glmnet) # implementing regularized regression approaches library(caret) # automating the tuning process library(vip) # variable importance To illustrate various regularization concepts we will use the Ames Housing data; however, at the end of the chapter we will also apply regularization to the employee attrition data. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 9.2 Why Regularize The easiest way to understand regularized regression is to explain how it is applied to ordinary least squares regression (OLS). The objective of OLS regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. Illustrated below, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane). Figure 9.1: Fitted regression line using Ordinary Least Squares. More formally, this objective function can be written as: \\[\\begin{equation} \\tag{9.1} \\text{minimize} \\bigg \\{ SSE = \\sum^n_{i=1} (y_i - \\hat{y}_i)^2 \\bigg \\} \\end{equation}\\] As we discussed in Chapter 7, the OLS objective function performs quite well when our data align to the key assumptions of OLS regression: Linear relationship Multivariate normality No autocorrelation Homoscedastic (constant variance in residuals) There are more observations (n) than features (p) (\\(n &gt; p\\)) No or little multicollinearity However, for many real-life data sets we have very wide data, meaning we have a large number of features (p) that we believe are informative in predicting some outcome. As p increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. This was illustrated in Chapter 7 where multicollinearity was biasing our coefficients and preventing us from maximizing our predictive accuracy. By reducing multicollinearity, we were able to increase our model‚Äôs accuracy. In addition to the above barriers to OLS performing well, with a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called hard threshholding feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a soft threshholding approach that slowly pushes a feature‚Äôs effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals. When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as penalized models or shrinkage methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model. The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P). \\[\\begin{equation} \\tag{9.2} \\text{minimize} \\big \\{ SSE + P \\big \\} \\end{equation}\\] This penalty parameter constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). This concept generalizes to all GLM models. So far, we have be discussing OLS and the sum of squared errors. However, different models within the GLM family (i.e. logistic regression, Poisson regression) have different loss functions. Yet we can think of the penalty parameter all the same - it constrains the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the model‚Äôs loss function. There are three types of penalty parameters we can implement: Ridge Lasso Elastic net, which is a combination of Ridge and Lasso 9.2.1 Ridge penalty Ridge regression (Hoerl and Kennard 1970) controls the coefficients by adding \\(\\lambda \\sum^p_{j=1} \\beta_j^2\\) to the objective function. This penalty parameter is also referred to as ‚Äú\\(L_2\\)‚Äù as it signifies a second-order penalty being used on the coefficients.13 \\[\\begin{equation} \\tag{9.3} \\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} \\beta_j^2 \\bigg \\} \\end{equation}\\] This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter \\(\\lambda\\). When \\(\\lambda = 0\\) there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing SSE. However, as \\(\\lambda \\rightarrow \\infty\\), the penalty becomes large and forces our coefficients to near zero. This is illustrated in Figure 9.2 where exemplar coefficients have been regularized with \\(\\lambda\\) ranging from 0 to over 8,000 (\\(log(8103) = 9\\)). Figure 9.2: Ridge regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Although these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when \\(\\lambda \\rightarrow 0\\). Furthermore, you‚Äôll notice the large negative parameter that fluctuates until \\(log(\\lambda) \\approx 2\\) where it then continuously skrinks to zero. This is indicitive of multicollinearity and likely illustrates that constraining our coefficients with \\(log(\\lambda) &gt; 2\\) may reduce the variance, and therefore the error, in our model. In essence, the ridge regression model pushes many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features get pushed to near zero. This allows us to reduce the noise in our data, which provides us more clarity in identifying the true signals in our model. However, a ridge model will retain all variables. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform automated feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso or elastic net penalty may be preferable. 9.2.2 Lasso penalty The least absolute shrinkage and selection operator (lasso) model (Tibshirani 1996) is an alternative to the ridge penalty that has a small modification to the penalty in the objective function. Rather than the \\(L_2\\) penalty we use the following \\(L_1\\) penalty \\(\\lambda \\sum^p_{j=1} | \\beta_j|\\) in the objective function. \\[\\begin{equation} \\tag{9.4} \\text{minimize } \\bigg \\{ SSE + \\lambda \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\end{equation}\\] Whereas the ridge penalty approach pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients to zero as illustrated in Figure 9.3. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection. Figure 9.3: Lasso regression coefficients as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). Numbers on top axis illustrate how many non-zero coefficients remain. In the figure above we see that when \\(log(\\lambda) = -5\\) all 15 variables are in the model, when \\(log(\\lambda) = -1\\) 12 variables are retained, and when \\(log(\\lambda) = 1\\) only 3 variables are retained. Consequently, when a data set has many features, lasso can be used to identify and extract those features with the largest (and most consistent) signal. 9.2.3 Elastic nets A generalization of the ridge and lasso penalties is the elastic net penalty (Zou and Hastie 2005), which combines the two penalties. \\[\\begin{equation} \\tag{9.5} \\text{minimize } \\bigg \\{ SSE + \\lambda_1 \\sum^p_{j=1} \\beta_j^2 + \\lambda_2 \\sum^p_{j=1} | \\beta_j | \\bigg \\} \\end{equation}\\] Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net penalty is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. 9.3 Implementation We illustrate implementation of regularized regression with the glmnet package; however, realize there are other implementations available (i.e. h2o, elasticnet, penalized). The glmnet package is a fast implementation, but it requires some extra processing up-front to your data if it‚Äôs not already represented as a numeric matrix. glmnet does not use the formula method (y ~ x) so prior to modeling we need to create our feature and target set. Furthermore, we use the model.matrix function on our feature set (see Matrix::sparse.model.matrix for increased efficiency on large dimension data). We also log transform our response variable due to its skeweness. The log transformation of the response variable is not required; however, parametric models such as regularized regression are sensitive to skewed values so it is always recommended to normalize your response variable. # Create training and testing feature matrices # we use model.matrix(...)[, -1] to discard the intercept train_x &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1] test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1] # Create training and testing response vectors # transform y with log transformation train_y &lt;- log(ames_train$Sale_Price) test_y &lt;- log(ames_test$Sale_Price) To apply a regularized model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (\\(0 &lt; alpha &lt; 1\\)) model. Behind the scenes, glmnet is doing two things that you should be aware of: Since regularized methods apply a penalty to the coefficients, we need to ensure our coefficients are on a common scale. If not, then predictors with naturally larger values (i.e. total square footage) will be penalized more than predictors with naturally smaller values (i.e. total number of rooms). glmnet automatically standardizes your features. If you standardize your predictors prior to glmnet you can turn this argument off with standardize = FALSE. glmnet will perform ridge models across a wide range of \\(\\lambda\\) parameters, which are illustrated in the figure below. # Apply Ridge regression to attrition data ridge &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) plot(ridge, xvar = &quot;lambda&quot;) Figure 9.4: Coefficients for our ridge regression model as \\(\\lambda\\) grows from \\(0 \\rightarrow \\infty\\). In fact, we can see the exact \\(\\lambda\\) values applied with ridge$lambda. Although you can specify your own \\(\\lambda\\) values, by default glmnet applies 100 \\(\\lambda\\) values that are data derived. glmnet has built-in functions to auto-generate the appropriate \\(\\lambda\\) values based on the data so the vast majority of the time you will have little need to adjust the default \\(\\lambda\\) values. We can also directly access the coefficients for a model using coef. glmnet stores all the coefficients for each model in order of largest to smallest \\(\\lambda\\). Due to the number of features, here I just peak at the two largest coefficients (Latitude &amp; Overall_QualVery_Excellent) features for the largest \\(\\lambda\\) (279.1035) and smallest \\(\\lambda\\) (0.02791035). You can see how the largest \\(\\lambda\\) value has pushed these coefficients to nearly 0. # lambdas applied to penalty parameter ridge$lambda %&gt;% head() ## [1] 279.1 254.3 231.7 211.1 192.4 175.3 # small lambda results in large coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 100] ## Latitude Overall_QualVery_Excellent ## 0.6059 0.0980 # large lambda results in small coefficients coef(ridge)[c(&quot;Latitude&quot;, &quot;Overall_QualVery_Excellent&quot;), 1] ## Latitude Overall_QualVery_Excellent ## 6.228e-36 9.373e-37 However, at this point, we do not understand how much improvement we are experiencing in our loss function across various \\(\\lambda\\) values. 9.4 Tuning Recall that \\(\\lambda\\) is a tuning parameter that helps to control our model from over-fitting to the training data. However, to identify the optimal \\(\\lambda\\) value we need to perform cross-validation (CV). cv.glmnet provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV. Here we perform a CV glmnet model for both a ridge and lasso penalty. By default, cv.glmnet uses MSE as the loss function but you can also use mean absolute error by changing the type.measure argument. # Apply CV Ridge regression to Ames data ridge &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 0 ) # Apply CV Lasso regression to Ames data lasso &lt;- cv.glmnet( x = train_x, y = train_y, alpha = 1 ) # plot results par(mfrow = c(1, 2)) plot(ridge, main = &quot;Ridge penalty\\n\\n&quot;) plot(lasso, main = &quot;Lasso penalty\\n\\n&quot;) Figure 9.5: 10-fold cross validation MSE for a ridge and lasso model. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. Figure 9.5 illustrate the 10-fold CV mean squared error (MSE) across the \\(\\lambda\\) values. In both models we see a slight improvement in the MSE as our penalty \\(log(\\lambda)\\) gets larger , suggesting that a regular OLS model likely overfits our data. But as we constrain it further (continue to increase the penalty), our MSE starts to increase. The numbers at the top of the plot refer to the number of variables in the model. Ridge regression does not force any variables to exactly zero so all features will remain in the model but we see the number of variables retained in the lasso model go down as our penalty increases. The first and second vertical dashed lines represent the \\(\\lambda\\) value with the minimum MSE and the largest \\(\\lambda\\) value within one standard error of the minimum MSE. The minimum MSE for our ridge model is 0.0215 (produced when \\(\\lambda = 0.1026649\\)) whereas the minimium MSE for our lasso model is 0.0228 (produced when \\(\\lambda = 0.003521887\\)). # Ridge model min(ridge$cvm) # minimum MSE ## [1] 0.02148 ridge$lambda.min # lambda for this min MSE ## [1] 0.1237 ridge$cvm[ridge$lambda == ridge$lambda.1se] # 1 st.error of min MSE ## [1] 0.02488 ridge$lambda.1se # lambda for this MSE ## [1] 0.6599 # Lasso model min(lasso$cvm) # minimum MSE ## [1] 0.02411 lasso$lambda.min # lambda for this min MSE ## [1] 0.003865 lasso$cvm[lasso$lambda == lasso$lambda.1se] # 1 st.error of min MSE ## [1] 0.02819 lasso$lambda.1se # lambda for this MSE ## [1] 0.0156 We can assess this visually. Figure 9.6 plot the coefficients across the \\(\\lambda\\) values and the dashed red line represents the \\(\\lambda\\) with the smallest MSE and the dashed blue line represents largest \\(\\lambda\\) that falls within one standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy. Above, we saw that both ridge and lasso penalties provide similiar MSEs; however, these plots illustrate that ridge is still using all 299 variables whereas the lasso model can get a similar MSE by reducing our feature set from 299 down to 131. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. Although this lasso model does not offer significant improvement over the ridge model, we get approximately the same accuracy by using only 63 features! If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor. # Ridge model ridge_min &lt;- glmnet( x = train_x, y = train_y, alpha = 0 ) # Lasso model lasso_min &lt;- glmnet( x = train_x, y = train_y, alpha = 1 ) par(mfrow = c(1, 2)) # plot ridge model plot(ridge_min, xvar = &quot;lambda&quot;, main = &quot;Ridge penalty\\n\\n&quot;) abline(v = log(ridge$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(ridge$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) # plot lasso model plot(lasso_min, xvar = &quot;lambda&quot;, main = &quot;Lasso penalty\\n\\n&quot;) abline(v = log(lasso$lambda.min), col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = log(lasso$lambda.1se), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Figure 9.6: Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the \\(\\lambda\\) with the smallest MSE and the second represents the \\(\\lambda\\) with an MSE within one standard error of the minimum MSE. So far we‚Äôve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas alpha \\(\\rightarrow 0\\) will have a heavier ridge penalty applied and alpha \\(\\rightarrow 1\\) will have a heavier lasso penalty. Figure 9.7: Coefficients for various penalty parameters. Often, the optimal model contains an alpha somewhere between 0-1, thus we want to tune both the \\(\\lambda\\) and the alpha parameters. As in Chapters 7 and 8, we can use the caret package to automate the tuning process. The following performs a grid search over 10 values of the alpha parameter between 0-1 and ten values of the lambda parameter from the lowest to highest lambda values identified by glmnet. This grid search took 71 seconds to compute. The following shows the model that minimized RMSE used an alpha of 0.1 and lambda of 0.0453. The minimum RMSE of 0.1448677 (\\(MSE = 0.1448677^2 = 0.02099\\)) is only slightly lower than our full ridge model produced earlier. Figure 9.8 illustrates how the combination of alpha values (x-axis) and lambda values (line color) influence the RMSE. # for reproducibility set.seed(123) # grid search across tuned_mod &lt;- train( x = train_x, y = train_y, method = &quot;glmnet&quot;, preProc = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneLength = 10 ) # model with lowest RMSE tuned_mod$bestTune ## alpha lambda ## 8 0.1 0.04528 # plot cross-validated RMSE plot(tuned_mod) Figure 9.8: The 10-fold cross valdation RMSE across 10 alpha values (x-axis) and 10 lambda values (line color). So how does this compare to our previous best model for the Ames data? Keep in mind that for this chapter we log transformed our response variable. Consequently, to provide a fair comparison to our partial least squares RMSE of $31,522.47, we need to re-transform our predicted values. The following illustrates that our optimal regularized model achieves an RMSE of $26,608.12. Introducing a penalty parameter to constrain the coefficients provides quite an improvement over our dimension reduction approach. # predict sales price on training data pred &lt;- predict(tuned_mod, train_x) # compute RMSE of transformed predicted RMSE(exp(pred), exp(train_y)) ## [1] 26608 9.5 Feature interpretation Variable importance for regularized models provide a similar interpretation as in linear (or logistic) regression. Importance is determined by the absolute value of the t-statistic and we can see in Figure 9.9 some of the same variables that were considered highly influential in our partial least squares model, albeit in differing order (i.e. Gr_Liv_Area, Overall_Qual, First_Flr_SF, Garage_Cars). vip(tuned_mod, num_features = 20, bar = FALSE) Figure 9.9: Top 20 most important variables for the optimal regularized regression model. Similar to linear and logistic regression, the relationship between these influential variables and the response is monotonic linear. However, since we modeled our response with a log transformation, the relationship between will be monotonic but non-linear for the untransformed relationship. Figure 9.10 illustrates the relationship between the top four most influential variables and the non-transformed sales price. All relationships are positive in nature, as the values in these features increase (or for Overall_QualExcellent if it exists) the average predicted sales price increases. Figure 9.10: Partial dependence plots for the first four most important variables. However, we see the \\(5^{th}\\) most influential variable is Overall_QualPoor. When a home has an overall quality rating of poor we see that the average predicted sales price decreases versus when it has some other overall quality rating. Consequently, its important to not only look at the variable importance ranking, but also observe the positive or negative nature of the relationship. Figure 9.11: Partial dependence plots for the first four most important variables. 9.6 Attrition data We saw that regularization significantly improved our predictive accuracy for the Ames data, but how about for the attrition data. In Chapter 8 we saw a maximum cross-validated accuracy of 86.3% for our logistic regression model. Performing a regularized logistic regression model provides us with about 1.5% improvement in our accuracy. df &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) # Create training (70%) and test (30%) sets for the rsample::attrition data. # Use set.seed for reproducibility set.seed(123) churn_split &lt;- initial_split(df, prop = .7, strata = &quot;Attrition&quot;) train &lt;- training(churn_split) test &lt;- testing(churn_split) # train logistic regression model set.seed(123) glm_mod &lt;- train( Attrition ~ ., data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, preProc = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # train regularized logistic regression model set.seed(123) penalized_mod &lt;- train( Attrition ~ ., data = train, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, preProc = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneLength = 10 ) # extract out of sample performance measures summary(resamples(list( logistic_model = glm_mod, penalized_model = penalized_mod )))$statistics$Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. ## logistic_model 0.8058 0.8389 0.8586 0.8632 0.8949 ## penalized_model 0.8447 0.8568 0.8744 0.8787 0.9069 ## Max. NA&#39;s ## logistic_model 0.9135 0 ## penalized_model 0.9135 0 9.7 Final thoughts Regularized regression is a great start for building onto generalized linear models (i.e. OLS, logistic regression) to make them more robust to assumption violations and perform automated feature selection. This chapter illustrated how constraining our coefficients with a regulazation penalty helped to improve predictive accuary for both the ames and attrition data. However, regularized models still assume linear relationships. The chapters that follow will start exploring non-linear algorithms to see if we can further improve our predictive accuracy. The following summarizes some of the advantages and disadvantages discussed regarding regularized regression. FIXME: refine this section Advantages: Normal GLM models require that you have more observations than variables (\\(n&gt;p\\)); regularized regression allows you to model wide data where \\(n&lt;p\\). Minimizes the impact of multicollinearity. Provides automatic feature selection (at least when you apply a Lasso or elastic net penalty). Minimal hyperparameters making it easy to tune. Computationally efficient - relatively fast compared to other algorithms in this guide and does not require large memory. Disdvantages: Requires data pre-processing - requires all variables to be numeric (i.e. one-hot encode). However, some implementations (i.e. h2o package) helps to automate this process. Does not handle missing data - must impute or remove observations with missing values. Not robust to outliers as they can still bias the coefficients. Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion). Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks). 9.8 Learning more This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric (i.e. Cox proportional hazard, poisson, support vector machines) and non-parametric (i.e. Least Angle Regression, the Bayesian Lasso, neural networks) models. The following are great resources to learn more (listed in order of complexity): Applied Predictive Modeling Practical Machine Learning with H2o Introduction to Statistical Learning The Elements of Statistical Learning Statistical Learning with Sparsity References "],
["MARS.html", "Chapter 10 Multivariate Adaptive Regression Splines 10.1 Prerequisites 10.2 The basic idea 10.3 Fitting a basic MARS model 10.4 Tuning 10.5 Feature interpretation 10.6 Attrition data 10.7 Final thoughts 10.8 Learning more", " Chapter 10 Multivariate Adaptive Regression Splines The previous chapters discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects); however, to do so you must know the specific nature of the nonlinearity a priori. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy. This chapter discusses multivariate adaptive regression splines (MARS), an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future chapters will focus on other nonlinear algorithms. 10.1 Prerequisites For this chapter we will use the following packages: library(rsample) # data splitting library(ggplot2) # plotting library(earth) # fit MARS models library(caret) # automating the tuning process library(vip) # variable importance library(pdp) # variable relationships To illustrate various MARS modeling concepts we will use the Ames Housing data; however, at the end of the chapter we will also apply a MARS model to the employee attrition data. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 10.2 The basic idea In the previous chapters, we focused on linear models. We illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients. However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy. We can extend linear models to capture non-linear relationships. Typically, this is done by explicitly including polynomial parameters or step functions. Polynomial regression is a form of regression in which the relationship between the independent variable x and the dependent variable y is modeled as an n\\(^{th}\\) degree polynomial of x. For example, Equation (10.1) represents a polynomial regression function where y is modeled as a function of x with d degrees. Generally speaking, it is unusual to use d greater than 3 or 4 as the larger d becomes, the easier the function fit becomes overly flexible and oddly shapened‚Ä¶especially near the boundaries of the range of x values. \\[\\begin{equation} \\tag{10.1} y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x^2_i + \\beta_3 x^3_i \\dots + \\beta_d x^d_i + \\epsilon_i, \\end{equation}\\] An alternative to polynomial regression is step function regression. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of x into bins, and fit a different constant for each bin. This amounts to converting a continuous variable into an ordered categorical variable such that our linear regression function is converted to Equation (10.2) \\[\\begin{equation} \\tag{10.2} y_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\beta_3 C_3(x_i) \\dots + \\beta_d C_d(x_i) + \\epsilon_i, \\end{equation}\\] where \\(C_1(x)\\) represents x values ranging from \\(c_1 \\leq x &lt; c_2\\), \\(C_2(x)\\) represents x values ranging from \\(c_2 \\leq x &lt; c_3\\), \\(\\dots\\), \\(C_d(x)\\) represents x values ranging from \\(c_{d-1} \\leq x &lt; c_d\\). Figure 10.1 illustrate polynomial and step function fits for Sale_Price as a function of Year_Built in our ames data. Figure 10.1: Blue line represents predicted Sale_Price values as a function of Year_Built for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional nonlinear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting Year_Built into three categorical levels. Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at what points of a variable x should cut points be made for the step functions. Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unncessary time commitment from an analyst to determine these explicit non-linear settings. 10.2.1 Multivariate regression splines Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinearity aspect of polynomial regression by assessing cutpoints (knots) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). For example, consider our simple model of Sale_Price ~ Year_Built. The MARS procedure will first look for the single point across the range of Year_Built values where two different linear relationships between Sale_Price and Year_Built achieve the smallest error. What results is known as a hinge function (\\(h(x-a)\\) where a is the cutpoint value). For a single knot (Figure 10.2 (A)), our hinge function is \\(h(\\text{Year_Built}-1968)\\) such that our two linear models for Sale_Price are \\[\\begin{equation} \\tag{10.3} \\text{Sale_Price} = \\begin{cases} 136091.022 &amp; \\text{Year_Built} \\leq 1968, \\\\ 136091.022 + 3094.208(\\text{Year_Built} - 1968) &amp; \\text{Year_Built} &gt; 1968 \\end{cases} \\end{equation}\\] Once the first knot has been found, the search continues for a second knot which is found at 2006 (Figure 10.2 (B)). This results in three linear models for Sale_Price: \\[\\begin{equation} \\tag{10.4} \\text{Sale_Price} = \\begin{cases} 136091.022 &amp; \\text{Year_Built} \\leq 1968, \\\\ 136091.022 + 2898.424(\\text{Year_Built} - 1968) &amp; 1968 &lt; \\text{Year_Built} \\leq 2006, \\\\ 136091.022 + 20176.284(\\text{Year_Built} - 2006) &amp; \\text{Year_Built} &gt; 2006 \\end{cases} \\end{equation}\\] Figure 10.2: Examples of fitted regression splines of one (A), two (B), three (C), and four (D) knots. This procedure can continue until many knots are found, producing a highly non-linear pattern. Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. For example, Figure 10.3 includes nine knots but this likley will not generalize very well to our test data. Figure 10.3: Too many knots may not generalize well to unseen data. Consequently, once the full set of knots have been created, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as ‚Äúpruning‚Äù and we can use cross-validation, as we have with the previous models, to find the optimal number of knots. 10.3 Fitting a basic MARS model We can fit a MARS model with the earth package (Trevor Hastie and Thomas Lumley‚Äôs leaps wrapper. 2018). By default, earth::earth() will assess all potential knots across all supplied features and then will prune to the optimal number of knots based on an expected change in \\(R^2\\) (for the training data) of less than 0.001. This calculation is performed by the Generalized cross-validation procedure (GCV statistic), which is a computational shortcut for linear models that produces an error value that approximates leave-one-out cross-validation (Golub, Heath, and Wahba 1979). The term ‚ÄúMARS‚Äù is trademarked and licensed exclusively to Salford Systems http://www.salfordsystems.com. We can use MARS as an abbreviation; however, it cannot be used for competing software solutions. This is why the R package uses the name earth. The following applies a basic MARS model to our ames data and performs a search for required knots across all features. The results show us the final models GCV statistic, generalized \\(R^2\\) (GRSq), and more. # Fit a basic MARS model mars1 &lt;- earth( Sale_Price ~ ., data = ames_train ) # Print model summary print(mars1) ## Selected 37 of 45 terms, and 26 of 307 predictors ## Termination condition: RSq changed by less than 0.001 at 45 terms ## Importance: Gr_Liv_Area, Year_Built, ... ## Number of terms at each degree of interaction: 1 36 (additive model) ## GCV 521186626 RSS 9.958e+11 GRSq 0.9165 RSq 0.9223 It also shows us that 38 of 41 terms were used from 27 of the 307 original predictors. But what does this mean? If we were to look at all the coefficients, we would see that there are 38 terms in our model (including the intercept). These terms include hinge functions produced from the original 307 predictors (307 predictors because the model automatically dummy encodes our categorical variables). Looking at the first 10 terms in our model, we see that Gr_Liv_Area is included with a knot at 2945 (the coefficient for \\(h(2945-Gr_Liv_Area)\\) is -49.85), Year_Built is included with a knot at 2003, etc. You can check out all the coefficients with summary(mars1). summary(mars1) %&gt;% .$coefficients %&gt;% head(10) ## Sale_Price ## (Intercept) 301399.98 ## h(2945-Gr_Liv_Area) -49.85 ## h(Year_Built-2003) 2698.40 ## h(2003-Year_Built) -357.11 ## h(Total_Bsmt_SF-2171) -265.31 ## h(2171-Total_Bsmt_SF) -29.77 ## Overall_QualExcellent 88345.90 ## Overall_QualVery_Excellent 116330.49 ## Overall_QualVery_Good 36646.56 ## h(Bsmt_Unf_SF-278) -21.16 The plot method for MARS model objects provide convenient performance and residual plots. Figure 10.4 illustrates the model selection plot that graphs the GCV \\(R^2\\) (left-hand y-axis and solid black line) based on the number of terms retained in the model (x-axis) which are constructed from a certain number of original predictors (right-hand y-axis). The vertical dashed lined at 37 tells us the optimal number of non-intercept terms retained where marginal increases in GCV \\(R^2\\) are less than 0.001. plot(mars1, which = 1) Figure 10.4: Model summary capturing GCV \\(R^2\\) (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 37 non-intercept terms were retained which are based on 26 predictors. Any additional terms retained in the model, over and above these 37, results in less than 0.001 improvement in the GCV \\(R^2\\). In addition to pruning the number of knots, earth::earth() allows us to also assess potential interactions between different hinge functions. The following illustrates by including a degree = 2 argument. You can see that now our model includes interaction terms between multiple hinge functions (i.e. h(Year_Built-2003)*h(Gr_Liv_Area-2274)) is an interaction effect for those houses built prior to 2003 and have less than 2,274 square feet of living space above ground). # Fit a basic MARS model mars2 &lt;- earth( Sale_Price ~ ., data = ames_train, degree = 2 ) # check out the first 10 coefficient terms summary(mars2) %&gt;% .$coefficients %&gt;% head(10) ## Sale_Price ## (Intercept) 242611.64 ## h(Gr_Liv_Area-2945) 144.39 ## h(2945-Gr_Liv_Area) -57.72 ## h(Year_Built-2003) 10909.70 ## h(2003-Year_Built) -780.24 ## h(Year_Built-2003)*h(Gr_Liv_Area-2274) 18.55 ## h(Year_Built-2003)*h(2274-Gr_Liv_Area) -10.31 ## h(Total_Bsmt_SF-1035) 62.12 ## h(1035-Total_Bsmt_SF) -33.04 ## h(Total_Bsmt_SF-1035)*Kitchen_QualTypical -32.76 10.4 Tuning Since there are two tuning parameters associated with our MARS model: the degree of interactions and the number of retained terms, we need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of cross-validated performance on the training data rather than an actual k-fold cross validation process). As in previous chapters, we will perform a cross-validated grid search to identify the optimal mix. Here, we set up a search grid that assesses 30 different combinations of interaction effects (degree) and the number of terms to retain (nprune). # create a tuning grid hyper_grid &lt;- expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor() ) head(hyper_grid) ## degree nprune ## 1 1 2 ## 2 2 2 ## 3 3 2 ## 4 1 12 ## 5 2 12 ## 6 3 12 As in the previous chapters, we can use caret to perform a grid search using 10-fold cross-validation. The model that provides the optimal combination includes second degree interactions and retains 34 terms. The cross-validated RMSE for these models are illustrated in Figure 10.5 and the optimal model‚Äôs cross-validated RMSE is $24,021.68. This grid search took 5 minutes to complete. # for reproducibiity set.seed(123) # cross validated model tuned_mars &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = hyper_grid ) # best model tuned_mars$bestTune ## nprune degree ## 14 34 2 # plot results ggplot(tuned_mars) Figure 10.5: Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 34 terms and includes up to 2\\(^{nd}\\) degree interactions. The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for nprune (i.e. comparing 25-40 terms retained). However, for brevity we will leave this as an exercise for the reader. So how does this compare to our previously built linear models for the Ames housing data? The following table compares the cross-validated RMSE for our tuned MARS model to a regular multiple regression model along with tuned principal component regression (PCR), partial least squares (PLS), and regularized regression (elastic net) models. By incorporating non-linear relationships and interaction effects, the MARS model provides a substantial improvement over the previous linear models that we have explored. Min. 1st Qu. Median Mean 3rd Qu. Max. NA‚Äôs Multiple_regression 21305 24403 46475 41438 53958 63247 0 PCR 28214 30775 36548 35770 40253 44760 0 PLS 22812 24196 31162 31522 35383 44895 0 Elastic_net 20970 24265 30590 30717 34216 45241 0 MARS 18440 20745 23147 24022 26241 31755 0 10.5 Feature interpretation MARS models via earth::earth() include a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction is used as the variable importance measure (value = &quot;gcv&quot;). Since MARS will automatically include and exclude terms during the pruning process, it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. This is illustrated in Figure 10.6 where 27 features have \\(&gt;0\\) importance values while the rest of the features have an importance value of zero since they were no included in the final model. Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (value = &quot;rss&quot;); however, you will see very little difference between these methods. # variable importance plots p1 &lt;- vip(tuned_mars, num_features = 40, bar = FALSE, value = &quot;gcv&quot;) + ggtitle(&quot;GCV&quot;) p2 &lt;- vip(tuned_mars, num_features = 40, bar = FALSE, value = &quot;rss&quot;) + ggtitle(&quot;RSS&quot;) gridExtra::grid.arrange(p1, p2, ncol = 2) Figure 10.6: Variable importance based on impact to GCV (left) and RSS (right) values as predictors are added to the model. Both variable importance measures will usually give you very similar results. Its important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature. For example, in Figure 10.6 we see that Gr_Liv_Area and Year_Built are the two most influential variables; however, variable importance does not tell us how our model is treating the non-linear patterns for each feature. Also, if we look at the interaction terms our model retained, we see interactions between different hinge functions for Gr_Liv_Area and Year_Built. coef(tuned_mars$finalModel) %&gt;% tidy() %&gt;% filter(stringr::str_detect(names, &quot;\\\\*&quot;)) ## # A tibble: 16 x 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 h(Year_Built-2003) * h(Gr_Liv_Area-2274) 1.87e+1 ## 2 h(Year_Built-2003) * h(2274-Gr_Liv_Area) -1.09e+1 ## 3 h(Total_Bsmt_SF-1035) * Kitchen_QualTypi‚Ä¶ -3.31e+1 ## 4 NeighborhoodEdwards * h(Gr_Liv_Area-2945) -5.07e+2 ## 5 h(Lot_Area-4058) * h(3-Garage_Cars) -7.91e-1 ## 6 h(2003-Year_Built) * h(Year_Remod_Add-19‚Ä¶ 7.00e+0 ## 7 Overall_QualExcellent * h(Total_Bsmt_SF-‚Ä¶ 1.04e+2 ## 8 NeighborhoodCrawford * h(2003-Year_Built) 4.24e+2 ## 9 h(Lot_Area-4058) * Overall_CondFair -3.29e+0 ## 10 Overall_QualAbove_Average * h(2003-Year_‚Ä¶ 1.36e+2 ## 11 h(Lot_Area-4058) * Overall_CondGood 1.35e+0 ## 12 Bsmt_ExposureNo * h(Total_Bsmt_SF-1035) -2.25e+1 ## 13 NeighborhoodGreen_Hills * h(5-Bedroom_Ab‚Ä¶ 2.74e+4 ## 14 Overall_QualVery_Good * Bsmt_QualGood -1.86e+4 ## 15 h(2003-Year_Built) * Sale_ConditionNormal 1.92e+2 ## 16 h(Lot_Area-4058) * h(Full_Bath-2) 1.61e+0 To better understand the relationship between these features and Sale_Price, we can create partial dependence plots (PDPs) for each feature individually and also an interaction PDP. The individual PDPs illustrate that our model found that one knot in each feature provides the best fit. For Gr_Liv_Area, as homes exceed 2,945 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less than 2,945 square feet. Similarly, for homes built after 2003, there is a greater marginal effect on sales price based on the age of the home than for homes built prior to 2003. The interaction plot (far right plot) illustrates the strong effect these two features have when combined. p1 &lt;- partial(tuned_mars, pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 10) %&gt;% autoplot() p2 &lt;- partial(tuned_mars, pred.var = &quot;Year_Built&quot;, grid.resolution = 10) %&gt;% autoplot() p3 &lt;- partial(tuned_mars, pred.var = c(&quot;Gr_Liv_Area&quot;, &quot;Year_Built&quot;), grid.resolution = 10) %&gt;% plotPartial(levelplot = FALSE, zlab = &quot;yhat&quot;, drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60)) gridExtra::grid.arrange(p1, p2, p3, ncol = 3) Figure 10.7: Partial dependence plots to understand the relationship between Sale_Price and the Gr_Liv_Area and Year_Built features. The PDPs tell us that as Gr_Liv_Area increases and for newer homes, Sale_Price increases dramatically. 10.6 Attrition data We saw significant improvement to our predictive accuracy on the Ames data with a MARS model, but how about the attrition data? In Chapter 8 we saw a slight improvement in our cross-validated accuracy rate using regularized regression. Here, we tune a MARS model using the same search grid as we did above. We see our best models include no interaction effects and the optimal model retains 45 terms. # get attrition data df &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE) # Create training (70%) and test (30%) sets for the rsample::attrition data. # Use set.seed for reproducibility set.seed(123) churn_split &lt;- initial_split(df, prop = .7, strata = &quot;Attrition&quot;) churn_train &lt;- training(churn_split) churn_test &lt;- testing(churn_split) # for reproducibiity set.seed(123) # cross validated model tuned_mars &lt;- train( x = subset(churn_train, select = -Attrition), y = churn_train$Attrition, method = &quot;earth&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = hyper_grid ) # best model tuned_mars$bestTune ## nprune degree ## 5 45 1 # plot results ggplot(tuned_mars) Figure 10.8: Cross-validated accuracy rate for the 30 different hyperparameter combinations in our grid search. The optimal model retains 45 terms and includes no interaction effects. However, comparing our MARS model to the previous linear models (logistic regression and regularized regression), we do not see any improvement in our overall accuracy rate. Min. 1st Qu. Median Mean 3rd Qu. Max. NA‚Äôs Logistic_model 0.8058 0.8389 0.8586 0.8632 0.8949 0.9135 0 Elastic_net 0.8447 0.8568 0.8744 0.8787 0.9069 0.9135 0 MARS_model 0.7961 0.8450 0.8641 0.8593 0.8824 0.9135 0 10.7 Final thoughts MARS provides a great stepping stone into nonlinear modeling and tends to be fairly intuitive due to being closely related to multiple regression techniques. They are also easy to train and tune. This chapter illustrated how incorporating non-linear relationships via MARS modeling greatly improved predictive accuracy on our Ames housing data. The chapters that follow will explore additional non-linear algorithms to see if we can further improve our predictive accuracy. The following summarizes some of the advantages and disadvantages discussed regarding MARS modeling: FIXME: refine this section Advantages: Accurate if the local linear relationships are correct. Quick computation. Can work well even with large and small data sets. Provides automated feature selection. The non-linear relationship between the features and response are fairly intuitive. Disadvantages: Not accurate if the local linear relationships are correct. Typically not as accurate as more advanced non-linear algorithms (random forests, gradient boosting machines). The earth package does not incorporate more advanced spline features (i.e. Piecewise cubic models). Missing values must be pre-processed. 10.8 Learning more This will get you up and running with MARS modeling. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more: An Introduction to Statistical Learning, Ch. 7 Applied Predictive Modeling, Ch. 7 Elements of Statistical Learning, Ch. 5 Notes on the earth package by Stephen Milborrow References "],
["RF.html", "Chapter 11 Random Forests 11.1 Prerequisites 11.2 Decision trees 11.3 Bagging 11.4 Random forests 11.5 Fitting a basic random forest model 11.6 Tuning 11.7 Feature interpretation 11.8 Attrition data 11.9 Final thoughts 11.10 Learning more", " Chapter 11 Random Forests The previous chapters covered models where the algorithm is based on a linear expansions in simple basis functions of the form \\[ \\sum_{i=1}^p\\beta_ih_i\\left(\\boldsymbol{x}\\right), \\tag{11.1} \\] where the \\(\\beta_i\\) are unknown coefficients to be estimated and the \\(h_i\\left(\\cdot\\right)\\) are transformations applied to the features \\(\\boldsymbol{x}\\). For ordinalry linear regression (with or without regularization), these transformations are supplied by the user; hence, these are parametric models. For example, the prediction equation \\(f\\left(\\boldsymbol{x}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 + \\beta_4x_1^2\\) has \\[h_1\\left(\\boldsymbol{x}\\right) = x_1 \\quad \\text{(main effect})\\] \\[h_2\\left(\\boldsymbol{x}\\right) = x_2 \\quad \\text{(main effect})\\] \\[h_3\\left(\\boldsymbol{x}\\right) = x_2x_4 \\quad \\text{(two-way interaction effect)}\\] \\[h_2\\left(\\boldsymbol{x}\\right) = x_2^2 \\quad \\text{(quadratic effect})\\] MARS, on the other hand, uses a specific algorithm to find the transformations to use automatically; hence, MARS is a nonparametric model. Tree-based models, are also nonparametric, but they work very differently. Tree-based models use algorithms to partition the feature space into a number of smaller (non-overlapping) regions based on a set of splitting rules and then fits a simpler model (e.g., a constant) in each region. Such divide-and-conquor methods (e.g., a single decision tree) can produce simple rules that are easy to interpret and visualize with tree diagrams. Although fitted tree-based models can still be written as a linear expansions in simple basis functions (here the basis functions define the feature space partitioning), there is no benfit to doing so as other techniques, like tree diagrams, are better and conveying the information. Simple decision trees typically lack in predictive performance compared to more complex algorithms like neural networks and MARS. More sophisticated tree-based models, such as random forests and gradient boosting machines, are less interpretable, but tend to have very good predictive accuracy. This chapter will get you familiar with the basics behind decision trees, and two ways inwhich to combine them into a more accurate ensemble; namely, bagging and random forests. In the next chapter, we‚Äôll cover (stochastic) gradient boosting machines, which is another powerful way of combining decision trees into a more accurate ensemble. 11.1 Prerequisites For this chapter we‚Äôll use the following packages: library(caret) # for classification and regression training library(randomForest) # for Breiman and Cutler&#39;s random forest library(ranger) # for fast implementation of random forests library(rpart) # for fitting CART-like decision trees library(rpart.plot) # for flexible decision tree diagrams library(rsample) # for data splitting library(pdp) # for partial dependence plots library(vip) # for variable importance plots To illustrate the various concepts we‚Äôll use the Ames Housing data (regression); however, at the end of the chapter we‚Äôll also apply fit a random forest model to the employee attrition data (classification). # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 11.2 Decision trees There are many methodologies for constructing decision trees but the most well-known is the classification and regression tree (CART¬©) algorithm proposed in Breiman (1984). Basic decision trees partition the training data into homogenious subgroups and then fit a simple constant in each (e.g., the mean of the within group response values for regression). The partitioning is achieved by recursive binary partitions formed by asking yes-or-no questions about each feature (e.g., is age &lt; 18?). This is done a number of times until a suitable stopping critera is satiscfied (e.g., a maximum depth of the tree is reached). After all the partitioning has been done, the model predicts the output based on (1) the average response values for all observations that fall in that subgroup (regression problem), or (2) the class that has majority representation (classification problem). For classification, predicted probabilites can be obtained using the proportion of each class within the subgroups. The basic idea behind decision trees is to ask simple yes-or-no questions about each feature in order partion the training data into subgroups with similar response rates. Ideally, the repsonses within each subgroup will be as similar or homegenous as possible, while responses across subgroups will be as different or as heterogenous as possible. 11.2.1 A simple regression tree example For example, suppose we want to use a decision tree to predict the miles per gallon a car will average (mpg) based on two features: the number of cylinders (cyl) and the horsepower (hp); such data are available in the mtcars data frame which is part of the standard datasets package. A (regression) tree is built using the rpart package (Therneau and Atkinson 2018) (see the code chunk below and Figure ?? which was produced using the rpart.plot package (Milborrow 2018)), and all the training data are passed down this tree. Whenever an obervation reaches a partiular node in the tree (i.e., a yes-or-no question about one of the features; in this case, cyl or hp), it proceeds either to the left (if the answer is ‚Äúyes‚Äù) or to the right (if the answer is ‚Äúno‚Äù). This tree has only three terminal nodes. To start traversing the tree, all observations that have 6 or 8 cylinders go to the left branch, all other observations proceed to the right branch. Next, the left branch is further partitioned by hp. Of all the observations with 6 or 8 cylinders with hp equal to or greater than 192 proceed to the left branch; those with less than 192 hp proceed to the right. These branches lead to terminal nodes or leafs which contain the predicted response value; in this case, the average mpg of cars that fall within that terminal node. In short, cars with less than 5 cylinders (region \\(R_1\\)) average 27 mpg, cars with cyl \\(\\ge 5\\) and hp \\(&lt; 193\\) (region \\(R_2\\)) average 18 mpg, and all other cars (region \\(R_3\\)) in the training data average 13 mph. This is sumamrized in the tree diagram in Figure ?? tree &lt;- rpart(mpg ~ cyl + hp, data = mtcars) # CART-like regression tree rpart.plot(tree) # tree diagram Figure 11.1: Using a CART-like decision tree to predict mpg based on cyl and hp. Using a linear combination in simple basis functions, we can write the predicted mpg as \\[ \\widehat{mpg} = 27 \\cdot I\\left(cyl &lt; 5\\right) + 18 \\cdot I\\left(cyl \\ge 5 \\text{ and } hp &lt; 193 \\right) + 13 \\cdot I\\left(cyl \\ge 5 \\text{ and } hp \\ge 193\\right), \\] where \\(I\\left(\\cdot\\right)\\) is an indicator function that evaluates to one if its argument is true and zero otherwise. The coefficients (i.e., 27, 18, 13) correspond to the average response value within the respective region. While this simple example illustrates that decision trees are estimating a model of the same form as equation (11.1), the results are more easily interpreted in the form of a tree diagram like in Figure ??. 11.2.2 Deciding on splits FIXME: Continue with the previous example? How do decision trees partition the data into nonoverlapping regions? In particular, how did the tree in Figure ?? decide which variables to split on and which split points to use? The answer depends on the tree algorithm used and therther or not context is classification or regression. For CART-like decision trees (like those discussed in this book and implemented in rpart), the partitioning of the feature space is done in a top-down, greedy fashion. This means that any partion in the tree depends on the previous partitions. But how are these partions made? The algorithm begins with the entire training data set and searches every distinct value of every input variable to find the ‚Äúbest‚Äù feature/split combination that partitions the data into two regions (\\(R_1\\) and \\(R_2\\)) such that the overall error is minimized (typically, SSE for regression problems, and cross-entropy or the Gini index for classification problems‚Äìsee Section 6.4.7). Having found the best feature/split combination, the data are partitioned into two regions and the splitting process is repeated on each of the two regions (hence the name binary recursive partioning. This process is continued until some stopping criterion is reached (e.g., a mxaimu depth is reached or the tree becomes ‚Äútoo complex‚Äù). What results is, typically, a very deep, complex tree that may produce good predictions on the training set, but is likely to generalize well to new unseen data (i.e., overfitting), leading to poor generalization performance. To illustrate, we print the struture of the previous regression tree below. print(tree) ## n= 32 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 32 1126.00 20.09 ## 2) cyl&gt;=5 21 198.50 16.65 ## 4) hp&gt;=192.5 7 28.83 13.41 * ## 5) hp&lt; 192.5 14 59.87 18.26 * ## 3) cyl&lt; 5 11 203.40 26.66 * The root node refers to all of the training data. In this node, the mean response is 20.0906 and the SSE is sse &lt;- function(x) sum((x - mean(x))^2) sse(mtcars$mpg) # see the row labeled 1) ## [1] 1126 The feature/splint combination that gives the largest rediction to this SSE is defined by the regions cyl&gt;=5 and cyl&lt; 5. The resulting SSEs within these regions are sse(mtcars[mtcars$cyl &gt;= 5, ]$mpg) # see the row labeled 2) ## [1] 198.5 sse(mtcars[mtcars$cyl &lt; 5, ]$mpg) # see the row labeled 3) ## [1] 203.4 The algorithm further split the region defined by cyl&gt;=5 into to further regions. The feature/split combination that gave the biggest reduction to that node‚Äôs SSE (i.e., 198.47240) is defined by the regions hp&gt;=192.5 and hp&lt; 192.5. The resulting SSEs within these regions are sse(mtcars[mtcars$cyl &gt;= 5 &amp; mtcars$hp &gt;= 192.5, ]$mpg) # see the row labeled 4) ## [1] 28.83 sse(mtcars[mtcars$cyl &gt;= 5 &amp; mtcars$hp &lt; 192.5, ]$mpg) # see the row labeled 5) ## [1] 59.87 At this point, a stopping criteria was reached (in this case, the minimum number of observations that must exist in a node in order for a split to be attempted) and the tree algorithm stopped partitioning the data. For a list of all the stopping criteria, see ?rpart::rpart.control. 11.3 Bagging The major drawback of decision trees is that they are not typically as accurate as current state-of-the-art ML algorithms like neural networks. However, decision trees to have a number of desirable properties which are listed in Table X below. The idea behind the ensembles of decision trees discussed in this chapter and the next is to improve the predictive performance of trees while retaining most of the other properties. Table X. A comparison of binary recursive partitioning and neural networks. Property Recursive partitioning Neural networks Naturally handles numeric and categorical variables ‚úÖ ‚ùå Naturally handles missing values ‚úÖ ‚ùå Robust to features with outliers ‚úÖ ‚ùå Insensitive to monotone transformations of features ‚úÖ ‚ùå Computational scalability ‚úÖ ‚ùå Ability to deal with irrelevant inputs ‚úÖ ‚ùå Interpretibility ‚úÖ ‚ùå Predictive accuracy ‚ö†Ô∏è ‚úÖ As discussed in Friedman, Hastie, and Tibshirani (2001), the key to accuracy is low bias and low variance. Trees are naturally high-variance models; a small change in the training data can lead to substantial different trees. Although pruning the tree (i.e., determining a nested sequence of subtrees by recursively snipping off the least important splits) helps reduce the variance14 of a single tree (i.e., by helping to avoid overfitting), there are methods that exploit this variance in a way that can significantly improve performance over and above that of single trees. Bootstrap aggregating (bagging) (Breiman 1996) is one such approach. Bagging creates an ensemble[^Combining multiple models is referred to as ensembling.] of decision trees with low bias and high variance. The bias of each tree is kept at a minimum by constructing overgrown decision trees (i.e., no pruning). In other words, the tree models constructed in bagging are intentionally overfitting the training data. The variance of the final predictions is reduced by averaging (regression) or popular vote (classification). In regression, for example, the prediction of a new observation is obtained by averaging the predictions from each individual tree. To construct bagger model of size \\(B\\), we follow three simple steps: Create \\(B\\) bootstrap replciates of the original training data (Section 4.6) by selecting rows with replacement For each bootstrap sample, train a single, unpruned decision tree Average individual predictions from each tree to create an overall average predicted value (regression) or use a popular vote (classification) Figure 11.2: The bagging process. Technically, bagging can is a general algorithm that can be applied to any regression or classification algorithm; however, it provides the greatest improvement for models that are adaptive and have high variance. For example, more stable parametric models, such as linear regression and MARS, tend to experience less improvement in predictive performance with bagging. Although bagging trees can help reduce the variance of a single tree‚Äôs prediction and improve predictive performance, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to the fact that stronger relationships appear at the top. For example, if we create six decision trees with different bootstrapped samples of the Boston housing data, we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both lstat and rm driving the first few splits. This between-tree correlation limits the effect of averaging to reduce the variance of the overall ensemble. In order to reduce variance further, we need to take an additiaonl step to help de-correlate the trees in the ensemble. Figure 11.3: Six bagged decision trees fit to the Boston housing data set. 11.4 Random forests Random forests are an extension of bagging and injects more randomness into the tree-growing process. Random forests achieve this in two ways: Bootstrap: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and somewhat decorrelates them. Split-variable randomization: each time a split is to be performed, the search for the split variable is limited to a random subset of m of the p variables. Typical default values for \\(m\\) are \\(m = \\frac{p}{3}\\) (regression problems) and \\(m = \\sqrt{p}\\) for classification models. However, this should be considered a tuning parameter. When \\(m = p\\), the randomization amounts to using only step 1 and is the same as bagging. The basic algorithm for a random forest model can be generalized to the following: 1. Given training data set 2. Select number of trees to build (ntrees) 3. for i = 1 to ntrees do 4. | Generate a bootstrap sample of the original data 5. | Grow a regression or classification tree to the bootstrapped data 6. | for each split do 7. | | Select m variables at random from all p variables 8. | | Pick the best variable/split-point among the m 9. | | Split the node into two child nodes 10. | end 11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune) 12. end Since the algorithm randomly selects a bootstrap sample to train on and predictors to use at each split, tree correlation will be lessened beyond bagged trees. 11.4.1 OOB error vs. test set error One benefit of bagging (and thus also random forests) is that, on average, a bootstrap sample will contain 63% of the training data. This leaves about 37% of the data out of the bootstrapped sample. We call this the out-of-bag (OOB) sample. We can use the OOB observations to estimate the model‚Äôs accuracy, creating a natural cross-validation process, which allows you to not need to sacrifice any of your training data to use for validation. This makes identifying the number of trees required to stablize the error rate during tuning more efficient; however, as illustrated below some difference between the OOB error and test error are expected. Figure 11.4: Random forest out-of-bag error versus validation error. Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, you‚Äôd want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation. 11.5 Fitting a basic random forest model There are over 20 random forest packages in R.15 To demonstrate the basic implementation we illustrate the use of the randomForest package (Breiman et al. 2018), the oldest and most well known implementation of the Random Forest algorithm in R. However, as your data set grows in size randomForest does not scale well (although you can parallelize with foreach). randomForest() can use the formula or separate x, y matrix notation for specifying our model. Below we apply the default randomForest model using the formulaic specification. The default random forest performs 500 trees and \\(\\frac{features}{3} = 26\\) randomly selected predictor variables at each split. Averaging across all 500 trees provides an OOB \\(MSE = 661089658\\) (\\(RMSE = \\$25,711\\)). # for reproduciblity set.seed(123) # default RF model rf1 &lt;- randomForest( formula = Sale_Price ~ ., data = ames_train ) rf1 ## ## Call: ## randomForest(formula = Sale_Price ~ ., data = ames_train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 26 ## ## Mean of squared residuals: 661089658 ## % Var explained: 89.8 Plotting the model will illustrate the OOB error rate as we average across more trees and shows that our error rate stabalizes with around 100 trees but continues to decrease slowly until around 300 or so trees. plot(rf1) Figure 11.5: OOB error (MSE) as a function of the number of trees. We see the MSE reduces quickly for the first 100 trees and then slowly thereafter. We want to make sure that we are providing enough trees so that our OOB error has stabalized or flatlined. The plotted error rate above is based on the OOB sample error and can be accessed directly at rf1$mse. Thus, we can find which number of trees provides the lowest error rate, which is 447 trees providing an average home sales price error of $25,649. # number of trees with lowest MSE which.min(rf1$mse) ## [1] 447 # RMSE of this optimal random forest sqrt(rf1$mse[which.min(rf1$mse)]) ## [1] 25649 Random forests are one of the best ‚Äúout-of-the-box‚Äù machine learning algorithms. They typically perform remarkably well with very little tuning required. As illustrated above, we were able to get an RMSE of $25,649 without any tuning which is nearly as good as the best, fully tuned model we‚Äôve explored thus far. However, we can still seek improvement by tuning hyperparameters in our random forest model. 11.6 Tuning Compared to the algorithms explored in the previous chapters, random forests have more hyperparameters to tune. However, compared to gradient boosting machines and neural networks, which we explore in future chapters, random forests are much easier to tune. Typically, the primary concern when starting out is tuning the number of candidate variables to select from at each split. The two primary tuning parameters you should always tune in a random forest model are: Number of trees as you want to ensure you apply enough trees to minimize and stabalize the error rate. Number of candidate variables to select from at each split. However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present: Number of trees: We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets. Number of variables to randomly sample as candidates at each split: Commonly referred to as ‚Äúmtry‚Äù. When mtry \\(=p\\) the model equates to bagging. When mtry \\(=1\\) the split variable is completely random, so all variables get a chance but can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to p. Sample size to train on: The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range. Minimum number of samples within the terminal nodes: Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and larger node size results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data). Maximum number of terminal nodes: Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees. Split rule:As stated in the introduction, the most traditional splitting rules are based on minimizing the variance or MSE across the terminal nodes for regression problems and Cross-entropy or Gini index for classification problems. However, additional splitrules have been developed that can offer improved predictive accuracy. For example, the extra trees split rule chooses cut-points fully at random and uses the whole learning sample (rather than a bootstrap replica) to grow the trees (Geurts, Ernst, and Wehenkel 2006). Tuning a larger set of hyperparameters requires a larger grid search than we‚Äôve performed thus far. Unfortunately, this is where randomForest becomes quite inefficient since it does not scale well. Instead, we can use ranger (Wright, Wager, and Probst 2018) which is a C++ implementation of Brieman‚Äôs random forest algorithm and, as the following illustrates, is over 27 times faster than randomForest. # names of features features &lt;- setdiff(names(ames_train), &quot;Sale_Price&quot;) # randomForest speed system.time( ames_randomForest &lt;- randomForest( formula = Sale_Price ~ ., data = ames_train, ntree = 500, mtry = floor(length(features) / 3) ) ) ## user system elapsed ## 36.75 0.12 36.94 # ranger speed system.time( ames_ranger &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = floor(length(features) / 3) ) ) ## user system elapsed ## 7.174 0.142 1.115 11.6.1 Tuning via ranger There are two approaches to tuning a ranger model. The first is to tune ranger manually using a for loop. To perform a manual grid search, first we want to construct our grid of hyperparameters. We‚Äôre going to search across 48 different models with varying mtry, minimum node size, sample size, and trying different split rules. # create a tuning grid hyper_grid &lt;- expand.grid( mtry = seq(20, 35, by = 5), min.node.size = seq(3, 9, by = 3), sample.fraction = c(.632, .80), splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;), OOB_RMSE = 0 ) dim(hyper_grid) ## [1] 48 5 We loop through each hyperparameter combination and apply 500 trees since our previous examples illustrated that 500 was plenty to achieve a stable error rate. Also note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. Our OOB RMSE ranges between ~25,900-28,500. Our top 10 performing models all have RMSE values right around 26,000 and the results show that models with larger sample sizes (80%) and a variance splitrule perform best. However, no definitive evidence suggests that certain values of mtry or min.node.size are better than other values. This grid search took 69 seconds to complete. for(i in seq_len(nrow(hyper_grid))) { # train model model &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = hyper_grid$mtry[i], min.node.size = hyper_grid$min.node.size[i], sample.fraction = hyper_grid$sample.fraction[i], splitrule = hyper_grid$splitrule[i], seed = 123 ) # add OOB error to grid hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error) } hyper_grid %&gt;% dplyr::arrange(OOB_RMSE) %&gt;% head(10) ## mtry min.node.size sample.fraction splitrule ## 1 20 3 0.8 variance ## 2 25 3 0.8 variance ## 3 20 6 0.8 variance ## 4 20 9 0.8 variance ## 5 30 3 0.8 variance ## 6 25 6 0.8 variance ## 7 30 6 0.8 variance ## 8 35 3 0.8 variance ## 9 35 6 0.8 variance ## 10 25 9 0.8 variance ## OOB_RMSE ## 1 25964 ## 2 25981 ## 3 25982 ## 4 26091 ## 5 26123 ## 6 26145 ## 7 26167 ## 8 26176 ## 9 26189 ## 10 26210 However, using this approach does not provide us with a cross validated measure of error. To get a k-fold CV error, we would have to expand our for loop approach or use an alternative approach. One such approach follows. 11.6.2 Tuning via caret The second tuning approach is to use the caret package. caret only allows you to tune some, not all, of the available ranger hyperparameters (mtry, splitrule, min.node.size). However, caret will allow us to get a CV measure of error to compare to our previous models (i.e. regularized regression, MARS). The following creates a similar tuning grid as before but with only those hyperparameters that caret will accept. If you do not know what hyperparameters caret allows you to tune for a specific model you can find that info at https://topepo.github.io/caret/train-models-by-tag.html or with caret::getModelInfo. For example, we can find the parameters available for a ranger with caret::getModelInfo(‚Äúranger‚Äù)\\(ranger\\)parameter. # create a tuning grid hyper_grid &lt;- expand.grid( mtry = seq(20, 35, by = 5), min.node.size = seq(3, 9, by = 3), splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;) ) Tuning with caret provides similar results as our ranger grid search. Both results suggest mtry = 20 and min.node.size = 3. With ranger, our OOB RMSE was 25963.96 and with caret our 10-fold CV RMSE was 25531.47. This grid search took a little over 6 minutes to complete. # cross validated model tuned_rf &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = hyper_grid, num.trees = 500, seed = 123 ) # best model tuned_rf$bestTune ## mtry splitrule min.node.size ## 7 25 variance 3 # plot results ggplot(tuned_rf) Figure 11.6: Cross validated RMSE for the caret grid search. 11.7 Feature interpretation Whereas many of the linear models discussed previously use the standardized coefficients to signal importance, random forests have, historically, applied two different approaches to measure variable importance. Impurity: At each split in each tree, compute the improvement in the split-criterion (MSE for regression Gini for classification). Then average the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in the error metric are considered most important. Permutation: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly ‚Äúshaking up‚Äù of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important. To compute these variable importance measures with ranger, you must include the importance argument. Once you‚Äôve identified the optimal parameter values from the grid search, you will want to re-run your model with these hyperparameter values. # re-run model with impurity-based variable importance rf_impurity &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = 20, min.node.size = 3, sample.fraction = .80, splitrule = &quot;variance&quot;, importance = &#39;impurity&#39;, verbose = FALSE, seed = 123 ) # re-run model with permutation-based variable importance rf_permutation &lt;- ranger( formula = Sale_Price ~ ., data = ames_train, num.trees = 500, mtry = 20, min.node.size = 3, sample.fraction = .80, splitrule = &quot;variance&quot;, importance = &#39;permutation&#39;, verbose = FALSE, seed = 123 ) For both options, you can directly access the variable importance values with model_name$variable.importance. However, here we‚Äôll plot the variable importance using the vip package. Typically, you will not see the same variable importance order between the two options; however, you will often see similar variables at the top of the plots. Consquently, in this example, we can comfortably state that there appears to be enough evidence to suggest that two variables stand out as most influential: Overall_Qual Gr_Liv_Area Looking at the next ~10 variables in both plots, you will also see some commonality in influential variables (i.e. Garage_Cars, Bsmt_Qual, Year_Built, Exter_Qual). p1 &lt;- vip(rf_impurity, num_features = 25, bar = FALSE) + ggtitle(&quot;Impurity-based variable importance&quot;) p2 &lt;- vip(rf_permutation, num_features = 25, bar = FALSE) + ggtitle(&quot;Permutation-based variable importance&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Figure 11.7: Top 25 most important variables based on impurity (left) and permutation (right). To better understand the relationship between these important features and Sale_Price, we can create partial dependence plots (PDPs). If you recall in the linear and regularized regression sections, we saw that the linear model assumed a continously increasing relationship between Gr_Liv_Area and Sale_Price. In the MARS chapter, we saw as homes exceed 2,945 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less than 2,945 square feet. However, in between the knots of a MARS model, the relationship will remain linear. However, the PDP plot below displays how random forest models can capture unique non-linear and non-monotonic relationships between predictors and the target. In this case, Sale_Price appears to not be influenced by Gr_Liv_Area values below 750 sqft or above 3500 sqft. This change in realtionship was not well captured by the prior parametric models. # partial dependence of Sale_Price on Gr_Liv_Area rf_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50) %&gt;% autoplot(rug = TRUE, train = ames_train) Figure 11.8: The mean predicted sale price as the above ground living area increases. Additionally, if we assess the relationship between the Overall_Qual predictor and Sale_Price, we see a continual increase as the overall quality increases. This provides a more comprehensive understanding of the relationship than the results we saw in the regularized regression section (9.5). We see that the largest impact on Sale_Price occurs when houses go from ‚ÄúGood‚Äù overall quality to ‚ÄúVery Good‚Äù. # partial dependence of Sale_Price on Overall_Qual rf_impurity %&gt;% partial(pred.var = &quot;Overall_Qual&quot;, train = as.data.frame(ames_train)) %&gt;% autoplot() Figure 11.9: The mean predicted sale price for each level of the overall quality variable. Individual conditional expectation (ICE) curves (Goldstein et al. 2015) are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently ‚Äústacked‚Äù on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. The plots below show that marginal changes in Gr_Liv_Area have a fairly homogenous effect on our response variable. As Gr_Liv_Area increases, the vast majority of observations show a similar increasing effect on the predicted Sale_Price value. The primary differences is in the magnitude of the increasing effect. However, in the centered ICE plot you see evidence of a few observations that display a different pattern. Some have a higher \\(\\hat y\\) value when Gr_Liv_Area is between 2500-4000 and some have a decreasing \\(\\hat y\\) as Gr_Liv_AreA increases. These results may be a sign of interaction effects and would be worth exploring more closely. # ice curves of Sale_Price on Gr_Liv_Area ice1 &lt;- rf_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.1) + ggtitle(&quot;Non-centered ICE plot&quot;) ice2 &lt;- rf_impurity %&gt;% partial(pred.var = &quot;Gr_Liv_Area&quot;, grid.resolution = 50, ice = TRUE) %&gt;% autoplot(rug = TRUE, train = ames_train, alpha = 0.1, center = TRUE) + ggtitle(&quot;Centered ICE plot&quot;) gridExtra::grid.arrange(ice1, ice2, nrow = 1) Figure 11.10: Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations. Both PDPs and ICE curves should be assessed for the most influential variables as they help to explain the underlying patterns in the data that the random forest model is picking up. 11.8 Attrition data With the Ames data, the random forest models obtained predictive accuracy that was close to our best MARS model, but how about the attrition data? The following performs a grid search across 48 hyperparameter combinations. # get attrition data df &lt;- rsample::attrition %&gt;% dplyr::mutate_if(is.ordered, factor, ordered = FALSE) # Create training (70%) and test (30%) sets for the rsample::attrition data. # Use set.seed for reproducibility set.seed(123) churn_split &lt;- initial_split(df, prop = .7, strata = &quot;Attrition&quot;) churn_train &lt;- training(churn_split) churn_test &lt;- testing(churn_split) # create a tuning grid hyper_grid &lt;- expand.grid( mtry = seq(3, 18, by = 3), min.node.size = seq(1, 10, by = 3), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;) ) # cross validated model tuned_rf &lt;- train( x = subset(churn_train, select = -Attrition), y = churn_train$Attrition, method = &quot;ranger&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = hyper_grid, num.trees = 500, seed = 123 ) # best model tuned_rf$bestTune ## mtry splitrule min.node.size ## 31 12 gini 10 # plot results ggplot(tuned_rf) Figure 11.11: Cross-validated accuracy rate for the 48 different hyperparameter combinations in our grid search. The optimal model uses mtry = 9, splitrule = gini, and min.node.size = 4, which obtained a 10-fold CV accuracy rate of 85.8%. Similar to the MARS model, the random forest model does not improve predictive accuracy over an above the regularized regression model. Min. 1st Qu. Median Mean 3rd Qu. Max. NA‚Äôs Logistic_model 0.8447 0.8533 0.8558 0.8670 0.8900 0.8942 0 Elastic_net 0.8269 0.8662 0.8744 0.8749 0.8900 0.9314 0 MARS_model 0.8039 0.8474 0.8647 0.8651 0.8811 0.9314 0 RF_model 0.8235 0.8410 0.8654 0.8612 0.8802 0.9020 0 11.9 Final thoughts Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. However, as we illustrated in this chapter, random forests are not guaranteed to improve predictive accuracy over and above linear models and their cousins. The following summarizes some of the advantages and disadvantages discussed regarding random forests modeling: TODO: may need to better tie in some of these advantages and disadvantages throughout the chapter. Advantages: Typically have very good performance. Remarkably good ‚Äúout-of-the box‚Äù - very little tuning required. Built-in validation set - don‚Äôt need to sacrifice data for extra validation. Does not overfit. No data pre-processing required - often works great with categorical and numerical values as is. Robust to outliers. Handles missing data - imputation not required. Provide automatic feature selection. Disadvantages: Can become slow on large data sets. Although accurate, often cannot compete with the accuracy of advanced boosting algorithms. Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.). 11.10 Learning more The literature behind random forests are rich and we have only touched on the fundamentals. To learn more I would start with the following resources listed in order of complexity: An Introduction to Statistical Learning Applied Predictive Modeling Computer Age Statistical Inference The Elements of Statistical Learning References "],
["references.html", "References", " References "],
["appendix-data.html", "Chapter 12 (APPENDIX) Appendix {-}", " Chapter 12 (APPENDIX) Appendix {-} "],
["data-sets.html", "Data sets Ames Iowa housing data Employee attrition data", " Data sets We have strived to use real data sets throughout the book. An introduction to each data set, as well as it‚Äôs source and how to import it, is given in the following subsections. Ames Iowa housing data Cock (2011) describes a data set containing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables involved in assessing home values. These data offer a contemporary alternative to the often used Boston housing data described in Harrison Jr and Rubinfeld (1978). The Ames housing data, which we refer to as simply the ames data, are available in the AmesHousing package (Kuhn 2017) which is available from CRAN: install.packages(&quot;AmesHousing&quot;) The raw data are also available from Kaggle: https://www.kaggle.com/c/house-prices-advanced-regression-techniques. In the code chunk below, we use the make_ames() function from AmesHousing to create a processed version of the data. For full details on the difference between the processed and raw versions of the ames data, see the help file ?AmesHousing::make_ames. ames &lt;- AmesHousing::make_ames() dim(ames) ## [1] 2930 81 table(sapply(ames, class)) ## ## factor integer numeric ## 46 23 12 The function make_ames() returns a &quot;tibble&quot; object, rather than just an R data frame. For more information on tibbles, see the corresponding vignette available in the tibble package (M√ºller and Wickham 2018) browseVignettes(package = &quot;tibble&quot;). Running the code chunk above, we see that there are 2930 observations on 81 variables (46 are factors, 23 are integer valued, and 12 are numeric). Employee attrition data Due to the continuing concerns organizations have with attracting and retaining top talent, the IBM Watson Analytics Lab published an employee attrition data set that allows analysts to explore factors that lead to employee attrition and investigate important questions such as ‚Äòshow me a breakdown of distance from home by job role and attrition‚Äô or ‚Äòcompare average monthly income by education and attrition‚Äô. The employee attrition data, which we refer to as simply the churn data, are available in the rsample package (Kuhn and Wickham 2017) which is available from CRAN: install.packages(&quot;rsample&quot;) The raw data are also available from IBM Watson Analytics Lab: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/. In the code chunk below, we import the attrition data from rsample. churn &lt;- rsample::attrition dim(churn) ## [1] 1470 31 dplyr::glimpse(churn) ## Observations: 1,470 ## Variables: 31 ## $ Age &lt;int&gt; 41, 49, 37, 33, 2... ## $ Attrition &lt;fct&gt; Yes, No, Yes, No,... ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Tr... ## $ DailyRate &lt;int&gt; 1102, 279, 1373, ... ## $ Department &lt;fct&gt; Sales, Research_D... ## $ DistanceFromHome &lt;int&gt; 1, 8, 2, 3, 2, 2,... ## $ Education &lt;ord&gt; College, Below_Co... ## $ EducationField &lt;fct&gt; Life_Sciences, Li... ## $ EnvironmentSatisfaction &lt;ord&gt; Medium, High, Ver... ## $ Gender &lt;fct&gt; Female, Male, Mal... ## $ HourlyRate &lt;int&gt; 94, 61, 92, 56, 4... ## $ JobInvolvement &lt;ord&gt; High, Medium, Med... ## $ JobLevel &lt;int&gt; 2, 2, 1, 1, 1, 1,... ## $ JobRole &lt;fct&gt; Sales_Executive, ... ## $ JobSatisfaction &lt;ord&gt; Very_High, Medium... ## $ MaritalStatus &lt;fct&gt; Single, Married, ... ## $ MonthlyIncome &lt;int&gt; 5993, 5130, 2090,... ## $ MonthlyRate &lt;int&gt; 19479, 24907, 239... ## $ NumCompaniesWorked &lt;int&gt; 8, 1, 6, 1, 9, 0,... ## $ OverTime &lt;fct&gt; Yes, No, Yes, Yes... ## $ PercentSalaryHike &lt;int&gt; 11, 23, 15, 11, 1... ## $ PerformanceRating &lt;ord&gt; Excellent, Outsta... ## $ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, M... ## $ StockOptionLevel &lt;int&gt; 0, 1, 0, 0, 1, 0,... ## $ TotalWorkingYears &lt;int&gt; 8, 10, 7, 8, 6, 8... ## $ TrainingTimesLastYear &lt;int&gt; 0, 3, 3, 3, 3, 2,... ## $ WorkLifeBalance &lt;ord&gt; Bad, Better, Bett... ## $ YearsAtCompany &lt;int&gt; 6, 10, 0, 8, 2, 7... ## $ YearsInCurrentRole &lt;int&gt; 4, 7, 0, 7, 2, 7,... ## $ YearsSinceLastPromotion &lt;int&gt; 0, 1, 0, 3, 2, 3,... ## $ YearsWithCurrManager &lt;int&gt; 5, 7, 0, 0, 2, 6,... Running the code chunk above, we see that there are 1470 observations on 31 variables, which consist of a mixture of integers, factors, and ordered factors data types). "]
]
