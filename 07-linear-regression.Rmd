# Linear regression {#linear-regression}

```{r ch7-setup, include=FALSE}

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  warning = FALSE, 
  message = FALSE, 
  collapse = TRUE, 
  fig.align = "center",
  fig.height = 3.5
)
```

Linear regression is a very simple approach for supervised learning, has been around for a long time, and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches: as we will see in later chapters, many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated. This chapter introduces linear regression with an emphasis on predictive purposes rather than inferential purposes (see @faraway2016linear for discussion of linear regression in R with an inferential emphasis).

## Prerequisites

For this section we will use the following packages:

```{r 07-pkgs, message=FALSE}
library(tidyverse)  # data manipulation & visualization
library(rsample)    # data splitting
library(caret)      # regression modeling
library(modelr)     # provides easy pipeline modeling functions
library(broom)      # helps to tidy up model outputs
library(vip)        # variable importance
```

To illustrate linear regression concepts we will use the Ames, IA housing data, where our intent is to predict `Sale_Price`. As discussed in the _Data splitting_ section \@ref(reg-perf-split), we'll set aside 30% of our data as a test set to assess our generalizability error.

```{r 07-data-import}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
train <- training(ames_split)
test  <- testing(ames_split)
```

## Simple linear regression

*Simple linear regression* lives up to its name: it is a very straightforward approach for predicting a continuous quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$. 

Consider our housing data, suppose we wish to model the linear relationship between the year the house was built (`Year_Built`) and sale price (`Sale_Price`).  We can write this as Equation \@ref(eq:lm) and we often state this as *regressing Y onto X*.

\begin{equation}
(\#eq:lm)
  Y = \beta_0 + \beta_1X + \epsilon,
\end{equation}


where $Y$ represents `Sale_Price`, $X$ represents `Year_Built`, $\beta_0$ and $\beta_1$ represent two unknown constants (commonly referred to as coefficients or parameters) that represent the intercept and slope terms in the linear model, and $\epsilon$ is a mean-zero random error term. To estimate the coefficient parameters ($\beta_i$), the linear regression algorithm will identify the best-fit linear relationship that fits the data well.  There are multiple ways to measure "best-fit" but the most common involves minimizing the *least squares* criterion also referred to as *ordinary least squares* (OLS).  The OLS criterion identifies the "best-fit" line by minimizing the error between the best-fit line (the predicted sales price) and the actual sale price values.  

To perform an OLS regression model in R we can use `lm`. 

```{r model1}
# fit model
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = train)
```

Figure \@ref(fig:07-visualize-model1) illustrates this linear model with the best fit linear line. The individual dots represent the actual sales price, and the vertical grey lines represent the individual errors for each observation. The OLS criterion identifies the best-fit line that minimizes the residual sum of squared errors (RSS), which follows Equation \@ref(eq:rss)

\begin{equation}
(\#eq:rss)
  RSS = \sum^n_{i=1}(y_i - f(x_i))^2,
\end{equation}

where $y_i$ is the i$^{th}$ value of the explanatory variable, and $f(x_i)$ is the predicted value of $y_i$ (also termed ($\hat y_i$)). The left plot of \@ref(fig:07-visualize-model1) illustrates this model's fit across homes that have between 1000-2000 square feet of living above ground whereas the right plot shows the model's fit across the entire training set.  

```{r 07-visualize-model1, fig.align='center', fig.width=10, fig.height=3.5, echo=FALSE, fig.cap="The least squares fit for regressing sale price onto above ground square footage for the the Ames housing data. The least squares criterion finds the 'best-fit' line that minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (blue line). _Left_: model fit for homes with 1000-2000 square feet. _Right_: model fit across all observations."}
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = train)

p1 <- train %>%
  filter(Gr_Liv_Area >= 1000, Gr_Liv_Area <= 2000) %>%
  add_predictions(model1) %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_smooth(se = FALSE, method = "lm") +
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = pred), alpha = .5) +
  geom_point(color = "red", size = 1, alpha = .4) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Subset of data: 1000 < Gr_Liv_Area < 2000")

p2 <- train %>%
  add_predictions(model1) %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_smooth(se = FALSE, method = "lm") +
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = pred), alpha = .5) +
  geom_point(color = "red", size = 1, alpha = .4) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Full data set")

grid.arrange(p1, p2, nrow = 1)
```

To identify the coefficients that minimize the RSS, OLS selects the optimal estimated coefficients ($\hat \beta_0$ and $\hat \beta_1$). Its been proven that the coefficients that minimize the RSS are:

\begin{align}
(\#eq:coefficients)
  \hat \beta_1 = \frac{\sum^n_{i=1}(x_i - \bar x)(y_i - \bar y)}{\sum^n_{i=1}(x_i - \bar x)^2},
  \hat \beta_0 = \bar y - \hat \beta_1 \bar x
\end{align}

where $\bar y = \frac{1}{n}\sum^n_{i=1}y_i$ and $\bar x = \frac{1}{n}\sum^n_{i=1}x_i$ are the sample means. `summary` allows us to access the coeffients for our model along with other model results. For this simple model, our coefficients are $\hat \beta_0 = 17797.0728$ and $\hat \beta_1 = 108.0344$. In other words, according to this approximation, an additional one square foot of above ground living space of a house is associated with approximately an additional \$108 in selling price.  This ease in interpreting the relationship between the sale price and square footage with a single number is what makes linear regression such a intuitive and popular modeling tool.

```{r}
summary(model1)
```

It's also important to understand if these coefficients are statistically significant. In other words, can we state these coefficients are statistically different then 0?  To do that we can start by assessing the standard error (SE). The SE for $\beta_0$ and $\beta_1$ are computed with:

\begin{align}
(\#eq:coef-se)
SE(\beta_0)^2 = \sigma^2\bigg[\frac{1}{n}+\frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2} \bigg],
\quad SE(\beta_1)^2 = \frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
\end{align}

where $\sigma^2 = Var(\epsilon)$.  We see that our model results provide the SE ($SE_{\beta_1}=2.47$).  We can use the SE to compute the 95% confidence interval for the coefficients:

\begin{equation}
(\#eq:coef-interval)
 \beta_1 \pm 2 \cdot SE(\beta_1)
\end{equation}

To get this information in R we can simply use:

```{r}
confint(model1)
```

Our results show us that our 95% confidence interval for $\beta_1$ (Gr_Liv_Area) is [103.191, 112.8779].  Thus, since zero is not in this interval we can conclude that as the `Gr_Liv_Area` increases by one square foot we can expect `Sale_Price` to increase by approximately \$103--\$113. This is also supported by the *t-statistic* provided by our results, which are computed by

\begin{equation}
(\#eq:tstat)
  t=\frac{\beta_1 - 0}{SE(\beta_1)}
\end{equation}

which measures the number of standard deviations that $\beta_1$ is away from 0.  Thus a large *t-statistic* such as ours will produce a small *p-value* (a small p-value indicates that it is unlikely to observe such a substantial association between the predictor variable and the response due to chance).  Thus, we can conclude that a relationship between `Gr_Liv_Area` and `Sale_Price` exists.

## Multiple linear regression {#multi-lm}

However, in practice we often have more than one predictor. For example, in the Ames housing data, we may wish to understand if above ground square footage (`Gr_Liv_Area`) _and_ the year the house was built (`Year_Built`) are related to sales price (`Sale_Price`).  We can extend the simple linear regression model so that it can directly accommodate multiple predictors; this is referred to as _multiple linear regression_ and is represented by Equation \@ref(eq:mlm) and illustrated in Figure \@ref(fig:07-visualize-model2).

\begin{equation}
(\#eq:mlm)
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,
\end{equation}


where now $X_1$ represents `Gr_Liv_Area` and $X_2$ represents `Year_Built`.

```{r 07-visualize-model2, fig.align='left', fig.width=10, fig.height=6, echo=FALSE, fig.cap="In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane)."}
library(dplyr)
library(plotly)

# get coordinates of sub sample of data
y_values <- round(train$Sale_Price, 3)
x1_values <- round(train$Gr_Liv_Area, 3)
x2_values <- round(train$Year_Built, 3)

# Construct x and y grid elements
x_grid <- seq(from = min(x1_values), to = max(x1_values), length = 50)
y_grid <- seq(from = min(x2_values), to = max(x2_values), length = 50)

# Construct z grid by computing
# 1) fitted beta coefficients
# 2) fitted values of outer product of x_grid and y_grid
# 3) extracting z_grid (matrix needs to be of specific dimensions)
beta_hat <- train %>% 
  lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = .) %>% 
  coef()
fitted_values <- crossing(y_grid, x_grid) %>% 
  mutate(z_grid = beta_hat[1] + beta_hat[2]*x_grid + beta_hat[3]*y_grid)
z_grid <- fitted_values %>% 
  pull(z_grid) %>%
  matrix(nrow = length(x_grid)) %>%
  t()

# plot
plot_ly() %>%
  # 3D scatterplot:
  add_markers(
    x = x1_values,
    y = x2_values,
    z = y_values,
    opacity = .3
    ) %>%
  # Regression plane:
  add_surface(
    x = x_grid,
    y = y_grid,
    z = z_grid,
    showscale = FALSE
  ) %>%
  # Axes labels and title:
  layout(
    scene = list(
      zaxis = list(title = "y: Sale_Price"),
      yaxis = list(title = "x2: Year_Built"),
      xaxis = list(title = "x1: Gr_Liv_Area")
    )
  ) %>%
  hide_guides()
```

This model in R is built by simply adding the `Year_Built` variable to our original model. The below results illustrate that the best-fit plane identified in Figure \@ref(fig:07-visualize-model2) resulted in $\hat \beta_1 = 91.73$ and $\hat \beta_2 = 1098$. In other words, according to this approximation, an additional one square foot of above ground square footage is now associated with approximately an additional \$92 in selling price when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of \$1,098 in selling price when holding the main floor square footage constant.

```{r model2}
# fit model
model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = train)
summary(model2)
```

We can continue to add predictors to our multiple regression and generalize the multiple regression model to Equation \@ref(eq:generalmlm) where we have *p* distinct predictors.

\begin{equation}
(\#eq:generalmlm)
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon,
\end{equation}

Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS.  The below creates a third model where we use all features in our data set to predict `Sale_Price`.

```{block, type="tip"}
The dot in `Sale_Price ~ .` signals to regress `Sale_Price` onto all other variables in your data set.
```


```{r model3}
model3 <- lm(Sale_Price ~ ., data = train)
glance(model3)
```

## Assessing Model Accuracy

We've fit three models, but the question remains, which model is "best". To get a good assessment of this accuracy, we want to use cross-validation as discussed in Section \@ref(cv).  We can use the `caret::train` function to apply a linear model (`method = "lm"`).  The benefit of __caret__ is that it provides built-in cross validation capabilities whereas the `lm` function does not.  The following shows an average root mean square error (RMSE) of 56873 across our 10 cross validation folds.  How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about \$56,873 off from the actual sale price. 


```{r model1-accuracy}
# reproducible CV results
set.seed(123)

# use caret package to train 10-fold cross-validated model
cv_model1 <- train(
  Sale_Price ~ Gr_Liv_Area, 
  data = train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

cv_model1
```

We can perform cross validation on the other two models.  Extracting the results for each model we see that by adding more information via more predictors, we are able to improve the out-of-sample cross validation performance metrics. Specifically, our average prediction RMSE reduces from __\$56,872__ down to __\$41,438__ for our full model.

```{r mult-models}
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built, 
  data = train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

# extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2, 
  model3 = cv_model3
  )))
```


## Model concerns

As previously stated, linear regression has been a popular modeling tool due to the ease of interpreting the coefficients. However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model.  Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results. 

__1. Linear relationship:__ Linear regression assumes a linear relationship between the predictor and the response variable. When a linear relationship does not hold then the coefficient estimate makes a flawed assumption that a constant relationship holds.  However, as discussed in Chapter \@ref(descriptive), non-linear relationships can be made linear (or near-linear) by applying power transformations to the response and/or predictor.  For example, Figure \@ref(fig:07-linear-relationship) illustrates the relationship between sale price and the year a home was built.  The left plot illustrates the non-linear relationship that exists.  However, we can achieve a near-linear relationship by log transforming sale price; although some non-linearity still exists for older homes.


```{r 07-linear-relationship, fig.align='center', fig.width=8, fig.height=3.5, fig.cap="Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s)."}
p1 <- ggplot(train, aes(Year_Built, Sale_Price)) + 
  geom_point(size = 1, alpha = .4) +
  geom_smooth(se = FALSE) +
  scale_y_continuous("Sale price", labels = scales::dollar) +
  xlab("Year built") +
  ggtitle("Non-transformed variables with a \nnon-linear relationship.")

p2 <- ggplot(train, aes(Year_Built, Sale_Price)) + 
  geom_point(size = 1, alpha = .4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("Sale price", labels = scales::dollar, breaks = seq(0, 400000, by = 100000)) +
  xlab("Year built") +
  ggtitle("Transforming variables can provide a \nnear-linear relationship.")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

__2. Constant variance among residuals:__ Linear regression assumes the variance among error terms ($\epsilon_1, \epsilon_2, ..., \epsilon_p$) are constant (also referred to as homoscedasticity). When residuals are not constant, the _p_-values and confidence intervals of the coefficients are invalid resulting in invalid prediction estimates and confidence intervals.  Similar to the linear relationships assumption, non-constant variance can often be resolved with variable transformations or by including additional predictors.  For example, Figure \@ref(fig:07-homoskedasticity) shows residuals across predicted values for our linear regression models `model1` and `model3`. `model1` displays a classic violation of constant variance with cone-shaped residuals.  However, `model3` appears to have near-constant variance.

```{block, type="tip"}
The `broom::augment` function is an easy way to add model results to each observation (i.e. predicted values, residuals).
```

```{r 07-homoskedasticity, fig.align='center', fig.width=8, fig.height=3.5, fig.cap="Linear regression assumes constant variance among the residuals. `model1` (left) shows definitive signs of heteroskedasticity whereas `model3` (right) appears to have constant variance."}

df1 <- augment(cv_model1$finalModel, train)

p1 <- ggplot(df1, aes(.fitted, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 1",
    subtitle = "Sale_Price ~ Gr_Liv_Area")

df2 <- augment(cv_model3$finalModel, train)

p2 <- ggplot(df2, aes(.fitted, .resid)) + 
  geom_point(size = 1, alpha = .4)  +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 3",
    subtitle = "Sale_Price ~ .")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

__3. No autocorrelation:__ Linear regression assumes the error terms are also independent and uncorrelated. If in fact, there is correlation among the error terms, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be.  For example, the left plot in Figure \@ref(fig:07-autocorrelation) displays the residuals (y-axis) to the observation ID (x-axis) for `model1`.  A clear pattern exists suggesting that information about $\epsilon_1$ provides information about $\epsilon_2$.

This pattern is a result of the data being ordered by neighborhood, which we have not accounted for in this model.  Consequently, the residuals for homes in the same neighborhood are correlated (homes within a neighborhood are typically the same size and can often contain similar features).  Since the `Neighborhood` predictor is included in `model3` (right plot), our errors are no longer correlated.

```{r 07-autocorrelation, fig.align='center', fig.width=8, fig.height=3.5, fig.cap="Linear regression assumes uncorrelated errors. The residuals in `model1` (left) have a distinct pattern suggesting that information about $\\epsilon_1$ provides information about $\\epsilon_2$.  Whereas residuals in `model3` have no signs of autocorrelation."}

df1 <- mutate(df1, id = row_number())
df2 <- mutate(df2, id = row_number())

p1 <- ggplot(df1, aes(id, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 1",
    subtitle = "Correlated residuals.")

p2 <- ggplot(df2, aes(id, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 3",
    subtitle = "Uncorrelated residuals.")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```


__4. More observations than predictors:__ Although not an issue with the Ames housing data, when the number of features exceed the number of observations ($p > n$), the OLS solution matrix is not invertible. This causes significant issues because it means the least-squares estimates are not unique. In fact, there are an infinite set of solutions available so we lose our ability to meaningfully interpret coefficients.  Consequently, to resolve this issue an analyst can remove variables until $p < n$ and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach [@apm, 43-47], it can be cumbersome and prone to errors. Alternatively, we will introduce regularized regression in Chapter \@ref(regularize) which provides you an alternative linear regression technique when $p > n$.

__5. No or little multicollinearity:__  _Collinearity_ refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant.  This, obviously, leads to inaccurate interpretation of coefficients and identifying influential predictors.  

For example, in our data, `Garage_Area` and `Garage_Cars` are two variables that have a correlation of `r round(cor(train$Garage_Area, train$Garage_Cars), 2)` and both variables are strongly correlated to our response variable (`Sale_Price`). Looking at our full model where both of these variables are included, we see that `Garage_Area` is found to be statistically significant but `Garage_Cars` is not.

```{r, collinearity-1}
# fit with two strongly correlated variables
summary(cv_model3) %>%
  tidy() %>%
  filter(term %in% c("Garage_Area", "Garage_Cars"))
```

However, if we refit the full model without `Garage_Area`, the coefficient estimate for `Garage_Cars` increases three fold and becomes statistically significant.

```{r collinearity-2}
# model without Garage_Area
set.seed(123)
mod_wo_Garage_Area <- train(
  Sale_Price ~ ., 
  data = select(train, -Garage_Area), 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

summary(mod_wo_Garage_Area) %>%
  tidy() %>%
  filter(term == "Garage_Cars")
```

This reflects the instability in the linear regression model caused by the between-predictor relationships and this instability gets propagated directly to the model predictions. Considering 16 of our 34 numeric predictors have medium to strong correlation (Section \@ref(pca)), the biased coefficients of these predictors are likely restricting the predictive accuracy of our model.  How can we control for this problem?  One option is to manually remove one of the offending predictors.  However, when the number of predictors is large such as in our case, this becomes difficult.  Moreover, relationships between predictors can become complex and involve many predictors. In these cases, manual removal of specific predictors may not be possible.  Consequently, the following sections offers two simple extensions of linear regression where dimension reduction is applied prior to performing linear regression.  Chapter \@ref(regularize) offers a modified regression approach that helps to deal with the problem.  And future chapters provide alternative methods that are not effected by multicollinearity.

## Principal component regression

As discussed in Chapter \@ref(unsupervised), principal components analysis can be used to represent correlated variables in a lower dimension and the resulting components can be used as predictors in the linear regression model. This two-step process is known as __principal component regression__ (PCR) [@massy1965principal]. 

Performing PCR with __caret__ is an easy extension from our previous model.  We simply change the `method` to "pcr" within `train` to perform PCA on all our numeric predictors prior to applying the multiple regression.  Often, we can greatly improve performance by only using a small subset of all principal components as predictors.  Consequently, you can think of the number of principal components as a tuning parameter (see Section \@ref(tune)).  The following performs cross validated PCR with $1, 2, \dots, 20$ principal components, and Figure \@ref(fig:pcr-regression) illustrates the cross-validated RMSE.  You can see a significant drop in prediction error using just five principal components followed by a gradual decrease.  Using 17 principal components provided the lowest RMSE of \$35,769.99 (see `cv_model_pcr` for a comparison of the cross-validated results).

```{block, type="warning"}
Per Section \@ref(pca), don't forget to center and scale your predictors, which you can do by incorporating the `preProcess` argument.
```


```{r pcr-regression, fig.height=3.5, fig.width=6, fig.cap="The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components."}

# perform 10-fold cross validation on a PCR model tuning the number of
# principal components to use as predictors from 1-20
set.seed(123)
cv_model_pcr <- train(
  Sale_Price ~ ., 
  data = train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# model with lowest RMSE
cv_model_pcr$bestTune

# plot cross-validated RMSE
plot(cv_model_pcr)
```

By controlling for multicollinearity with PCR, we saw significant improvement in our predictive accuary (reducing out-of-sample RMSE from 41438 down to 35770).  However, since PCR is a two step process, the PCA step does not consider any aspects of the response when it selects the components.  Consequently, the new predictors produced by the PCA step are not designed to maximize the relationship with the response. Instead, it simply seeks to reduce the variability present throughout the predictor space. If that variability happens to be related to the response variability, then PCR has a good chance to identify a predictive relationship, as in our case. If, however, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exists (i.e. we may actually experience a decrease in our predictive accuracy).  Thus, an alternative approach to reduce the impact of multicollinearity is partial least squares.

## Partial least squares

_Partial least squares (PLS)_ can be viewed as a supervised dimension reduction procedure [@apm]. Similar to PCR this technique also constructs a set of linear combinations of the inputs for regression, but unlike PCR it uses the response variable to aid the construction of the principal components.  Thus, we can think of PLS as a supervised dimension reduction procedure that finds new features that not only appromxate the old features well, but also that are related to the response.

__TODO: IMAGE of PCR vs PLS__

Referring back to Equation \@ref(eq:pca1), PLS will compute the first principal ($z_1$) by setting each $\phi_{j1}$ to the coefficient from a simple linear regression model of $y$ onto that respective $x_j$. One can show that this coefficient is proportional to the correlation between $y$ and $x_j$. Hence, in computing $z_1 = \sum^p_{j=1} \phi_{j1}x_j$, PLS places the highest weight on the variables that are most strongly related to the response.

To compute the second principal ($z_2$), we first regress each variable on $z_1$. The residuals from this regression captures the remaining signal that has not been explained by the first principal. We substitute these residual values for the predictor values in Equation \@ref(eq:pca2). This process continues until all $m$ components have been computed and then we use OLS to regress the response on $z_1, \dots, z_m$.

```{block, type="note"}
See @friedman2001elements and @geladi1986partial for a thorough discussion of PLS.
```

Similar to PCR, we can easily fit a PLS model by changing the `method` argument in `caret::train`.  As with PCR, the number of principal components to use is a tuning parameter that is determined by the model that maximize predictive accuracy (minimizes RMSE in this case). The following performs cross validated PLS with $1, 2, \dots, 20$ principal components, and Figure \@ref(fig:pls-regression) illustrates the cross-validated RMSE.  You can see a greater drop in prediction error than PCR. Using PLS with $m = 10$ principal components provided the lowest RMSE of \$31,522.47.

```{r pls-regression, fig.height=3.5, fig.width=6, fig.cap="The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components."}
# perform 10-fold cross validation on a PLS model tuning the number of
# principal components to use as predictors from 1-20
set.seed(123)
cv_model_pls <- train(
  Sale_Price ~ ., 
  data = train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# model with lowest RMSE
cv_model_pls$bestTune

# plot cross-validated RMSE
plot(cv_model_pls)
```


## Model interpretation {#lm-model-interp}

Once we've found the model that minimizes the predictive accuracy, our next goal is to interpret the model structure.  Linear regression models provide a very intuitive model structure as they assume a ___monotonic linear relationship___ between the predictor variables and the response. The _linear_ relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the chapter, this constant change is provided by the given coefficient for a predictor. The _monotonic_ relationship means that a given predictor variable will always have a positive or negative relationship.  But how do we determine the most influential variables?

Variable importance seeks to identify those variables that are most influential in our model.  For linear regression models, this is most often measured by the absolute value of the _t_-statistic (Equation \@ref(eq:tstat)) for each model parameter used. For a PLS model, variable importance is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the RSS across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the RSS.

We can use `vip::vip` to extract and plot the most important variables.  The importance measure is normalized from 100 (most important) to 0 (least important). Figure \@ref(fig:pls-vip) illustrates that the top 4 most important variables are `Gr_liv_Area`, `First_Flr_SF`, `Garage_Area`, and `Garage_Cars` respectively.

```{r pls-vip, fig.cap="Top 20 most important variables for the PLS model."}
vip(cv_model_pls, num_features = 20)
```

As stated earlier, linear regression models assume a monotonic linear relationship.  To illustrate this, we can apply partial dependence plots (PDPs). PDPs plot the change in the average predicted value ($\hat y$) as specified feature(s) vary over their marginal distribution. As you will see in later chapters, PDPs become more useful when non-linear relationships are present.  However, PDPs of linear models help illustrate how a fixed change in $x_i$ relates to a fixed linear change in $\hat y_i$.  

```{block, type="tip"}
The __pdp__ package [@pkg-pdp] provides convenient functions for computing and plotting PDPs. For example, the following code chunk would plot the PDP for the `Gr_Liv_Area` predictor.

`pdp::partial(cv_model_pls, pred.var = "Gr_Liv_Area", grid.resolution = 20) %>% autoplot()`
```


All four of the most important predictors have a positive relationship with sale price; however, we see that the slope ($\beta_i$) is steepest for the most important predictor and gradually decreases for lessor important variables.

```{r, echo=FALSE, fig.height=5, fig.width=7, fig.cap="Partial dependence plots for the first four most important variables."}
p1 <- pdp::partial(cv_model_pls, pred.var = "Gr_Liv_Area", grid.resolution = 20) %>% 
  autoplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
p2 <- pdp::partial(cv_model_pls, pred.var = "First_Flr_SF", grid.resolution = 20) %>% 
  autoplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
p3 <- pdp::partial(cv_model_pls, pred.var = "Garage_Area", grid.resolution = 20) %>% 
  autoplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)
p4 <- pdp::partial(cv_model_pls, pred.var = "Garage_Cars", grid.resolution = 4) %>% 
  autoplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)

grid.arrange(p1, p2, p3, p4, nrow = 2)
```


## Final thoughts

Linear regression is a great starting point in learning more advanced predictive analytic approaches because, in its simplest form, it is very intuitive and easy to interpret.  Training a linear regression model is very easy and computationally efficient.  However, due to the many assumptions required, the disadvantages of linear regression often outweigh their benefits. In our example, we saw how multicollinearity was interferring with predictive accuracy.  By controlling multicollinearity with PCR and PLS we were able to improve predictive accuracy.  Later chapters will build on the concepts illustrated in this chapter and will compare cross-validated performance results to identify the best predictive model.  The following summarizes some of the advantages and disadvantages discussed regarding linear regression.

__FIXME: refine this section__

__Advantages__:

* Normal linear regression has no hyperparameters to tune and PCR and PLS have only one hyperparameter to tune; making these methods very simple to train.
* Computationally efficient - relatively fast compared to other algorithms in this book and does not require large memory.
* Easy to interpret results.

__Disadvantages__:

* Makes strong asssumptions about the data.
* Does not handle missing data - must impute or remove observations with missing values.
* Not robust to outliers as they can still bias the coefficients.
* Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion).
* Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks).
* Most large data sets violate one of the several assumptions made for linear regression to hold, which cause instability in the modeling results. 



## Learning more

This will get you up and running with linear regression.  Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)
- [Elements of Statistical Learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)



