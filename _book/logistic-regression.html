<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</title>
  <meta name="description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  
  <meta name="twitter:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  

<meta name="author" content="Bradley C. Boehmke and Brandon M. Greenwell">


<meta name="date" content="2018-10-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-regression.html">
<link rel="next" href="regularized-regression.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<script src="libs/kePrint/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="extra.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Business Analytics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-not-for"><i class="fa fa-check"></i>Who this book is not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-really-not-for"><i class="fa fa-check"></i>Who this book is really not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i>Why R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-this-book-is-organized"><i class="fa fa-check"></i>How this book is organized</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#using-code-examples"><i class="fa fa-check"></i>Using code examples</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-information"><i class="fa fa-check"></i>Software information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>Who are these Guys?</a><ul>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#bradley-c.-boehmke"><i class="fa fa-check"></i>Bradley C. Boehmke</a></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#brandon-m.-greenwell"><i class="fa fa-check"></i>Brandon M. Greenwell</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>1.1</b> Descriptive</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#predictive"><i class="fa fa-check"></i><b>1.2</b> Predictive</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#prescriptive"><i class="fa fa-check"></i><b>1.3</b> Prescriptive</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.4</b> Data sets</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#ames-iowa-housing-data"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#employee-attrition-data"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Descriptive Analytics</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive.html"><a href="descriptive.html"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive.html"><a href="descriptive.html#measures-of-location"><i class="fa fa-check"></i><b>2.2</b> Measures of location</a><ul>
<li class="chapter" data-level="2.2.1" data-path="descriptive.html"><a href="descriptive.html#the-sample-mean"><i class="fa fa-check"></i><b>2.2.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.2.2" data-path="descriptive.html"><a href="descriptive.html#the-sample-median"><i class="fa fa-check"></i><b>2.2.2</b> The sample median</a></li>
<li class="chapter" data-level="2.2.3" data-path="descriptive.html"><a href="descriptive.html#the-mean-or-the-median"><i class="fa fa-check"></i><b>2.2.3</b> The mean or the median</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive.html"><a href="descriptive.html#measures-of-spread"><i class="fa fa-check"></i><b>2.3</b> Measures of spread</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive.html"><a href="descriptive.html#empirical-rule"><i class="fa fa-check"></i><b>2.3.1</b> The empirical rule</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive.html"><a href="descriptive.html#percentiles"><i class="fa fa-check"></i><b>2.4</b> Percentiles</a></li>
<li class="chapter" data-level="2.5" data-path="descriptive.html"><a href="descriptive.html#robust-measures-of-spread"><i class="fa fa-check"></i><b>2.5</b> Robust measures of spread</a></li>
<li class="chapter" data-level="2.6" data-path="descriptive.html"><a href="descriptive.html#outliers"><i class="fa fa-check"></i><b>2.6</b> Outlier detection</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive.html"><a href="descriptive.html#categorical"><i class="fa fa-check"></i><b>2.7</b> Describing categorical data</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive.html"><a href="descriptive.html#contingency-tables"><i class="fa fa-check"></i><b>2.7.1</b> Contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive.html"><a href="descriptive.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>3</b> Visual data exploration</a><ul>
<li class="chapter" data-level="3.1" data-path="visualization.html"><a href="visualization.html#prerequisites-1"><i class="fa fa-check"></i><b>3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="3.2" data-path="visualization.html"><a href="visualization.html#univariate-data"><i class="fa fa-check"></i><b>3.2</b> Univariate data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="visualization.html"><a href="visualization.html#continuous-variables"><i class="fa fa-check"></i><b>3.2.1</b> Continuous Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="visualization.html"><a href="visualization.html#categorical-variables"><i class="fa fa-check"></i><b>3.2.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="visualization.html"><a href="visualization.html#bivariate-data"><i class="fa fa-check"></i><b>3.3</b> Bivariate data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="visualization.html"><a href="visualization.html#scatter-plots"><i class="fa fa-check"></i><b>3.3.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="visualization.html"><a href="visualization.html#multivariate-data"><i class="fa fa-check"></i><b>3.4</b> Multivariate data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="visualization.html"><a href="visualization.html#facetting"><i class="fa fa-check"></i><b>3.4.1</b> Facetting</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="visualization.html"><a href="visualization.html#data-quality"><i class="fa fa-check"></i><b>3.5</b> Data quality</a></li>
<li class="chapter" data-level="3.6" data-path="visualization.html"><a href="visualization.html#further-reading"><i class="fa fa-check"></i><b>3.6</b> Further reading</a></li>
<li class="chapter" data-level="3.7" data-path="visualization.html"><a href="visualization.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4</b> Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="inference.html"><a href="inference.html#the-frequentist-approach"><i class="fa fa-check"></i><b>4.1</b> The frequentist approach</a><ul>
<li class="chapter" data-level="4.1.1" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>4.1.1</b> The central limit theorem</a></li>
<li class="chapter" data-level="4.1.2" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference.html"><a href="inference.html#one-sided-versus-two-sided-tests"><i class="fa fa-check"></i><b>4.1.3</b> One-sided versus two-sided tests</a></li>
<li class="chapter" data-level="4.1.4" data-path="inference.html"><a href="inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>4.1.4</b> Type I and type II errors</a></li>
<li class="chapter" data-level="4.1.5" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>4.1.5</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1.6</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference.html"><a href="inference.html#one--and-two-sample-t-tests"><i class="fa fa-check"></i><b>4.2</b> One- and two-sample t-tests</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inference.html"><a href="inference.html#one-sample-t-test"><i class="fa fa-check"></i><b>4.2.1</b> One-sample t-test</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference.html"><a href="inference.html#two-sample-t-test"><i class="fa fa-check"></i><b>4.2.2</b> Two-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference.html"><a href="inference.html#tests-involving-more-than-two-means-anova-models"><i class="fa fa-check"></i><b>4.3</b> Tests involving more than two means: ANOVA models</a></li>
<li class="chapter" data-level="4.4" data-path="inference.html"><a href="inference.html#testing-for-association-in-contingency-tables"><i class="fa fa-check"></i><b>4.4</b> Testing for association in contingency tables</a></li>
<li class="chapter" data-level="4.5" data-path="inference.html"><a href="inference.html#nonparametric-tests"><i class="fa fa-check"></i><b>4.5</b> Nonparametric tests</a></li>
<li class="chapter" data-level="4.6" data-path="inference.html"><a href="inference.html#bootstrap"><i class="fa fa-check"></i><b>4.6</b> The nonparametric bootstrap</a><ul>
<li class="chapter" data-level="4.6.1" data-path="inference.html"><a href="inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>4.6.1</b> Bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inference.html"><a href="inference.html#further-reading-1"><i class="fa fa-check"></i><b>4.7</b> Further reading</a></li>
<li class="chapter" data-level="4.8" data-path="inference.html"><a href="inference.html#exercises-2"><i class="fa fa-check"></i><b>4.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>5</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised.html"><a href="unsupervised.html#prerequisites-2"><i class="fa fa-check"></i><b>5.1</b> Prerequisites</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>5.2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="5.2.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-principal-components"><i class="fa fa-check"></i><b>5.2.1</b> Finding principal components</a></li>
<li class="chapter" data-level="5.2.2" data-path="unsupervised.html"><a href="unsupervised.html#performing-pca-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Performing PCA in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="unsupervised.html"><a href="unsupervised.html#selecting-the-number-of-principal-components"><i class="fa fa-check"></i><b>5.2.3</b> Selecting the Number of Principal Components</a></li>
<li class="chapter" data-level="5.2.4" data-path="unsupervised.html"><a href="unsupervised.html#extracting-additional-insights"><i class="fa fa-check"></i><b>5.2.4</b> Extracting additional insights</a></li>
<li class="chapter" data-level="5.2.5" data-path="unsupervised.html"><a href="unsupervised.html#pca-with-mixed-data"><i class="fa fa-check"></i><b>5.2.5</b> PCA with mixed data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="unsupervised.html"><a href="unsupervised.html#cluster-analysis"><i class="fa fa-check"></i><b>5.3</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="5.3.1" data-path="unsupervised.html"><a href="unsupervised.html#clustering-distance-measures"><i class="fa fa-check"></i><b>5.3.1</b> Clustering distance measures</a></li>
<li class="chapter" data-level="5.3.2" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>5.3.2</b> K-means clustering</a></li>
<li class="chapter" data-level="5.3.3" data-path="unsupervised.html"><a href="unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.3.3</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="5.3.4" data-path="unsupervised.html"><a href="unsupervised.html#clustering-with-mixed-data"><i class="fa fa-check"></i><b>5.3.4</b> Clustering with mixed data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Predictive Analytics</b></span></li>
<li class="chapter" data-level="6" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html"><i class="fa fa-check"></i><b>6</b> Fundamental concepts</a><ul>
<li class="chapter" data-level="6.1" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#regression-problems"><i class="fa fa-check"></i><b>6.1</b> Regression problems</a></li>
<li class="chapter" data-level="6.2" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#classification-problems"><i class="fa fa-check"></i><b>6.2</b> Classification problems</a></li>
<li class="chapter" data-level="6.3" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#algorithm-comparison-guide"><i class="fa fa-check"></i><b>6.3</b> Algorithm Comparison Guide</a></li>
<li class="chapter" data-level="6.4" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#general-modeling-process"><i class="fa fa-check"></i><b>6.4</b> General modeling process</a><ul>
<li class="chapter" data-level="6.4.1" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#reg_perf_prereq"><i class="fa fa-check"></i><b>6.4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="6.4.2" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#reg-perf-split"><i class="fa fa-check"></i><b>6.4.2</b> Data splitting</a></li>
<li class="chapter" data-level="6.4.3" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#reg_perf_feat"><i class="fa fa-check"></i><b>6.4.3</b> Feature engineering</a></li>
<li class="chapter" data-level="6.4.4" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#model-form"><i class="fa fa-check"></i><b>6.4.4</b> Basic model formulation</a></li>
<li class="chapter" data-level="6.4.5" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#tune"><i class="fa fa-check"></i><b>6.4.5</b> Model tuning</a></li>
<li class="chapter" data-level="6.4.6" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#cv"><i class="fa fa-check"></i><b>6.4.6</b> Cross Validation for Generalization</a></li>
<li class="chapter" data-level="6.4.7" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#reg-perf-eval"><i class="fa fa-check"></i><b>6.4.7</b> Model evaluation</a></li>
<li class="chapter" data-level="6.4.8" data-path="fundamentalconcepts.html"><a href="fundamentalconcepts.html#interpreting-predictive-models"><i class="fa fa-check"></i><b>6.4.8</b> Interpreting predictive models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>7</b> Linear regression</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-regression.html"><a href="linear-regression.html#prerequisites-3"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="7.3" data-path="linear-regression.html"><a href="linear-regression.html#multi-lm"><i class="fa fa-check"></i><b>7.3</b> Multiple linear regression</a></li>
<li class="chapter" data-level="7.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>7.4</b> Assessing Model Accuracy</a></li>
<li class="chapter" data-level="7.5" data-path="linear-regression.html"><a href="linear-regression.html#model-concerns"><i class="fa fa-check"></i><b>7.5</b> Model concerns</a></li>
<li class="chapter" data-level="7.6" data-path="linear-regression.html"><a href="linear-regression.html#principal-component-regression"><i class="fa fa-check"></i><b>7.6</b> Principal component regression</a></li>
<li class="chapter" data-level="7.7" data-path="linear-regression.html"><a href="linear-regression.html#partial-least-squares"><i class="fa fa-check"></i><b>7.7</b> Partial least squares</a></li>
<li class="chapter" data-level="7.8" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-interp"><i class="fa fa-check"></i><b>7.8</b> Feature Interpretation</a></li>
<li class="chapter" data-level="7.9" data-path="linear-regression.html"><a href="linear-regression.html#final-thoughts"><i class="fa fa-check"></i><b>7.9</b> Final thoughts</a></li>
<li class="chapter" data-level="7.10" data-path="linear-regression.html"><a href="linear-regression.html#learning-more"><i class="fa fa-check"></i><b>7.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#prerequisites-4"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Why logistic regression</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>8.3</b> Simple logistic regression</a></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Multiple logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>8.5</b> Assessing model accuracy</a></li>
<li class="chapter" data-level="8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>8.6</b> Feature interpretation</a></li>
<li class="chapter" data-level="8.7" data-path="logistic-regression.html"><a href="logistic-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>8.7</b> Final thoughts</a></li>
<li class="chapter" data-level="8.8" data-path="logistic-regression.html"><a href="logistic-regression.html#learning-more-1"><i class="fa fa-check"></i><b>8.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>9</b> Regularized regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regularized-regression.html"><a href="regularized-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>9.1</b> Prerequisites</a></li>
<li class="chapter" data-level="9.2" data-path="regularized-regression.html"><a href="regularized-regression.html#why"><i class="fa fa-check"></i><b>9.2</b> Why Regularize</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#ridge"><i class="fa fa-check"></i><b>9.2.1</b> Ridge penalty</a></li>
<li class="chapter" data-level="9.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#lasso"><i class="fa fa-check"></i><b>9.2.2</b> Lasso penalty</a></li>
<li class="chapter" data-level="9.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#elastic"><i class="fa fa-check"></i><b>9.2.3</b> Elastic nets</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regularized-regression.html"><a href="regularized-regression.html#implementation"><i class="fa fa-check"></i><b>9.3</b> Implementation</a></li>
<li class="chapter" data-level="9.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-tune"><i class="fa fa-check"></i><b>9.4</b> Tuning</a></li>
<li class="chapter" data-level="9.5" data-path="regularized-regression.html"><a href="regularized-regression.html#lm-features"><i class="fa fa-check"></i><b>9.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="9.6" data-path="regularized-regression.html"><a href="regularized-regression.html#attrition-data"><i class="fa fa-check"></i><b>9.6</b> Attrition data</a></li>
<li class="chapter" data-level="9.7" data-path="regularized-regression.html"><a href="regularized-regression.html#final-thoughts-2"><i class="fa fa-check"></i><b>9.7</b> Final thoughts</a></li>
<li class="chapter" data-level="9.8" data-path="regularized-regression.html"><a href="regularized-regression.html#learning-more-2"><i class="fa fa-check"></i><b>9.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="MARS.html"><a href="MARS.html"><i class="fa fa-check"></i><b>10</b> Multivariate Adaptive Regression Splines</a><ul>
<li class="chapter" data-level="10.1" data-path="MARS.html"><a href="MARS.html#prerequisites-6"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="MARS.html"><a href="MARS.html#the-basic-idea"><i class="fa fa-check"></i><b>10.2</b> The basic idea</a><ul>
<li class="chapter" data-level="10.2.1" data-path="MARS.html"><a href="MARS.html#multivariate-regression-splines"><i class="fa fa-check"></i><b>10.2.1</b> Multivariate regression splines</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="MARS.html"><a href="MARS.html#fitting-a-basic-mars-model"><i class="fa fa-check"></i><b>10.3</b> Fitting a basic MARS model</a></li>
<li class="chapter" data-level="10.4" data-path="MARS.html"><a href="MARS.html#tuning"><i class="fa fa-check"></i><b>10.4</b> Tuning</a></li>
<li class="chapter" data-level="10.5" data-path="MARS.html"><a href="MARS.html#feature-interpretation-1"><i class="fa fa-check"></i><b>10.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="10.6" data-path="MARS.html"><a href="MARS.html#attrition-data-1"><i class="fa fa-check"></i><b>10.6</b> Attrition data</a></li>
<li class="chapter" data-level="10.7" data-path="MARS.html"><a href="MARS.html#final-thoughts-3"><i class="fa fa-check"></i><b>10.7</b> Final thoughts</a></li>
<li class="chapter" data-level="10.8" data-path="MARS.html"><a href="MARS.html#learning-more-3"><i class="fa fa-check"></i><b>10.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="RF.html"><a href="RF.html"><i class="fa fa-check"></i><b>11</b> Random Forests</a><ul>
<li class="chapter" data-level="11.1" data-path="RF.html"><a href="RF.html#prerequisites-7"><i class="fa fa-check"></i><b>11.1</b> Prerequisites</a></li>
<li class="chapter" data-level="11.2" data-path="RF.html"><a href="RF.html#decision-trees"><i class="fa fa-check"></i><b>11.2</b> Decision trees</a><ul>
<li class="chapter" data-level="11.2.1" data-path="RF.html"><a href="RF.html#a-simple-regression-tree-example"><i class="fa fa-check"></i><b>11.2.1</b> A simple regression tree example</a></li>
<li class="chapter" data-level="11.2.2" data-path="RF.html"><a href="RF.html#deciding-on-splits"><i class="fa fa-check"></i><b>11.2.2</b> Deciding on splits</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="RF.html"><a href="RF.html#bagging"><i class="fa fa-check"></i><b>11.3</b> Bagging</a></li>
<li class="chapter" data-level="11.4" data-path="RF.html"><a href="RF.html#random-forests"><i class="fa fa-check"></i><b>11.4</b> Random forests</a><ul>
<li class="chapter" data-level="11.4.1" data-path="RF.html"><a href="RF.html#oob-error-vs.test-set-error"><i class="fa fa-check"></i><b>11.4.1</b> OOB error vs. test set error</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="RF.html"><a href="RF.html#fitting-a-basic-random-forest-model"><i class="fa fa-check"></i><b>11.5</b> Fitting a basic random forest model</a></li>
<li class="chapter" data-level="11.6" data-path="RF.html"><a href="RF.html#tuning-1"><i class="fa fa-check"></i><b>11.6</b> Tuning</a><ul>
<li class="chapter" data-level="11.6.1" data-path="RF.html"><a href="RF.html#tuning-via-ranger"><i class="fa fa-check"></i><b>11.6.1</b> Tuning via ranger</a></li>
<li class="chapter" data-level="11.6.2" data-path="RF.html"><a href="RF.html#tuning-via-caret"><i class="fa fa-check"></i><b>11.6.2</b> Tuning via caret</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="RF.html"><a href="RF.html#feature-interpretation-2"><i class="fa fa-check"></i><b>11.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="11.8" data-path="RF.html"><a href="RF.html#attrition-data-2"><i class="fa fa-check"></i><b>11.8</b> Attrition data</a></li>
<li class="chapter" data-level="11.9" data-path="RF.html"><a href="RF.html#final-thoughts-4"><i class="fa fa-check"></i><b>11.9</b> Final thoughts</a></li>
<li class="chapter" data-level="11.10" data-path="RF.html"><a href="RF.html#learning-more-4"><i class="fa fa-check"></i><b>11.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="12" data-path="appendix-data.html"><a href="appendix-data.html"><i class="fa fa-check"></i><b>12</b> (APPENDIX) Appendix {-}</a></li>
<li class="chapter" data-level="" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i>Data sets</a><ul>
<li class="chapter" data-level="" data-path="data-sets.html"><a href="data-sets.html#ames-iowa-housing-data-1"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="data-sets.html"><a href="data-sets.html#employee-attrition-data-1"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Logistic regression</h1>
<p>Linear regression is used to approximate the relationship between a continuous response variable and a set of predictor variables. However, when the response variable is categorical rather than continuous, linear regression is not appropriate. Fortunately, analysts can turn to an analogous method, <em>logistic regression</em>, which is similar to linear regression in many ways. This chapter explores the use of logistic regression for binary response variables. Logistic regression can be expanded for multinomial problems (see <span class="citation">Faraway (<a href="#ref-faraway2016extending">2016</a><a href="#ref-faraway2016extending">a</a>)</span> for discussion of multinomial logistic regression in R); however, that goes beyond our intent here.</p>
<div id="prerequisites-4" class="section level2">
<h2><span class="header-section-number">8.1</span> Prerequisites</h2>
<p>For this section we will use the following packages:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)  <span class="co"># data manipulation &amp; visualization</span>
<span class="kw">library</span>(rsample)    <span class="co"># data splitting</span>
<span class="kw">library</span>(caret)      <span class="co"># logistic regression modeling</span>
<span class="kw">library</span>(vip)        <span class="co"># variable importance</span></code></pre>
<p>To illustrate logistic regression concepts we will use the employee attrition data, where our intent is to predict the <code>Attrition</code> response variable (“Yes”|“no”). As in the previous chapter, we’ll set aside 30% of our data as a test set to assess our generalizability error.</p>
<pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span>attrition <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate_if</span>(is.ordered, factor, <span class="dt">ordered =</span> <span class="ot">FALSE</span>)

<span class="co"># Create training (70%) and test (30%) sets for the rsample::attrition data.</span>
<span class="co"># Use set.seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
churn_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(df, <span class="dt">prop =</span> <span class="fl">.7</span>, <span class="dt">strata =</span> <span class="st">&quot;Attrition&quot;</span>)
train &lt;-<span class="st"> </span><span class="kw">training</span>(churn_split)
test  &lt;-<span class="st"> </span><span class="kw">testing</span>(churn_split)</code></pre>
</div>
<div id="why-logistic-regression" class="section level2">
<h2><span class="header-section-number">8.2</span> Why logistic regression</h2>
<p>To provide a clear motivation of logistic regression, assume we have credit card default data for customers and we want to understand if the credit card balance the customer has is an indicator of whether or not the customer will default on their credit card. To classify a customer as a high- vs. low-risk defaulter based on their balance we could use linear regression; however, the left plot in Figure <a href="logistic-regression.html#fig:whylogit">8.1</a> illustrates how linear regression would predict the probability of defaulting. Unfortunately, for balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1. Contrast this with the logistic regression line (right plot) that is nonlinear (sigmoidal-shaped).</p>
<div class="figure" style="text-align: center"><span id="fig:whylogit"></span>
<img src="abar_files/figure-html/whylogit-1.png" alt="Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1." width="768" />
<p class="caption">
Figure 8.1: Comparing the predicted probabilities of linear regression (left) to logistic regression (right). Predicted probabilities using linear regression results in flawed logic whereas predicted values from logistic regression will always lie between 0 and 1.
</p>
</div>
<p>To avoid the inadequecies of the linear model fit on a binary response, we must model the probability of our response using a function that gives outputs between 0 and 1 for all values of <span class="math inline">\(X\)</span>. Many functions meet this description. In logistic regression, we use the logistic function, which is defined in Equation <a href="logistic-regression.html#eq:logistic">(8.1)</a> and produces the S-curve in the right plot above.</p>
<p><span class="math display" id="eq:logistic">\[\begin{equation}
\tag{8.1}
  p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(\beta_i\)</span> parameters represent the coefficients as in linear regression and <span class="math inline">\(p(x)\)</span> may be interpreted as the probability that the positive class (default in the above example) is present. The minimum for <span class="math inline">\(p(x)\)</span> is obtained at <span class="math inline">\(\text{lim}_{a \rightarrow -\infty} \big[ \frac{e^a}{1+e^a} \big] = 0\)</span>, and the maximium for <span class="math inline">\(p(x)\)</span> is obtained at <span class="math inline">\(\text{lim}_{a \rightarrow \infty} \big[ \frac{e^a}{1+e^a} \big] = 1\)</span> which restricts the output probabilities to 0-1. Furthermore, a useful transformation for logistic regression is the <em>logit transformation</em> with follows:</p>
<p><span class="math display" id="eq:logit">\[\begin{equation}
\tag{8.2}
  g(X) = \text{ln} \bigg[ \frac{p(x)}{1 - p(x)} \bigg] = \beta_0 + \beta_1x
\end{equation}\]</span></p>
<p>The logit transformation exhibits several attractive properties of the linear regression model such as its linearity and interpretability, which we will come back to shortly.</p>
</div>
<div id="simple-logistic-regression" class="section level2">
<h2><span class="header-section-number">8.3</span> Simple logistic regression</h2>
<p>We will fit two logistic regression models in order to predict the probability of an employee attriting. The first predicts the probability of attrition based on their monthly income (<code>MonthlyIncome</code>) and the second is based on whether or not the employee works overtime (<code>OverTime</code>). The <code>glm</code> function fits generalized linear models, a class of models that includes logistic regression. The syntax of the <code>glm</code> function is similar to that of <code>lm</code>, except that we must pass the argument <code>family = binomial</code> in order to tell R to run a logistic regression rather than some other type of generalized linear model.</p>
<pre class="sourceCode r"><code class="sourceCode r">model1 &lt;-<span class="st"> </span><span class="kw">glm</span>(Attrition <span class="op">~</span><span class="st"> </span>MonthlyIncome, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">data =</span> train)
model2 &lt;-<span class="st"> </span><span class="kw">glm</span>(Attrition <span class="op">~</span><span class="st"> </span>OverTime, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">data =</span> train)</code></pre>
<p>In the background <code>glm</code>, uses <em>maximum likelihood</em> to fit the model. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat p(x_i)\)</span> of attrition for each employee corresponds as closely as possible to the employee’s observed attrition status. In other words, we try to find <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> such that plugging these estimates into the model for <span class="math inline">\(p(x)\)</span> (Equation <a href="logistic-regression.html#eq:logistic">(8.1)</a>) yields a number close to one for all employees who attrited, and a number close to zero for all employees who did not. This intuition can be formalized using a mathematical equation called a <em>likelihood function</em>:</p>
<p><span class="math display" id="eq:max-like">\[\begin{equation}
\tag{8.3}
  \ell(\beta_0, \beta_1) = \prod_{i:y_i=1}p(x_i) \prod_{i&#39;:y_i&#39;=0}(1-p(x_i&#39;))
\end{equation}\]</span></p>
<p>The estimates <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are chosen to <em>maximize</em> this likelihood function. Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we will examine in future chapters. What results is the predicted probability of attrition. Figure <a href="logistic-regression.html#fig:glm-sigmoid">8.2</a> illustrates the predicted probablities for the two models.</p>
<div class="figure" style="text-align: center"><span id="fig:glm-sigmoid"></span>
<img src="abar_files/figure-html/glm-sigmoid-1.png" alt="Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, `model1` predicts a decreased probability of attrition and if employees work overtime `model2` predicts an increased probability." width="768" />
<p class="caption">
Figure 8.2: Predicted probablilities of employee attrition based on monthly income (left) and overtime (right). As monthly income increases, <code>model1</code> predicts a decreased probability of attrition and if employees work overtime <code>model2</code> predicts an increased probability.
</p>
</div>
<p>The below table shows the coefficient estimates and related information that result from fitting a logistic regression model in order to predict the probability of <em>Attrition = Yes</em> for our two models. Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a <em>log-odds</em> scale.</p>
<p>Thus, we see that the <code>MonthlyIncome</code> <span class="math inline">\(\hat \beta_1 =\)</span> -1.144610^{-4}. This indicates that an increase in <code>MonthlyIncome</code> is associated with a decrease in the probability of attrition. To be precise, a one-unit increase in <code>MonthlyIncome</code> is associated with a decrease in the log odds of attrition by -1.144610^{-4} units. Similarly for <code>model2</code>, an employee that works <code>OverTime</code> has an increase of 1.3076 logg odds of attrition.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(model1)
## # A tibble: 2 x 5
##   term           estimate std.error statistic  p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -0.984    0.152         -6.47 9.62e-11
## 2 MonthlyIncome -0.000114 0.0000244     -4.69 2.74e- 6
<span class="kw">tidy</span>(model2)
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    -2.14     0.120    -17.9  2.21e-71
## 2 OverTimeYes     1.31     0.175      7.47 8.03e-14</code></pre>
<p>Taking an exponential transformation of these coefficients converts them from log odds to odds. Furthermore, we can convert odds to a probability with <span class="math inline">\(\text{probability} = \frac{odds}{1 + odds}\)</span> Thus, for every one dollar increase in <code>MonthlyIncome</code>, the odds of an employee attriting decreases slightly, represented by a slightly less than 50% probability. Whereas an employee that works <code>OverTime</code> has nearly 4-1 odds of attriting over an employee that does not work <code>OverTime</code>, represented by an increased probability of 78.7%.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert to odds</span>
<span class="kw">exp</span>(<span class="kw">coef</span>(model1))
##   (Intercept) MonthlyIncome 
##        0.3740        0.9999
<span class="kw">exp</span>(<span class="kw">coef</span>(model2))
## (Intercept) OverTimeYes 
##      0.1178      3.6974

<span class="co"># convert to probability</span>
<span class="kw">exp</span>(<span class="kw">coef</span>(model1)) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="kw">coef</span>(model1)))
##   (Intercept) MonthlyIncome 
##        0.2722        0.5000
<span class="kw">exp</span>(<span class="kw">coef</span>(model2)) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="kw">coef</span>(model2)))
## (Intercept) OverTimeYes 
##      0.1054      0.7871</code></pre>
<p>Many aspects of the coefficient output are similar to those discussed in the linear regression output. For example, we can measure the confidence intervals and accuracy of the coefficient estimates by computing their standard errors. For instance, both models’s <span class="math inline">\(\hat \beta_1\)</span> have a p-value &lt; 0.05 suggesting a strong probability that a relationship between these predictors and the probability of attrition exists. We can also use the standard errors to get confidence intervals as we did in the linear regression tutorial:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(model1)
##                    2.5 %     97.5 %
## (Intercept)   -1.2812812 -0.6848677
## MonthlyIncome -0.0001648 -0.0000689
<span class="kw">confint</span>(model2)
##               2.5 % 97.5 %
## (Intercept) -2.3808 -1.911
## OverTimeYes  0.9653  1.652</code></pre>
</div>
<div id="multiple-logistic-regression" class="section level2">
<h2><span class="header-section-number">8.4</span> Multiple logistic regression</h2>
<p>We can also extend our model as seen in Eq. 1 so that we can predict a binary response using multiple predictors where <span class="math inline">\(X = (X_1,\dots, X_p)\)</span> are <em>p</em> predictors:</p>
<p><span class="math display" id="eq:multi-logistic">\[\begin{equation}
\tag{8.4}
p(X) = \frac{e^{\beta_0 + \beta_1X + \cdots + \beta_pX_p }}{1 + e^{\beta_0 + \beta_1X + \cdots + \beta_pX_p}} 
\end{equation}\]</span></p>
<p>Let’s go ahead and fit a model that predicts the probability of <code>Attrition</code> based on the <code>MonthlyIncome</code> and <code>OverTime</code>. Our results show that both features are statistically significant and Figure <a href="logistic-regression.html#fig:glm-sigmoid2">8.3</a> illustrates common trends between <code>MonthlyIncome</code> and <code>Attrition</code>; however, working <code>OverTime</code> tends to nearly double the probability of attrition.</p>
<pre class="sourceCode r"><code class="sourceCode r">model3 &lt;-<span class="st"> </span><span class="kw">glm</span>(Attrition <span class="op">~</span><span class="st"> </span>MonthlyIncome <span class="op">+</span><span class="st"> </span>OverTime, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">data =</span> train)
<span class="kw">tidy</span>(model3)
## # A tibble: 3 x 5
##   term           estimate std.error statistic  p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -1.44     0.173         -8.32 9.00e-17
## 2 MonthlyIncome -0.000124 0.0000254     -4.88 1.06e- 6
## 3 OverTimeYes    1.36     0.179          7.61 2.75e-14</code></pre>
<div class="figure" style="text-align: center"><span id="fig:glm-sigmoid2"></span>
<img src="abar_files/figure-html/glm-sigmoid2-1.png" alt="Predicted probability of attrition based on monthly income and whether or not employees work overtime." width="576" />
<p class="caption">
Figure 8.3: Predicted probability of attrition based on monthly income and whether or not employees work overtime.
</p>
</div>
</div>
<div id="assessing-model-accuracy-1" class="section level2">
<h2><span class="header-section-number">8.5</span> Assessing model accuracy</h2>
<p>With a basic understanding of logistic regression under our belt, similar to linear regression our concern now shifts to how well do our models predict. As in the last chapter, we will use <code>caret::train</code> and fit three 10-fold cross validated logistic regression models. Extracting the accuracy measures, we see that both <code>cv_model1</code> and <code>cv_model2</code> had an average accuracy of 83.89%. However, <code>cv_model3</code> which used all predictor variables in our data achieved an average accuracy rate of 86.3%.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model1 &lt;-<span class="st"> </span><span class="kw">train</span>(
  Attrition <span class="op">~</span><span class="st"> </span>MonthlyIncome, 
  <span class="dt">data =</span> train, 
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
  )

<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model2 &lt;-<span class="st"> </span><span class="kw">train</span>(
  Attrition <span class="op">~</span><span class="st"> </span>MonthlyIncome <span class="op">+</span><span class="st"> </span>OverTime, 
  <span class="dt">data =</span> train, 
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
  )

<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model3 &lt;-<span class="st"> </span><span class="kw">train</span>(
  Attrition <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> train, 
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
  )

<span class="co"># extract out of sample performance measures</span>
<span class="kw">summary</span>(<span class="kw">resamples</span>(<span class="kw">list</span>(
  <span class="dt">model1 =</span> cv_model1, 
  <span class="dt">model2 =</span> cv_model2, 
  <span class="dt">model3 =</span> cv_model3
  )))<span class="op">$</span>statistics<span class="op">$</span>Accuracy
##          Min. 1st Qu. Median   Mean 3rd Qu.   Max.
## model1 0.8350  0.8353 0.8365 0.8389  0.8431 0.8447
## model2 0.8350  0.8353 0.8365 0.8389  0.8431 0.8447
## model3 0.8058  0.8389 0.8586 0.8632  0.8949 0.9135
##        NA&#39;s
## model1    0
## model2    0
## model3    0</code></pre>
<p>We can get greater understanding of our model’s performance by assessing the confusion matrix (see section <a href="fundamentalconcepts.html#reg-perf-eval">6.4.7</a>). We can use <code>train::confusionMatrix</code> to compute a confusion matrix. We need to supply our model’s predicted class and the actuals from our trainin data. Our confusion matrix provides a host of information. Particularly, we can see that although we do well predicting cases of non-attrition (note the high specificity), our model does particularly poor predicting actual cases of attrition (note the low sensitivity).</p>
<div class="tip">
<p>
By default the <code>predict</code> function predicts the response class for a <strong>caret</strong> model; however, you can change the <code>type</code> argument to predict the probabilities (see <code>?predict.train</code>).
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict class</span>
pred_class &lt;-<span class="st"> </span><span class="kw">predict</span>(cv_model3, train)

<span class="co"># create confusion matrix</span>
<span class="kw">confusionMatrix</span>(<span class="kw">relevel</span>(pred_class, <span class="dt">ref =</span> <span class="st">&quot;Yes&quot;</span>), <span class="kw">relevel</span>(train<span class="op">$</span>Attrition, <span class="dt">ref =</span> <span class="st">&quot;Yes&quot;</span>))
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Yes  No
##        Yes  79  33
##        No   87 831
##                                         
##                Accuracy : 0.883         
##                  95% CI : (0.862, 0.902)
##     No Information Rate : 0.839         
##     P-Value [Acc &gt; NIR] : 3.05e-05      
##                                         
##                   Kappa : 0.504         
##  Mcnemar&#39;s Test P-Value : 1.31e-06      
##                                         
##             Sensitivity : 0.4759        
##             Specificity : 0.9618        
##          Pos Pred Value : 0.7054        
##          Neg Pred Value : 0.9052        
##              Prevalence : 0.1612        
##          Detection Rate : 0.0767        
##    Detection Prevalence : 0.1087        
##       Balanced Accuracy : 0.7189        
##                                         
##        &#39;Positive&#39; Class : Yes           
## </code></pre>
<p>One thing to point out, in the confusion matrix above you will note the metric <code>No Information Rate: 0.8388</code>. This represents the ratio of non-attrition versus attrition in our trainin data (<code>table(train$Attrition) %&gt;% prop.table()</code>). Consequently, if we simply predicted “No” for every employee we would still get an accuracy rate of 83.88%. Therefore, our goal is to maximize our accuracy rate over and above this no information benchmark while also trying to balance sensitivity and specificity. To understand how well we are achieving this we can visualize the ROC curve (section <a href="fundamentalconcepts.html#reg-perf-eval">6.4.7</a>). If we compare our simple model (<code>cv_model1</code>) to our full model <code>cv_model3</code>, we can see that we the lift achieved with the more accurate model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ROCR)

<span class="co"># create predicted probabilities</span>
m1_prob &lt;-<span class="st"> </span><span class="kw">predict</span>(cv_model1, train, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)<span class="op">$</span>Yes
m3_prob &lt;-<span class="st"> </span><span class="kw">predict</span>(cv_model3, train, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)<span class="op">$</span>Yes

<span class="co"># compute AUC metrics for cv_model1 and cv_model3</span>
perf1 &lt;-<span class="st"> </span><span class="kw">prediction</span>(m1_prob, train<span class="op">$</span>Attrition) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">performance</span>(<span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)

perf2 &lt;-<span class="st"> </span><span class="kw">prediction</span>(m3_prob, train<span class="op">$</span>Attrition) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">performance</span>(<span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)

<span class="co"># plot both ROC curves for cv_model1 and cv_model3</span>
<span class="kw">plot</span>(perf1, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">plot</span>(perf2, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)

<span class="kw">legend</span>(.<span class="dv">8</span>, <span class="fl">.2</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;cv_model1&quot;</span>, <span class="st">&quot;cv_model3&quot;</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">cex =</span> <span class="fl">0.6</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-82"></span>
<img src="abar_files/figure-html/unnamed-chunk-82-1.png" alt="ROC curve for `cv_model1` and `cv_model3`. The increase in the AUC represents the 'lift' that we achieve with `cv_model3`." width="576" />
<p class="caption">
Figure 8.4: ROC curve for <code>cv_model1</code> and <code>cv_model3</code>. The increase in the AUC represents the ‘lift’ that we achieve with <code>cv_model3</code>.
</p>
</div>
<p>Similar to linear regression, we can perform a PLS logistic regression to assess if reducing the dimension of our numeric predictors helps to achieve improved accuracy. There are 16 numeric features in our data set so the following performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1-16. The optimal model uses 14 principal components, which is not reducing the dimension by much. However, the mean accuracy of 0.866 was only marginally better than the average CV accuracy of <code>cv_model3</code> (0.863), likely within the margin of error.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># perform 10-fold cross validation on a PLS model tuning the number of</span>
<span class="co"># principal components to use as predictors from 1-20</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model_pls &lt;-<span class="st"> </span><span class="kw">train</span>(
  Attrition <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> train, 
  <span class="dt">method =</span> <span class="st">&quot;pls&quot;</span>,
  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),
  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;zv&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">tuneLength =</span> <span class="dv">16</span>
  )

<span class="co"># model with lowest RMSE</span>
cv_model_pls<span class="op">$</span>bestTune
##    ncomp
## 14    14

<span class="co"># plot cross-validated RMSE</span>
<span class="kw">plot</span>(cv_model_pls)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:pls-logistic-regression"></span>
<img src="abar_files/figure-html/pls-logistic-regression-1.png" alt="The 10-fold cross valdation RMSE obtained using PLS with 1-16 principal components." width="576" />
<p class="caption">
Figure 8.5: The 10-fold cross valdation RMSE obtained using PLS with 1-16 principal components.
</p>
</div>
</div>
<div id="feature-interpretation" class="section level2">
<h2><span class="header-section-number">8.6</span> Feature interpretation</h2>
<p>Similar to linear regression, once our preferred logistic regression model is identified, next we need to interpret how the features are influencing the results. As with normal linear regression models, variable importance for logistic regression models are computed with the absolute value of the <em>t</em>-statistic for each model parameter is used. Using <code>vip</code> we can extract our top 20 influential variables. Figure <a href="logistic-regression.html#fig:glm-vip">8.6</a> illustrates that <code>OverTime</code> is the most influential followed by <code>JobSatisfaction</code>, <code>NumCompaniesWorked</code>, and <code>EnvironmentSatisfaction</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vip</span>(cv_model3, <span class="dt">num_features =</span> <span class="dv">20</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:glm-vip"></span>
<img src="abar_files/figure-html/glm-vip-1.png" alt="Top 20 most important variables for the PLS model." width="672" />
<p class="caption">
Figure 8.6: Top 20 most important variables for the PLS model.
</p>
</div>
<p>Similar to linear regression, logistic regression assumes a monotonic linear relationship. However, the linear relationship is in the form of a log-odds probability; therefore, the regular probability relationship will have a curvilinear effect. This is illustrated in Figure <a href="logistic-regression.html#fig:glm-pdp">8.7</a> by the change in predicted probability of attrition associated with the marginal change in the number of companies an employee has work for (<code>NumCompaniesWorked</code>). Employees that have experienced more employment changes tend to have a high probability of making another future change.</p>
<p>Furthermore, the partial dependence plots for the three top categorical predictors (<code>OverTime</code>, <code>JobSatisfaction</code>, and <code>EnvironmentSatisfaction</code>) illustrate the change in predicted probability of attrition based on the employee’s status for each predictor.</p>
<div class="tip">
<p>
See the supplemental material at [<a href="https://github.com/koalaverse/abar" class="uri">https://github.com/koalaverse/abar</a>](<a href="https://github.com/koalaverse/abar" class="uri">https://github.com/koalaverse/abar</a>] for the code to produce the following plots.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:glm-pdp"></span>
<img src="abar_files/figure-html/glm-pdp-1.png" alt="Partial dependence plots for the first four most important variables.  We can see how the predicted probability of attrition changes for each value of the influential predictors." width="672" />
<p class="caption">
Figure 8.7: Partial dependence plots for the first four most important variables. We can see how the predicted probability of attrition changes for each value of the influential predictors.
</p>
</div>
</div>
<div id="final-thoughts-1" class="section level2">
<h2><span class="header-section-number">8.7</span> Final thoughts</h2>
<p>Logistic regression is a natural starting point for learning predictive models for classification purposes due to its similarity to linear regression. Later chapters will build on the concepts illustrated in this chapter and will compare cross-validated performance results to identify the best predictive model for our employee attrition problem. The following summarizes some of the advantages and disadvantages discussed regarding logistic regression.</p>
<p><strong>FIXME: refine this section</strong></p>
<p><strong>Advantages</strong>:</p>
<p><strong>Disadvantages</strong>:</p>
</div>
<div id="learning-more-1" class="section level2">
<h2><span class="header-section-number">8.8</span> Learning more</h2>
<p>This will get you up and running with logistic regression. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:</p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
<li><a href="https://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-faraway2016extending">
<p>Faraway, Julian J. 2016a. <em>Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models</em>. Vol. 124. CRC press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularized-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["abar.pdf", "abar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
