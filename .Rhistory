p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE) + ggtitle("Permutation-based variable importance")
gridExtra::grid.arrange(p1, p2, nrow = 1)
# partial dependence of Sale_Price on Gr_Liv_Area
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
library(vip)
library(pdp)
# partial dependence of Sale_Price on Gr_Liv_Area
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
rf_impurity %>%
partial(pred.var = "Overall_Qual", train = as.data.frame(ames_train)) %>%
autoplot()
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.2) +
ggtitle("Non-centered ICE plot")
ice1 <- rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.1) +
ggtitle("Non-centered ICE plot")
ice2 <- rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.1, center = TRUE) +
ggtitle("Centered ICE plot")
gridExtra::grid.arrange(ice1, ice2, nrow = 1)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# get attrition data
df <- rsample::attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
df
# get attrition data
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
dim(df)
sqrt(31)
seq(3, 12, by = 3)
seq(1, 9, by = 3)
seq(1, 10, by = 3)
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 12, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("variance", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
?ranger
hyper_grid <- expand.grid(
mtry            = seq(3, 12, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
ggplot(tuned_rf)
summary(resamples(list(
rf_model = tuned_rf
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
tuned_rf
nrow(hyper_grid)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
# train regularized logistic regression model
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# extract out of sample performance measures
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
# train logistic regression model
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
# train regularized logistic regression model
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# extract out of sample performance measures
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
glm_mod
penalized_mod
tuned_mars
tuned_rf
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10,
seed = 123
)
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
seed = 123
)
tuned_rf
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
warnings()
# train logistic regression model
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
glm_mod
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10,
seed = 123
)
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
penalized_mod
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
install.packages("pBrackets")
?bookdown::html_document2
install.packages("outliers")
install.packages("ISLR")
gitr::git_pull()
seq(2, 100, length.out = 10) %>% floor()
library(dplyr)
seq(2, 100, length.out = 10) %>% floor()
seq(2, 100, length.out = 10)
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)  # data splitting
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
library(gbm)
# for reproducibility
set.seed(123)
# train GBM model
gbm.fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
print(gbm.fit)
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# for reproducibility
set.seed(123)
# train GBM model
gbm.fit2 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
# find index for n trees with minimum CV error
min_MSE <- which.min(gbm.fit2$cv.error)
# get MSE and compute RMSE
sqrt(gbm.fit2$cv.error[min_MSE])
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit2, method = "cv")
library(tidyverse)   # basic data manipulation and plotting
library(h2o)
mybasket <- toystore::my_basket
my_basket <- toystore::my_basket
rm(mybasket)
h2o.init(max_mem_size = "5g")
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
# run PCA
my_pca <- h2o.prcomp(
training_frame = my_basket.h2o,
pca_method = "GramSVD",
k = ncol(my_basket.h2o),
transform = "STANDARDIZE",
impute_missing = TRUE,
max_runtime_secs = 1000
)
my_pca
my_pca@model$eigenvectors
my_pca@model$importance
my_pca@model$importance %>% .[2,]
my_pca@model$importance %>% .[3,]
my_pca@model$importance %>% .[3,] %>% unlist()
my_pca@model$importance %>% .[3,] %>% unlist() %>% .[26]
my_pca@model$importance %>% .[3,] %>% unlist() %>% .[1:26]
my_pca@model$importance[3,] %>% unlist() %>% .[1:26]
my_pca@model$importance[3, 1:26]
my_pca@model$importance[3, 1:26] %>% unlist()
predict(my_pca, my_basket)
predict(my_pca, my_basket.h20)
predict(my_pca, my_basket.h2o)
# re-run PCA with k = 26
final_pca <- h2o.prcomp(
training_frame = my_basket.h2o,
pca_method = "GramSVD",
k = 26,
transform = "STANDARDIZE",
impute_missing = TRUE,
max_runtime_secs = 1000
)
predict(final_pca, my_basket.h2o)
final_pca@model$eigenvectors
gitr::git_rdone("started outlining GBM chapter")
