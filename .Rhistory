summary(ames_mars) %>% .$coefficients %>% head(10)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
head(hyper_grid)
nrow(hyper_grid)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars
# extract out of sample performance measures
summary(resamples(list(
Multiple_regression = cv_model1,
PCR = cv_model2,
PLS = cv_model3,
Elastic_net = cv_model4,
MARS = tuned_mars
)))$statistics$RMSE %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "hover"))
install.packages("kableExtra")
citation(package = "earth")
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
library(rsample)   # data splitting
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
library(vip)       # variable importance
library(pdp)       # variable relationships
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p
p1
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% autoplot()
p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>%
plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
varImp(tuned_mars)
?varImp.earth
varImp(tuned_mars) %>% head()
varImp(tuned_mars) %>% str()
varImp(tuned_mars)$importance %>% head()
varImp(tuned_mars, value = "rss")$importance %>% head()
varImp(tuned_mars, value = "gcv")$importance %>% head()
vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
?vip
?vi
?vip::vi
#rtance plots
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
gridExtra::grid.arrange(p1, p2, ncol = 2)
vi(tuned_mars, num_features = 40, bar = FALSE, value = "gcv")
vi(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
vi(tuned_mars, num_features = 26, bar = FALSE, value = "rss")
vi(tuned_mars, num_features = 26, bar = FALSE, value = "rss") %>% as.data.frame()
coef(tuned_mars$finalModel)
coef(tuned_mars$finalModel) %>% tidy()
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "\*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "Year"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, [[:punct:]]))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "[[:punct:]]"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "\\*"))
coef(tuned_mars$finalModel)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# for reproducibiity
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# for reproducibiity
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
metric = "AUC",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars$bestTune
ggplot(tuned_mars)
tuned_mars
tuned_mars$bestTune
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
summary(resamples(list(
logistic_model = glm_mod,
penalized_model = penalized_mod,
MARS_model = tuned_mars,
)))$statistics$Accuracy
penalized_mod
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
)))$statistics$Accuracy
tuned_mars
glm_mod
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
)))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars
)))$statistics$Accuracy
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
library(rsample)
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# for reproduciblity
set.seed(123)
# default RF model
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
library(randomForest) # basic implementation
# for reproduciblity
set.seed(123)
# default RF model
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
m1
sqrt(661089658)
plot(m1)
# number of trees with lowest MSE
which.min(m1$mse)
# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
system.time(
ames_randomForest <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train,
ntree   = 500,
mtry    = floor(length(features) / 3)
)
)
# ranger speed
system.time(
ames_ranger <- ranger(
formula   = Sale_Price ~ .,
data      = ames_train,
num.trees = 500,
mtry      = floor(length(features) / 3)
)
)
library(ranger)
features <- setdiff(names(ames_train), "Sale_Price")
# randomForest speed
system.time(
ames_randomForest <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train,
ntree   = 500,
mtry    = floor(length(features) / 3)
)
)
# ranger speed
system.time(
ames_ranger <- ranger(
formula   = Sale_Price ~ .,
data      = ames_train,
num.trees = 500,
mtry      = floor(length(features) / 3)
)
)
35.292 / 1.298
ames_randomForest
ames_ranger
citation("randomForest")
citation("ranger")
?ranger
floor(length(features) / 3)
seq(1, length(features), length.out = 5)
seq(1, length(features), length.out = 6)
seq(1, length(features), length.out = 8)
modelLookup("ranger")
library(caret)
modelLookup("ranger")
# create a tuning grid
hyper_grid <- expand.grid(
.mtry = seq(1, length(features), length.out = 5),
.splitrule = c("variance", "extratrees"),
.min.node.size = c(1, 3, 5, 10)
)
head(hyper_grid)
hyper_grid
hyper_grid[1:2,]
tuned_mars <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid[1:2,]
)
tuned_mars
# create a tuning grid
hyper_grid <- expand.grid(
mtry = seq(1, length(features), length.out = 5),
splitrule = c("variance", "extratrees"),
min.node.size = c(1, 3, 5, 10)
)
head(hyper_grid)
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid[1:2,]
)
tuned_rf
tuned_rf$bestTune
ggplot(tuned_rf)
set.seed(123)
# cross validated model
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_rf
tuned_rf$bestTune
ggplot(tuned_rf)
system.time({tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
respect.unordered.factors = 'order',
seed = 123
)})
837.022/60
tuned_rf$bestTune
tuned_rf
ggplot(tuned_rf)
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
knitr::include_graphics("illustrations/oob-error-compare-1.svg")
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # automating the tuning process
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# for reproduciblity
set.seed(123)
# default RF model
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
m1
# for reproduciblity
set.seed(123)
# default RF model
rf1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
m1
plot(m1)
ggplot(m1)
# number of trees with lowest MSE
which.min(rf1$mse)
# RMSE of this optimal random forest
sqrt(rf1$mse[which.min(rf1$mse)])
# names of features
features <- setdiff(names(ames_train), "Sale_Price")
# randomForest speed
system.time(
ames_randomForest <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train,
ntree   = 500,
mtry    = floor(length(features) / 3)
)
)
# ranger speed
system.time(
ames_ranger <- ranger(
formula   = Sale_Price ~ .,
data      = ames_train,
num.trees = 500,
mtry      = floor(length(features) / 3)
)
)
36.316/1.26
# create a tuning grid
hyper_grid <- expand.grid(
mtry = seq(1, length(features), length.out = 5),
splitrule = c("variance", "extratrees"),
min.node.size = c(1, 3, 5, 10)
)
head(hyper_grid)
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
seed = 123
)
tuned_rf$bestTune
tuned_rf
81/3
seq(1, 9, by = 3)
seq(1, 9, by = 4)
seq(3, 9, by = 3)
# create a tuning grid
hyper_grid <- expand.grid(
mtry       = seq(20, 35, by = 5),
node_size  = seq(3, 9, by = 3),
sampe_size = c(.632, .80),
splitrule = c("variance", "extratrees"),
OOB_RMSE   = 0
)
dim(hyper_grid)
?ranger
# create a tuning grid
hyper_grid <- expand.grid(
mtry       = seq(20, 35, by = 5),
min.node.size  = seq(3, 9, by = 3),
sample.fraction = c(.632, .80),
splitrule = c("variance", "extratrees"),
OOB_RMSE   = 0
)
dim(hyper_grid)
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
tuneGrid = hyper_grid,
seed = 123
)
hyper_grid
hyper_grid <- expand.grid(
mtry = seq(20, 35, by = 5),
min.node.size = seq(3, 9, by = 3),
sample.fraction = c(.632, .80),
splitrule = c("variance", "extratrees")
)
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
tuneGrid = hyper_grid,
seed = 123
)
# create a tuning grid
hyper_grid <- expand.grid(
mtry = seq(20, 35, by = 5),
min.node.size = seq(3, 9, by = 3),
sample.fraction = c(.632, .80),
splitrule = c("variance", "extratrees"),
OOB_RMSE = 0
)
dim(hyper_grid)
seq_along(hyper_grid)
seq_len(nrow(hyper_grid))
system.time({for(i in seq_len(nrow(hyper_grid))) {
# train model
model <- ranger(
formula         = Sale_Price ~ .,
data            = ames_train,
num.trees       = 500,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
sample.fraction = hyper_grid$sample.fraction[i],
splitrule       = hyper_grid$splitrule[i],
seed            = 123
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
hyper_grid %>%
dplyr::arrange(OOB_RMSE) %>%
head(10)})
hyper_grid %>%
dplyr::arrange(OOB_RMSE) %>%
head(10)
range(hyper_grid$OOB_RMSE)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(20, 35, by = 5),
min.node.size   = seq(3, 9, by = 3),
sample.fraction = c(.632, .80),
splitrule       = c("variance", "extratrees")
)
head(hyper_grid)
# cross validated model
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
seed = 123
)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(20, 35, by = 5),
min.node.size   = seq(3, 9, by = 3),
splitrule       = c("variance", "extratrees")
)
system.time({tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)})
438.590/60
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
getModelInfo("ranger")
install.packages("arules")
