library(dplyr)
seq(2, 100, length.out = 10) %>% floor()
seq(2, 100, length.out = 10)
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)  # data splitting
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
library(gbm)
# for reproducibility
set.seed(123)
# train GBM model
gbm.fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
print(gbm.fit)
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# for reproducibility
set.seed(123)
# train GBM model
gbm.fit2 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
# find index for n trees with minimum CV error
min_MSE <- which.min(gbm.fit2$cv.error)
# get MSE and compute RMSE
sqrt(gbm.fit2$cv.error[min_MSE])
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit2, method = "cv")
library(tidyverse)   # basic data manipulation and plotting
library(h2o)
mybasket <- toystore::my_basket
my_basket <- toystore::my_basket
rm(mybasket)
h2o.init(max_mem_size = "5g")
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
# run PCA
my_pca <- h2o.prcomp(
training_frame = my_basket.h2o,
pca_method = "GramSVD",
k = ncol(my_basket.h2o),
transform = "STANDARDIZE",
impute_missing = TRUE,
max_runtime_secs = 1000
)
my_pca
my_pca@model$eigenvectors
my_pca@model$importance
my_pca@model$importance %>% .[2,]
my_pca@model$importance %>% .[3,]
my_pca@model$importance %>% .[3,] %>% unlist()
my_pca@model$importance %>% .[3,] %>% unlist() %>% .[26]
my_pca@model$importance %>% .[3,] %>% unlist() %>% .[1:26]
my_pca@model$importance[3,] %>% unlist() %>% .[1:26]
my_pca@model$importance[3, 1:26]
my_pca@model$importance[3, 1:26] %>% unlist()
predict(my_pca, my_basket)
predict(my_pca, my_basket.h20)
predict(my_pca, my_basket.h2o)
# re-run PCA with k = 26
final_pca <- h2o.prcomp(
training_frame = my_basket.h2o,
pca_method = "GramSVD",
k = 26,
transform = "STANDARDIZE",
impute_missing = TRUE,
max_runtime_secs = 1000
)
predict(final_pca, my_basket.h2o)
final_pca@model$eigenvectors
gitr::git_rdone("started outlining GBM chapter")
gitr::git_rdone("adjusted GBM model naming convention")
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)  # data splitting
library(gbm)
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# for reproducibility
set.seed(123)
# train GBM model
gbm_mod1 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
print(gbm_mod1)
# get MSE and compute RMSE
sqrt(min(gbm_mod1$cv.error))
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod1, method = "cv")
# for reproducibility
set.seed(123)
# train GBM model
gbm_mod2 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
# find index for n trees with minimum CV error
min_MSE <- which.min(gbm_mod2$cv.error)
# get MSE and compute RMSE
sqrt(gbm_mod2$cv.error[min_MSE])
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod2, method = "cv")
file.exists(".RProfile")
library(caret)
library(rsample)  # data splitting
library(gbm)
library(caret)
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
gbm_grid <- expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15)
)
system.time({gbm_tune <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "gbm",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
num.trees = 5000,
seed = 123,
verbose = FALSE
)})
gbm_grid <- expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = 5000
)
system.time({gbm_tune <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "gbm",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
num.trees = 5000,
seed = 123,
verbose = FALSE
)})
warnings()
subset(ames_train, select = -Sale_Price)
subset(ames_train, select = -Sale_Price) %>% as.data.frame()
system.time({gbm_tune <- train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
seed = 123,
verbose = FALSE
)})
warnings()
system.time({gbm_tune <- train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
seed = 123,
verbose = FALSE
)})
warnings()
metric <- "RMSE"
trainControl <- trainControl(method="cv", number=10)
set.seed(99)
gbm.caret <- train(Sepal.Length ~ .
, data=iris
, distribution="gaussian"
, method="gbm"
, trControl=trainControl
, verbose=FALSE
#, tuneGrid=caretGrid
, metric=metric
, bag.fraction=0.75
)
gbm.caret
gbm_grid <- expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = 5000
)
gbm_grid <- gbm_grid[1:3,]
gbm_grid
gbm_tune <- train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
seed = 123,
verbose = FALSE
)
warnings()
train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
interaction.depth = 1, shrinkage = .3, n.minobsinnode = 5,
seed = 123,
verbose = FALSE
)
train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
trControl = trainControl(method = "cv", number = 5),
interaction.depth = 1, shrinkage = .3, n.minobsinnode = 5,
seed = 123
)
train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
interaction.depth = 1, shrinkage = .3, n.minobsinnode = 5,
seed = 123
)
train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
interaction.depth = 1, shrinkage = .1, n.minobsinnode = 5,
seed = 123
)
warnings()
train(
x = subset(ames_train, select = -Sale_Price) %>% as.data.frame(),
y = ames_train$Sale_Price,
method = "gbm",
distribution = "gaussian",
.interaction.depth = 1, .shrinkage = .1, .n.minobsinnode = 5,
seed = 123
)
subset(ames_train, select = -Sale_Price) %>% as.data.frame()
x <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
y <- ames_train$Sale_Price
train(
x = x,
y = y,
method = "gbm",
distribution = "gaussian")
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
gbm_grid
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = gbm_grid,
seed = 123,
verbose = FALSE
)
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE"
)
train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid)
gbm_tune$bestTune
gbm_grid
kfold <- trainControl(method = "cv", number = 5)
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid,
trControl = kfold,
seed = 123
)
gbm_grid
gbm_grid[1,]
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid[1, ],
seed = 123
)
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid[1, ]
)
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid[1, ],
trControl = kfold,
verbose = FALSE
)
gbm_tune$bestTune
ggplot(gbm_tune)
gbm_grid <- expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = 5000
)
gbm_tune
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)
system.time({gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid,
trControl = kfold,
verbose = FALSE
)})
3049.576/60
gbm_tune$bestTune
ggplot(gbm_tune)
gbm_tune
gbm_tune$bestTune
summary(gbm_tune)
gbm_tune$results$RMSE
min(gbm_tune$results$RMSE)
gbm_grid2 <- expand.grid(
interaction.depth = seq(5, 9, by = 2),
shrinkage = c(.01, .05),
n.minobsinnode = c(1, 3, 5),
n.trees = 10000
)
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)
gbm_tune2 <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid2,
trControl = kfold,
verbose = FALSE
)
min(gbm_tune$results$RMSE)
gbm_tune2$bestTune
round(min(gbm_tune$results$RMSE), 0)
min(gbm_tune2$results$RMSE)
xgb_grid <- expand.grid(
nrounds = 5000,
max_depth = seq(3, 7, by = 2),
eta = c(.3, .1, .01),
gamma = 0,
colsample_bytree = c(.8, 1),
min_child_weight = c(1, 3, 5),
subsample = c(.65, .8)
)
xgb_grid <- xgb_grid[1,]
xgb_tune <- train(
x = features,
y = response,
method = "xgbTree",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = xgb_grid,
trControl = kfold,
verbose = FALSE
)
xgb_tune <- train(
x = features,
y = response,
method = "xgbTree",
#distribution = "gaussian",
metric = "RMSE",
tuneGrid = xgb_grid,
trControl = kfold,
verbose = FALSE
)
xgb_tune <- train(
x = features,
y = response,
method = "xgbTree"
)
warning
warnings()
iris
iris[, -4] %>% head()
iris[, -5] %>% head()
iris[, -5] %>% as.matrix()
features <- iris[, -5] %>% as.matrix()
response <- iris$Sepal.Length
features <- iris[, -5] %>% head
iris[, -5] %>% head
iris[, -1] %>% head
features <- iris[, -1] %>% as.matrix()
xgb_tune <- train(
x = features,
y = response,
method = "xgbTree"
)
warnings()
iris[, -1] %>% dummyVars()
dummyVars(iris[, -1])
dummyVars()
?dummyVars()
dummyVars(Sepal.Length ~ ., iris)
features <- predict(dummyVars(Sepal.Length ~ ., iris), iris)
features %>% head()
xgb_tune <- train(
x = features,
y = response,
method = "xgbTree"
)
xgb_tune
gitr::git_rdone("added more GBM code")
