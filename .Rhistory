) +
xlab(expression(theta[1])) +
ylab(expression(theta[2])) +
geom_point(data = random_walk, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
geom_point(data = optimal, aes(x, y), size = 4, shape = 21, fill = "yellow") +
geom_path(data = random_walk, aes(x, y), lty = "dotted") +
annotate("text", x = random_walk[1, "x"], y = random_walk[1, "y"], label = "Start", hjust = 1, vjust = -1) +
annotate("text", x = optimal[1, "x"], y = optimal[1, "y"], label = "Minimum", hjust = -1, vjust = 1) +
ylim(c(0, 28)) +
xlim(-5, 5)
ggplot(df, aes(x, y)) +
coord_polar() +
theme_minimal() +
theme(
axis.ticks = element_blank(),
axis.text = element_blank()
) +
xlab(expression(theta[1])) +
ylab(expression(theta[2])) +
geom_point(data = random_walk, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
geom_point(data = optimal, aes(x, y), size = 4, shape = 21, fill = "yellow") +
geom_path(data = random_walk, aes(x, y), lty = "dotted") +
annotate("text", x = random_walk[1, "x"], y = random_walk[1, "y"], label = "Start", hjust = 1, vjust = -1) +
annotate("text", x = optimal[1, "x"], y = optimal[1, "y"], label = "Minimum", hjust = -.2, vjust = 1) +
ylim(c(0, 28)) +
xlim(-5, 5)
gitr::git_rdone("added gradient descent section")
cite(gbm)
cite("gbm")
citation("gbm")
xaringan::inf_mr()
gbm_mod1 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
print(gbm_mod1)
sqrt(min(gbm_mod1$cv.error))
gbm.perf(gbm_mod1, method = "cv")
set.seed(123)
# train GBM model
gbm_mod2 <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
gbm.perf(gbm_mod2, method = "cv")
gbm_grid <- expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = 5000
)
gbm_grid[1:2, ]
gbm_grid <- gbm_grid[1:2, ]
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)
# cross validated model
gbm_tune <- train(
x = features,
y = response,
method = "gbm",
distribution = "gaussian",
metric = "RMSE",
tuneGrid = gbm_grid,
trControl = kfold,
verbose = FALSE
)
gbm_tune
gbm_tune$results
knitr::include_graphics("illustrations/gbm-xval-tuned.png")
set.seed(123)
# train GBM model
gbm_final_fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = 5,
n.cores = NULL,
verbose = FALSE
)
gbm_final_fit
plot(gbm_final_fit)
gbm_tune$bestTune
gbm_tune$modelInfo
set.seed(123)
# train GBM model
gbm_final_fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 5000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = 5,
cv.folds = 5,
n.cores = NULL,
verbose = FALSE
)
gbm.perf(gbm_final_fit, method = "cv")
gbm_final_fit$n.trees
set.seed(123)
# train GBM model
gbm_final_fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 4911,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = 5,
n.cores = NULL,
verbose = FALSE
)
vip(gbm_final_fit)
vip(gbm_final_fit)
?vip(gbm_final_fit)
?vip(gbm_final_fit, method = "relative.influence")
vip(gbm_final_fit, method = "relative.influence")
vip(gbm_final_fit, method = "model")
vip(gbm_final_fit, method = "permute")
par(mfrow = c(1, 2), mar = c(5, 10, 1, 1))
# relative influence approach
summary(gbm.fit.final, cBars = 10, method = relative.influence, las = 2)
# permutation approach
summary(gbm.fit.final, cBars = 10, method = permutation.test.gbm, las = 2)
par(mfrow = c(1, 2), mar = c(5, 10, 1, 1))
# relative influence approach
summary(gbm_fit_final, cBars = 10, method = relative.influence, las = 2)
# permutation approach
summary(gbm_fit_final, cBars = 10, method = permutation.test.gbm, las = 2)
par(mfrow = c(1, 2), mar = c(5, 10, 1, 1))
# relative influence approach
summary(gbm_final_fit, cBars = 10, method = relative.influence, las = 2)
# permutation approach
summary(gbm_final_fit, cBars = 10, method = permutation.test.gbm, las = 2)
# PDP plot
gbm_pdp <- gbm_final_fit %>%
partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100) %>%
autoplot(rug = TRUE, train = ames_train) +
ggtitle("PDP plot") +
scale_y_continuous(labels = scales::dollar)
# ICE curves
gbm_ice <- gbm_final_fit %>%
partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
ggtitle("Centered ICE curves") +
scale_y_continuous(labels = scales::dollar)
gridExtra::grid.arrange(gbm_pdp, gbm_ice, nrow = 1)
gridExtra::grid.arrange(gbm_pdp, gbm_ice, nrow = 1)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
autoplot() +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(center = TRUE) +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
autoplot() +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
p1 <- gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(ice = TRUE) +
scale_y_continuous(labels = scales::dollar)
p2 <- gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gridExtra::grid.arrange(p1, p2, ncol = 1)
p1 <- gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(ice = TRUE, center = TRUE) +
scale_y_continuous(labels = scales::dollar)
p2 <- gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gridExtra::grid.arrange(p1, p2, ncol = 1)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(ice = TRUE, center = TRUE, alpha = .3) +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(ice = TRUE, center = TRUE, alpha = .1) +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(center = TRUE, alpha = .02) +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
autoplot(center = TRUE, alpha = .03) +
scale_y_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point(alpha = .2) +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
geom_point() +
ylab(NULL) +
scale_x_continuous(labels = scales::dollar)
gitr::git_rdone("finished text in gbm modeling section and started interpretation section")
devtools::install_github("dkahle/ggmap", ref = "tidyup")
expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = c(5000, 10000)
) %>% nrows
nrows(expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = c(5000, 10000)
))
nrow(expand.grid(
interaction.depth = seq(1, 5, by = 2),
shrinkage = c(.01, .1, .3),
n.minobsinnode = c(5, 10, 15),
n.trees = c(5000, 10000)
))
library(ggmap)
us <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(us, zoom = 5, maptype = "toner-lite")
ggmap(map)
library("dplyr")
library("forcats")
# define helper
`%notin%` <- function(lhs, rhs) !(lhs %in% rhs)
# reduce crime to violent crimes in downtown houston
violent_crimes <- crime %>%
filter(
offense %notin% c("auto theft", "theft", "burglary"),
-95.39681 <= lon & lon <= -95.34188,
29.73631 <= lat & lat <=  29.78400
) %>%
mutate(
offense = fct_drop(offense),
offense = fct_relevel(offense,
c("robbery", "aggravated assault", "rape", "murder")
)
)
violent_crimes
crime
`%notin%` <- function(lhs, rhs) !(lhs %in% rhs)
crime %>%
filter(
offense %notin% c("auto theft", "theft", "burglary"),
-95.39681 <= lon & lon <= -95.34188,
29.73631 <= lat & lat <=  29.78400
)
crime %>%
head()
library(dplyr)
library(dplyr)
library(forcats)
library(ggmap)
`%notin%` <- function(lhs, rhs) !(lhs %in% rhs)
# reduce crime to violent crimes in downtown houston
violent_crimes <- crime %>%
filter(
offense %notin% c("auto theft", "theft", "burglary"),
-95.39681 <= lon & lon <= -95.34188,
29.73631 <= lat & lat <=  29.78400
) %>%
mutate(
offense = fct_drop(offense),
offense = fct_relevel(offense,
c("robbery", "aggravated assault", "rape", "murder")
)
)
violent_crimes
violent_crimes %>% head()
AmesHousing::make_ames() %>% names()
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train))
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)  # for data splitting
library(gbm)      # for GBM implementation
library(caret)    # for automated hyperparameter tuning
library(pdp)      # for partial dependence plots
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
set.seed(123)
# train GBM model
gbm_final_fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 4911,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = 5,
n.cores = NULL,
verbose = FALSE
)
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train))
AmesHousing::ames_geo %>% head()
ames_train %>% distinct(Neighborhood, Latitude, Longitude)
ames_train %>% select(Neighborhood, lat = Latitude, lon = Longitude)
ames_train %>% select(Neighborhood, lat = Latitude, lon = Longitude) %>% distinct()
ames_train %>% select(Neighborhood, lat = Latitude, lon = Longitude) %>% distinct(Neighborhood)
ames_train %>% select(Neighborhood, lat = Latitude, lon = Longitude) %>% unique()
ames_train %>%
select(Neighborhood)
ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
distinct(Neighborhood)
ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
unique()
ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
distinct()
ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
distinct() %>% as.data.frame() %>% head()
geo <- ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood))
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train))
AmesHousing::ames_schools
AmesHousing::ames_geo
?AmesHousing::ames_geo
ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
group_by(Neighborhood) %>%
summarise(lat = median(lat), lon = median(lon))
geo <- ames_train %>%
select(Neighborhood, lat = Latitude, lon = Longitude) %>%
mutate(Neighborhood = as.character(Neighborhood)) %>%
group_by(Neighborhood) %>%
summarise(lat = median(lat), lon = median(lon))
gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
left_join(geo)
df <- gbm_final_fit %>%
partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
inner_join(geo)
qmplot(lon, lat, data = df, geom = "blank",
zoom = 15, maptype = "toner-background", darken = .7, legend = "topleft"
) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA)
qmplot(lon, lat, data = df, geom = "blank", maptype = "toner-background", darken = .7, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Partial\nDependence", low = "white", mid = "yellow", high = "red", midpoint = 650)
qmplot(lon, lat, data = df, geom = "blank", maptype = "toner-background", darken = .7, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Partial\nDependence", low = "white", mid = "yellow", high = "red", midpoint = 200)
qmplot(lon, lat, data = df, geom = "blank", maptype = "toner-background", darken = .7, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Partial\nDependence", low = "white", mid = "yellow", high = "red", midpoint = 150)
qmplot(lon, lat, data = df, geom = "blank", maptype = "toner-background", darken = .7, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Partial\nDependence", low = "white", mid = "yellow", high = "red", midpoint = 175)
qmplot(lon, lat, data = df, geom = "blank", maptype = "toner-background", darken = .5, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Partial\nDependence", low = "white", mid = "yellow", high = "red", midpoint = 175)
gitr::git_rdone("finished first draft of GBM chapter")
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(dplyr) # data wrangling
library(h2o)   # data modeling
# launch h2o
h2o.no_progress()
h2o.init()
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
library(rsample)  # for data splitting
library(dplyr)    # data wrangling
library(h2o)      # data modeling
# launch h2o
h2o.no_progress()
h2o.init()
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# convert data to h2o objects
train_h2o <- as.h2o(ames_train)
test_h2o <- as.h2o(ames_test)
# get response and predictor names
response <- "Sale_Price"
features <- setdiff(names(ames_train), response)
dnn_fit <- h2o.deeplearning(
x = features,
y = response,
training_frame = train_h2o,
distribution = "gaussian",        # output is continuous
hidden = c(200, 200)              # two hidden layers
)
dnn_fit
?h2o.deeplearning
dim(ames_train)
5902/32
2051/32
nrow(ames_train)
round(nrow(ames_train) / 32, 0)
dnn_fit <- h2o.deeplearning(
x = features,
y = response,
training_frame = train_h2o,
distribution = "gaussian",                   # output is continuous
hidden = c(200, 200),                        # two hidden layers
activation = "Rectifier",                    # hidden layer activation f(x)
loss = "Automatic",                          # loss function is MSE
mini_batch_size = 32,                        # batch sizes
epochs = 20,                                 # of epochs
nfolds = 5,                                  # 5-fold CV
keep_cross_validation_predictions = TRUE,    # retain CV prediction values
seed = 123                                   # for reproducibility
)
h2o.performance(model, xval = TRUE)
h2o.performance(dnn_fit, xval = TRUE)
fit2 <- h2o.deeplearning(
x = features,
y = response,
training_frame = train_h2o,
distribution = "gaussian",
hidden = c(500, 250, 125),                   # deeper network
activation = "Rectifier",
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,                                # increased epochs
nfolds = 5,
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_metric = "RMSE",                    # stopping mechanism
stopping_rounds = 2,                         # number of rounds
stopping_tolerance = 0.01                    # looking for 1% improvement
)
?h2o.rmse
h2o.rmse(fit2, xval = TRUE)
h2o.rmse(fit2, train = TRUE, xval = TRUE)
gitr::git_rdone("started deep learning chapter")
