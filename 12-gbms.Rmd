# Gradient Boosting Machines {#GBM}

```{r gbm-ch12-setup, include=FALSE}

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE, 
  message = FALSE, 
  collapse = TRUE, 
  fig.align = "center",
  fig.height = 3.5
)
```

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests (Chapter \@ref(RF)) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This chapter will cover the fundamentals to understanding and implementing GBMs.

## Prerequisites

For this chapter we’ll use the following packages:

```{r gbm-prereq-pkgs}
library(rsample)  # data splitting
library(gbm)
library(caret)
library(vip)
library(pdp)
```


To illustrate the various concepts we’ll continue focusing on the Ames Housing data (regression); however, at the end of the chapter we’ll also fit a GBM model to the employee attrition data (classification).

```{r gbm-prereq-data}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


## The basic idea

Thus far, we have discussed several supervised machine learning algorithms that are founded on a single predictive model such as linear and logistic regression ($\S$\@ref(linear-regression), $\S$\@ref(logistic-regression)), regularized regression ($\S$\@ref(regularized-regression)), and multivariate adaptive regression splines ($\S$\@ref(MARS)). We have also introduced other approaches such as bagging and random forests ($\S$\@ref(RF)) that are built on the idea of building an ensemble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values. The family of boosting methods is based on a different, constructive strategy of ensemble formation.

The main idea of boosting is to add new models to the ensemble ___sequentially___. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.  

```{r sequential-fig, echo=FALSE, fig.align='center', fig.cap="Sequential ensemble approach.", out.height="75%", out.width="75%"}
knitr::include_graphics("illustrations/boosted-trees-process.png")
```

Let's discuss each component of the previous sentence in closer detail because they are important.

__Base-learning models__:  Boosting is a framework that iteratively improves _any_ weak learning model.  Many gradient boosting applications allow you to "plug in" various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this chapter will discuss boosting in the context of decision trees.

__Training weak models__: A weak model is one whose error rate is only slightly better than random guessing.  The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors.  With regards to decision trees, shallow trees represent a weak learner.  Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits:

- Speed: Constructing weak models is computationally cheap. 
- Accuracy improvement: Weak models allow the algorithm to _learn slowly_; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.
- Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).

__Sequential training with respect to errors__: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where _x_ represents our features and _y_ represents our response:

1. Fit a decision tree to the data: $F_1(x) = y$,
2. We then fit the next decision tree to the residuals of the previous: $h_1(x) = y - F_1(x)$,
3. Add this new tree to our algorithm: $F_2(x) = F_1(x) + h_1(x)$,
4. Fit the next decision tree to the residuals of $F_2$: $h_2(x) = y - F_2(x)$,
5. Add this new tree to our algorithm: $F_3(x) = F_2(x) + h_1(x)$,
6. Continue this process until some mechanism (i.e. cross validation) tells us to stop.

The basic algorithm for boosted decision trees can be generalized to the following where the final model is simply a stagewise additive model of *b* individual trees:

$$ f(x) =  \sum^B_{b=1}f^b(x) \tag{1} $$

To illustrate the behavior, assume the following *x* and *y* observations.  The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise).  The boosted prediction illustrates the adjusted predictions after additional sequential trees are added to the algorithm.  Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. If enough trees are added, the algorithm can overfit. As Figure \@ref(boosted-tree) shows, the predicted values approximately converge to the true underlying function at around 50-100 iterations; however, we definitely see that at larger iterations (500-1000) the predicted value becomes highly variable to the noise in the data.

```{r boosted-tree, echo=FALSE, fig.align='center', fig.cap="Boosted regression tree predictions illustrating how gradient boosted trees sequentially reduce the error.", fig.height=6, fig.width=9}
# Nicer color palette
cols <- RColorBrewer::brewer.pal(9, "Set1")

# Function to implement gradient boosting with squared-error loss. Based on
# algorithm 17.2 on page 333 of Computer Age Statistical Inference, by Bradley
# Efron amd Trevor Hastie
rpartBoost <- function(X, y, data, num_trees = 100, learn_rate = 0.1, 
                       tree_depth = 6, verbose = FALSE) {
  require(rpart)
  G_b_hat <- matrix(0, nrow = length(y), ncol = num_trees + 1)
  r <- y
  for (tree in seq_len(num_trees)) {
    if (verbose) {
      message("iter ", tree, " of ", num_trees)
    }
    g_b_tilde <- rpart(r ~ X, control = list(cp = 0, maxdepth = tree_depth))
    g_b_hat <- learn_rate * predict(g_b_tilde)
    G_b_hat[, tree + 1] <- G_b_hat[, tree] + matrix(g_b_hat)
    r <- r - g_b_hat
  }
  colnames(G_b_hat) <- paste0("tree_", c(0, seq_len(num_trees)))
  G_b_hat
}

# Function to plot the predictions from a particular boosting iteration
plotIter <- function(object, iter, show_legend = FALSE, ...) {
  plot(x, y, ...)
  lines(x, sin(x), lwd = 3, col = cols[2L])
  lines(x, object[, iter + 1], lwd = 3, col = cols[1L])
  if (show_legend) {
    legend("topright", legend = c("Boosted prediction", "True function"),
           lty = 1L, lwd = 3L, col = cols[1L:2L], inset = 0.01)
  }
}

# Simulate some sine wave data
set.seed(101)
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
#plot(x, y)

# gradient boosted decision trees
bst <- rpartBoost(X = x, y = y, num_trees = 1000, learn_rate = 0.1, 
                  tree_depth = 3, verbose = FALSE)

# Plot first 15 iterations
# par(mfrow = c(3, 3))
# for (i in c(0, 5, 10, 15, 25, 50, 100, 500, 1000)) {
#   plotIter(bst, iter = i, main = paste("Iter:", i))
# }

# plot 9 different iterations
sin_data <- data.frame(x, y, sin = sin(x))

as.data.frame(bst) %>% 
  cbind(sin_data) %>%
  gather(iteration, value, tree_0:tree_1000) %>% 
  filter(iteration %in% paste0("tree_", c(0, 5, 10, 15, 25, 50, 100, 500, 1000))) %>%
  mutate(
    iteration = paste("Iteration", stringr::str_extract(iteration, "[[:digit:]]+")),
    iteration = as.factor(iteration) %>% fct_inorder()
    ) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .1) +
  geom_line(aes(x, sin), color = "blue", size = 1) +
  geom_line(aes(y = value), color = "red", size = 1) +
  facet_wrap(~ iteration)
```

## Gradient descent


## Fitting a basic GBM


```{r, eval=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_mod1 <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 10000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

print(gbm_mod1)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
```

```{r, eval=FALSE}
# get MSE and compute RMSE
sqrt(min(gbm_mod1$cv.error))
## [1] 33079.61

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod1, method = "cv")
```

```{r, echo=FALSE}
knitr::include_graphics("illustrations/gbm1_gradient_descent.png")
```


## Tuning


```{r, eval=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_mod2 <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(gbm_mod2$cv.error)

# get MSE and compute RMSE
sqrt(gbm_mod2$cv.error[min_MSE])
## [1] 23813.34

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod2, method = "cv")
```

```{r, echo=FALSE}
knitr::include_graphics("illustrations/gbm2_gradient_descent.png")
```


```{r}
gbm_grid <- expand.grid(
  interaction.depth = seq(1, 5, by = 2),
  shrinkage = c(.01, .1, .3),
  n.minobsinnode = c(5, 10, 15),
  n.trees = 5000
)
```


```{block, type="warning"}
This grid search took 51 minutes to complete.
```

```{r, eval=FALSE}
# create train() parameters
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)

# cross validated model
gbm_tune <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "gaussian",
  metric = "RMSE",
  tuneGrid = gbm_grid,
  trControl = kfold,
  verbose = FALSE
)

# plot results
ggplot(gbm_tune)
```

```{r, echo=FALSE}
knitr::include_graphics("illustrations/gbm-xval-tuned.png")
```


```{r, eval=FALSE}
# best model
gbm_tune$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 7    5000                 5      0.01              5
```

Minimum RMSE was \$22,424

```{r, eval=FALSE, echo=FALSE}
# used this code chunk to find minimum RMSE with additional parameters
gbm_grid2 <- expand.grid(
  interaction.depth = 5,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.trees = c(5000, 7500, 10000)
)

# create train() parameters
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)

# cross validated model
gbm_tune2 <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "gaussian",
  metric = "RMSE",
  tuneGrid = gbm_grid2,
  trControl = kfold,
  verbose = FALSE
)

gbm_tune2$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 3   10000                 5      0.01              5
min(gbm_tune2$results$RMSE) # --> CV RMSE was 22084.87
```


```{r, echo=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_final_fit <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 10000,
  interaction.depth = 5,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.cores = NULL, 
  verbose = FALSE
  )  
```


## Feature Interpretation


```{r}
vip(gbm_final_fit)
```


```{r}
# PDP plot
gbm_pdp <- gbm_final_fit %>%
  partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = ames_train) +
  ggtitle("PDP plot") +
  scale_y_continuous(labels = scales::dollar)

# ICE curves
gbm_ice <- gbm_final_fit %>%
  partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100, ice = TRUE) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered ICE curves") +
  scale_y_continuous(labels = scales::dollar)

gridExtra::grid.arrange(gbm_pdp, gbm_ice, nrow = 1)
```



```{r, echo=FALSE}
gbm_final_fit %>%
  partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
  autoplot() +
  scale_y_continuous(labels = scales::dollar)
```


```{r, echo=FALSE}

gbm_final_fit %>%
  partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
  ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
  geom_point() +
  ylab(NULL) +
  scale_x_continuous(labels = scales::dollar)
```


## Attrition data


```{r, eval=FALSE}
# get attrition data
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)

# create a tuning grid
gbm_grid <- expand.grid(
  interaction.depth = seq(1, 5, by = 2),
  shrinkage = c(.01, .1, .3),
  n.minobsinnode = c(5, 10, 15),
  n.trees = c(5000, 10000)
)

# create train() parameters
features <- subset(churn_train, select = -Attrition) %>% as.data.frame()
response <- churn_train$Attrition
kfold <- trainControl(method = "cv", number = 10)

# cross validated model
gbm_churn <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "bernoulli",
  tuneGrid = gbm_grid,
  trControl = kfold,
  verbose = FALSE
)

gbm_churn$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 1    5000                 1      0.01              5
min(gbm_churn$results$RMSE)
## [1] 0.8718578
```



```{r gbm-attrition-model-comparison, echo=FALSE, eval=FALSE}
# train logistic regression model
glm_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
  )

# train regularized logistic regression model
penalized_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
  )

# train mars model
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
  )
tuned_mars <- train(
  x = subset(churn_train, select = -Attrition),
  y = churn_train$Attrition,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# train rf model
hyper_grid <- expand.grid(
  mtry            = seq(3, 18, by = 3),
  min.node.size   = seq(1, 10, by = 3),
  splitrule       = c("gini", "extratrees")
  )
tuned_rf <- train(
  x = subset(churn_train, select = -Attrition),
  y = churn_train$Attrition,
  method = "ranger",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid,
  num.trees = 500,
  seed = 123
)

# train gbm model
gbm_grid <- expand.grid(
  interaction.depth = 1,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.trees = c(5000, 10000)
)
gbm_churn <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "bernoulli",
  tuneGrid = gbm_grid,
  trControl = trainControl(method = "cv", number = 10),
  verbose = FALSE
)

# extract out of sample performance measures
gbm_attrition_model_comparison <- summary(resamples(list(
  Logistic_model = glm_mod, 
  Elastic_net = penalized_mod,
  MARS_model = tuned_mars,
  RF_model = tuned_rf,
  GBM_model = gbm_churn
  )))$statistics$Accuracy

saveRDS(gbm_attrition_model_comparison, file = "data/gbm_attrition_model_comparison.rds")
```

```{r, echo=FALSE}
readRDS("data/gbm_attrition_model_comparison.rds") %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```


## Final thoughts


## Learning more
