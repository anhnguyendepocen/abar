MARS = tuned_mars
)))$statistics$RMSE
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)   # data splitting
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
p1 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("(A) Linear regression")
p2 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 2, raw = TRUE)) +
ggtitle("(B) Degree-2 polynomial regression")
p3 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 3, raw = TRUE)) +
ggtitle("(C) Degree-3 polynomial regression")
# fit step function model (3 steps)
step_fit <- lm(Sale_Price ~ cut(ames_train$Year_Built, 3), data = ames_train)
step_pred <- predict(step_fit, ames_train)
p4 <- ggplot(cbind(ames_train, step_pred), aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
geom_line(aes(y = step_pred), size = 1, color = "blue") +
ggtitle("(D) Step function regression")
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
# Fit a second-degree MARS model
ames_mars <- earth(
Sale_Price ~ .,  #<<
data = ames_train,
degree = 2  # tuning parameter #<<
)
# Print model summary
print(ames_mars)
summary(ames_mars)
tidy(ames_mars)
ames_mars %>% broom::tidy()
summary(ames_mars) %>% str
summary(ames_mars) %>% .$coefficients
summary(ames_mars) %>% .$coefficients %>% head()
summary(ames_mars) %>% .$coefficients %>% head()
summary(ames_mars) %>% .$coefficients %>% head(10)
plot(ames_mars)
ggplot(ames_mars)
?plot.earth.models
plot.earth.models(ames_mars)
plot.earth(ames_mars)
?plot.earth
plot(ames_mars, which = 1)
plot(ames_mars, which = 1)
plot(ames_mars, which = 1)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
head(hyper_grid)
library(Boruta)
library(AmesHousing)
library(rsample)
library(ranger)
set.seed(123)
split <- initial_split(rsample::attrition, prop = .7, strata = "Attrition")
train <- training(split)
test <- testing(split)
system.time({Boruta_df <- Boruta(Attrition ~ ., data = train, doTrace = 1, ntree = 500)})
set.seed(123)
split <- initial_split(AmesHousing::make_ames(), prop = .7)
train <- training(split)
test <- testing(split)
# number of features
features <- setdiff(names(train), "Sale_Price")
system.time({Boruta_df <- Boruta(Sale_Price ~ ., data = train, doTrace = 1, ntree = 500)})
271.551/60
df <- data.table::fread("./Effoverse/data/sow_data.csv")
df <- data.table::fread("../Effoverse/data/sow_data.csv")
dim(df)
Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df, doTrace = 1, ntree = 500)
Boruta_df
system.time({Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df[1:1000, 1:50], doTrace = 1, ntree = 500)})
system.time({Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df[1:1000, c(1:50, 501)], doTrace = 1, ntree = 500)})
system.time({Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df[1:1000, c(1:50, 501)], doTrace = 1, ntree = 500, nthreads = 1)})
?ranger::ranger
system.time({Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df[1:1000, c(1:50, 501)], doTrace = 1, ntree = 500, num.threads = 1)})
system.time({Boruta_df <- Boruta(NIEL_COMP_SPEND ~ ., data = df[1:1000, c(1:50, 501)], doTrace = 1, ntree = 500, num.threads = 8)})
?earth::earth
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 2, pmethod = "forward")
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)   # data splitting
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 2, pmethod = "forward")
mars
mars1
summary(mars1)
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 4, pmethod = "forward")
summary(mars1)
mars1$cuts
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 2, pmethod = "forward")
mars1$cuts
mars1$cuts
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 2, pmethod = "forward")
mars1
mars1$cuts
mars1$dirs
m1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 2, pmethod = "forward")
m1$cuts
m1$prune.terms
m1$bx
m1$fitted.values
m1$coefficients
model.matrix(m1)
model.matrix(m1) %>% head()
citation(package = "earth")
m2 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 1, nprune = 3, pmethod = "forward")
m2$coefficients
mars9 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 10, nprune = 10, pmethod = "forward")
mars9$coefficients
mars9 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, nfold = 10)
mars9$coefficients
mars9$cv.groups
summary(m9)
summary(mars9)
ggplot(mutate(ames_train, predicted = pred9), aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
geom_line(aes(y = predicted), size = 1, color = "blue")
library(dplyr)
ggplot(mutate(ames_train, predicted = pred9), aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
geom_line(aes(y = predicted), size = 1, color = "blue")
pred9 <- predict(mars9, ames_train)
ggplot(mutate(ames_train, predicted = pred9), aes(Year_Built, Sale_Price)) +
geom_point(size = 1, alpha = .2) +
geom_line(aes(y = predicted), size = 1, color = "blue")
# Fit a second-degree MARS model
ames_mars <- earth(
Sale_Price ~ .,
data = ames_train
)
# Print model summary
print(ames_mars)
summary(ames_mars)
summary(ames_mars) %>% .$coefficients %>% head(10)
plot(ames_mars, which = 1)
# Fit a second-degree MARS model
ames_mars <- earth(
Sale_Price ~ .,
data = ames_train,
degree = 2
)
# Print model summary
print(ames_mars)
ames_mars$coefficients
ames_mars$coefficients %>% nrow()
# Fit a basic MARS model
ames_mars <- earth(
Sale_Price ~ .,
data = ames_train,
degree = 2
)
# check out the first 10 coefficient terms
summary(ames_mars) %>% .$coefficients %>% head(10)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
head(hyper_grid)
nrow(hyper_grid)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars
# extract out of sample performance measures
summary(resamples(list(
Multiple_regression = cv_model1,
PCR = cv_model2,
PLS = cv_model3,
Elastic_net = cv_model4,
MARS = tuned_mars
)))$statistics$RMSE %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "hover"))
install.packages("kableExtra")
citation(package = "earth")
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
library(rsample)   # data splitting
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
library(vip)       # variable importance
library(pdp)       # variable relationships
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p
p1
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% autoplot()
p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>%
plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
varImp(tuned_mars)
?varImp.earth
varImp(tuned_mars) %>% head()
varImp(tuned_mars) %>% str()
varImp(tuned_mars)$importance %>% head()
varImp(tuned_mars, value = "rss")$importance %>% head()
varImp(tuned_mars, value = "gcv")$importance %>% head()
vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
?vip
?vi
?vip::vi
#rtance plots
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
gridExtra::grid.arrange(p1, p2, ncol = 2)
vi(tuned_mars, num_features = 40, bar = FALSE, value = "gcv")
vi(tuned_mars, num_features = 40, bar = FALSE, value = "rss")
vi(tuned_mars, num_features = 26, bar = FALSE, value = "rss")
vi(tuned_mars, num_features = 26, bar = FALSE, value = "rss") %>% as.data.frame()
coef(tuned_mars$finalModel)
coef(tuned_mars$finalModel) %>% tidy()
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "\*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "Year"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, [[:punct:]]))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "[[:punct:]]"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "*"))
coef(tuned_mars$finalModel) %>%
tidy() %>%
filter(stringr::str_detect(names, "\\*"))
coef(tuned_mars$finalModel)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# for reproducibiity
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# for reproducibiity
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
metric = "AUC",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars$bestTune
ggplot(tuned_mars)
tuned_mars
tuned_mars$bestTune
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
summary(resamples(list(
logistic_model = glm_mod,
penalized_model = penalized_mod,
MARS_model = tuned_mars,
)))$statistics$Accuracy
penalized_mod
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
)))$statistics$Accuracy
tuned_mars
glm_mod
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
)))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars
)))$statistics$Accuracy
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
library(rsample)
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# for reproduciblity
set.seed(123)
# default RF model
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
library(randomForest) # basic implementation
# for reproduciblity
set.seed(123)
# default RF model
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
m1
sqrt(661089658)
plot(m1)
# number of trees with lowest MSE
which.min(m1$mse)
# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
system.time(
ames_randomForest <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train,
ntree   = 500,
mtry    = floor(length(features) / 3)
)
)
# ranger speed
system.time(
ames_ranger <- ranger(
formula   = Sale_Price ~ .,
data      = ames_train,
num.trees = 500,
mtry      = floor(length(features) / 3)
)
)
library(ranger)
features <- setdiff(names(ames_train), "Sale_Price")
# randomForest speed
system.time(
ames_randomForest <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train,
ntree   = 500,
mtry    = floor(length(features) / 3)
)
)
# ranger speed
system.time(
ames_ranger <- ranger(
formula   = Sale_Price ~ .,
data      = ames_train,
num.trees = 500,
mtry      = floor(length(features) / 3)
)
)
35.292 / 1.298
ames_randomForest
ames_ranger
citation("randomForest")
citation("ranger")
?ranger
floor(length(features) / 3)
seq(1, length(features), length.out = 5)
seq(1, length(features), length.out = 6)
seq(1, length(features), length.out = 8)
modelLookup("ranger")
library(caret)
modelLookup("ranger")
# create a tuning grid
hyper_grid <- expand.grid(
.mtry = seq(1, length(features), length.out = 5),
.splitrule = c("variance", "extratrees"),
.min.node.size = c(1, 3, 5, 10)
)
head(hyper_grid)
hyper_grid
hyper_grid[1:2,]
tuned_mars <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid[1:2,]
)
tuned_mars
# create a tuning grid
hyper_grid <- expand.grid(
mtry = seq(1, length(features), length.out = 5),
splitrule = c("variance", "extratrees"),
min.node.size = c(1, 3, 5, 10)
)
head(hyper_grid)
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid[1:2,]
)
tuned_rf
tuned_rf$bestTune
ggplot(tuned_rf)
set.seed(123)
# cross validated model
tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_rf
tuned_rf$bestTune
ggplot(tuned_rf)
system.time({tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
respect.unordered.factors = 'order',
seed = 123
)})
837.022/60
tuned_rf$bestTune
tuned_rf
ggplot(tuned_rf)
