# launch h2o
h2o.no_progress()
h2o.init()
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# convert data to h2o objects
train_h2o <- ames_train %>% mutate(Sale_Price = log(Sale_Price)) %>% as.h2o()
test_h2o <- ames_test %>% mutate(Sale_Price = log(Sale_Price)) %>% as.h2o()
# get response and predictor names
response <- "Sale_Price"
features <- setdiff(names(ames_train), response)
fit3 <- h2o.deeplearning(
x = features,
y = response,
training_frame = train_h2o,
distribution = "gaussian",
hidden = c(500, 250, 125),
activation = "RectifierWithDropout",  # need to specify new activation f(x)
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
nfolds = 5,
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
input_dropout_ratio = 0.1,             # 10% dropout of input variables
hidden_dropout_ratios = c(.2, .2, .2)  # 20% dropout of hidden nodes
)
h2o.rmse(fit3, train = TRUE, xval = TRUE)
fit4 <- h2o.deeplearning(
x = features,
y = response,
training_frame = train_h2o,
distribution = "gaussian",
hidden = c(500, 250, 125),
activation = "RectifierWithDropout",
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
nfolds = 5,
keep_cross_validation_predictions = TRUE,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
input_dropout_ratio = 0.1,
hidden_dropout_ratios = c(.2, .2, .2),
l2 = 0.001                               # add weight decay
)
h2o.rmse(fit4, train = TRUE, xval = TRUE)
hyper_params <- list(
hidden = list(c(500, 250, 150), c(250, 125)),
activation = c("Rectifier", "RectifierWithDropout"),
input_dropout_ratio = c(0, 0.05)
)
# Rather than comparing models by using cross-validation (which is “better” but
# takes longer), we will simply partition our training set into two pieces – one
# for training and one for validiation.
splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 1)
# train the grid
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = predictors,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
dl_gridperf <- h2o.getGrid(
grid_id = "dl_grid_full",
sort_by = "mse",
decreasing = TRUE
)
# you can look at the rank-order of the models
# print(dl_gridperf)
# Grab the model_id for the top model, chosen by validation error
best_dl_model_id <- dl_gridperf@model_ids[[1]]
best_dl <- h2o.getModel(best_dl_model_id)
# assess the performance
h2o.rmse(best_dl, train = TRUE, xval = TRUE)
h2o.rmse(best_dl, train = TRUE, valid = TRUE)
hyper_params <- list(
hidden = list(c(500, 250, 150), c(250, 125)),
activation = c("Rectifier", "RectifierWithDropout"),
input_dropout_ratio = c(0.05, 0.1),
hidden_dropout_ratios = c(.1, .2)
)
# Rather than comparing models by using cross-validation (which is “better” but
# takes longer), we will simply partition our training set into two pieces – one
# for training and one for validiation.
splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 1)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
dl_grid2 <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
# Rather than comparing models by using cross-validation (which is “better” but
# takes longer), we will simply partition our training set into two pieces – one
# for training and one for validiation.
splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 1)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full2",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
hyper_params <- list(
hidden = list(c(500, 250, 150), c(250, 150, 50), c(250, 125)),
activation = c("Rectifier", "RectifierWithDropout", "Maxout", "MaxoutWithDropout"),
input_dropout_ratio = c(0, 0.05),
l1 = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1),
l2 = c(0, 0.00001, 0.0001, 0.001, 0.01, 0.1)
)
# Rather than comparing models by using cross-validation (which is “better” but
# takes longer), we will simply partition our training set into two pieces – one
# for training and one for validiation.
splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 1)
# For a larger search space we can use random grid search
search_criteria <- list(
strategy = "RandomDiscrete",
max_runtime_secs = 360,
max_models = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 5,
stopping_tolerance = 0.01
)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_random",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
hyper_params = hyper_params,
search_criteria = search_criteria
)
dl_gridperf <- h2o.getGrid(
grid_id = "dl_grid_random",
sort_by = "mse",
decreasing = TRUE
)
best_dl_model_id <- dl_gridperf@model_ids[[1]]
best_dl <- h2o.getModel(best_dl_model_id)
# assess the performance
h2o.performance(best_dl, valid = TRUE)
hyper_params <- list(
hidden = list(c(500, 250, 150), c(250, 125)),
activation = c("Rectifier", "RectifierWithDropout"),
input_dropout_ratio = c(0.05, 0.1),
hidden_dropout_ratios = list(c(.2, .2, .2), c(.3, .3, .3))
)
# Rather than comparing models by using cross-validation (which is “better” but
# takes longer), we will simply partition our training set into two pieces – one
# for training and one for validiation.
splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 1)
dl_grid <- h2o.grid(
algorithm = "deeplearning",
x = features,
y = response,
distribution = "gaussian",
grid_id = "dl_grid_full2",
training_frame = splits[[1]],
validation_frame = splits[[2]],
keep_cross_validation_predictions = TRUE,
loss = "Automatic",
mini_batch_size = 32,
epochs = 100,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 2,
stopping_tolerance = 0.01,
hyper_params = hyper_params   # search space
)
dl_grid
dl_gridperf <- h2o.getGrid(
grid_id = "dl_grid_full2",
sort_by = "mse",
decreasing = TRUE
)
best_dl_model_id <- dl_gridperf@model_ids[[1]]
best_dl <- h2o.getModel(best_dl_model_id)
# assess the performance
h2o.rmse(best_dl, train = TRUE, valid = TRUE)
fit3
getwd()
saveRDS(fit3, file = "data/h2o_dl_ames_model.rds")
fit3@model
fit3@model$weights
fit3@model$cross_validation_predictions
fit3@model$cross_validation_predictions[1]
fit3@model$cross_validation_predictions[[1]]
fit3@model$cross_validation_predictions[[1]]$name
fit3@model$cross_validation_predictions[[2]]$`__meta`
fit3@model$cross_validation_predictions[[2]]$type
fit3@model$cross_validation_predictions[[2]]$URL
predict(fit3)
predict(fit3, train_h2o)
predict(fit3, train_h2o) %>%
as.data.frame() %>%
mutate(
predict_tran = exp(predict),
truth = ames_train$Sale_Price
)
# use model 3 as our best model
best_model <- fit3
predict(best_model, train_h2o) %>%
as.data.frame() %>%
mutate(
predict_tran = exp(predict),
truth = ames_train$Sale_Price
) %>%
yardstick::rmse(truth, predict_tran)
predict(best_model, test_h2o) %>%
as.data.frame() %>%
mutate(
predict_tran = exp(predict),
truth = ames_test$Sale_Price
) %>%
yardstick::rmse(truth, predict_tran)
best_model@model[["cross_validation_predictions"]]
h2o.getFrame(best_model@model[["cross_validation_predictions"]][["name"]])
h2o.getFrame(best_model@model[["prediction_DeepLearning_model_R_1542196950460_8_cv_1"]][["name"]])
best_model@model[["prediction_DeepLearning_model_R_1542196950460_8_cv_1"]]
best_model@model
best_model@model$cross_validation_predictions
best_model@model$cross_validation_predictions$name
best_model@model$cross_validation_predictions[[1]]
best_model@model$cross_validation_predictions[[1]]$name
h2o.getFrame(best_model@model$cross_validation_predictions[[1]]$name)
h2o.getFrame(best_model@model$cross_validation_predictions[[1]][["prediction_DeepLearning_model_R_1542196950460_8_cv_1"]])
h2o.getFrame(best_model@model$cross_validation_predictions[[1]][[prediction_DeepLearning_model_R_1542196950460_8_cv_1]])
h2o.getFrame(best_model@model$cross_validation_predictions[[1]]$name) %>% as.data.frame() %>% filter(predict > 0)
h2o.cross_validation_fold_assignment(best_model)
h2o.cross_validation_fold_assignment(fit3)
h2o.cross_validation_predictions(fit3)
h2o.cross_validation_holdout_predictions(fit3)
h2o.cross_validation_models(fit3)
h2o.cross_validation_holdout_predictions(fit3)
h2o.cross_validation_holdout_predictions(fit3) %>%
as.data.frame() %>%
mutate(
predict_tran = exp(predict),
truth = ames_train$Sale_Price
) %>%
yardstick::rmse(truth, predict_tran)
vip::vip(best_model)
?vip::vip(best_model)
vip::vip(best_model, num_features = 20)
pfun <- function(object, newdata) {
as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}
# compute ICE curves
best_model %>%
partial(
pred.var = "Gr_Liv_Area",
train = ames_train,
pred.fun = pfun,
grid.resolution = 100
) %>%
autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE)
library(vip)
library(pdp)
pfun <- function(object, newdata) {
as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}
# compute ICE curves
best_model %>%
partial(
pred.var = "Gr_Liv_Area",
train = ames_train,
pred.fun = pfun,
grid.resolution = 100
) %>%
autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE)
library(rsample)  # for data splitting
library(dplyr)    # for data wrangling
library(h2o)      # for data modeling
library(vip)      # for variable importance plots
library(pdp)      # for partial dependence plots
h2o.no_progress()
h2o.init()
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
# convert data to h2o objects
train_h2o <- ames_train %>% mutate(Sale_Price = log(Sale_Price)) %>% as.h2o()
test_h2o <- ames_test %>% mutate(Sale_Price = log(Sale_Price)) %>% as.h2o()
# get response and predictor names
response <- "Sale_Price"
features <- setdiff(names(ames_train), response)
fit3 <- readRDS("data/h2o_dl_ames_model.rds")
# use model 3 as our best model
best_model <- fit3
# compute cross validation error on re-transformed response variable
h2o.cross_validation_holdout_predictions(fit3) %>%
as.data.frame() %>%
mutate(
predict_tran = exp(predict),
truth = ames_train$Sale_Price
) %>%
yardstick::rmse(truth, predict_tran)
vip(best_model, num_features = 20)
pfun <- function(object, newdata) {
as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}
# compute ICE curves
best_model %>%
partial(
pred.var = "Gr_Liv_Area",
train = ames_train,
pred.fun = pfun,
grid.resolution = 20
) %>%
autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE)
library(ggplot2)
best_model %>%
partial(
pred.var = "Gr_Liv_Area",
train = ames_train,
pred.fun = pfun,
grid.resolution = 100
) %>%
autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE)
best_model %>%
partial(
pred.var = "Overall_Qual",
train = ames_train,
pred.fun = pfun
) %>%
autoplot()
best_model %>%
partial(
pred.var = "Overall_Qual",
train = ames_train,
pred.fun = pfun,
grid.resolution = 100
) %>%
autoplot()
best_model %>%
partial(
pred.var = "Overall_Qual",
train = as.data.frame(ames_train),
pred.fun = pfun
) %>%
autoplot()
pfun <- function(object, newdata) {
mean(as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]])
}
best_model %>%
partial(
pred.var = "Overall_Qual",
train = as.data.frame(ames_train),
pred.fun = pfun
) %>%
autoplot() +
ggtitle("Partial dependence plot")
pfun(best_model, as.data.frame(ames_train))
p2 <- best_model %>%
partial(
pred.var = "Overall_Qual",
train = as.data.frame(ames_train),
pred.fun = pfun
) %>%
autoplot() +
ggtitle("Partial dependence plot")
p2
gitr::git_rdone("finished neural network modeling and started feature interpretation")
knitr::include_graphics("illustrations/exemplar-decision-tree.png")
gitr::git_rdone("started incorporating decision tree updates from my slides")
.05 / (6 - 1)
rlang::fn_env(dplyr::add_count)
environment(dplyr::add_count)
library(rlang)
e <- env()
e$g <- function() 1
environment(e)
environment(e$g)
f <- function(x) {
g(x = 2)
}
g <- function(x) {
h(x = 3)
}
h <- function(x) {
stop()
}
f()
lobstr::cst(f())
install.packages("lobstr")
lobstr::cst(f())
lobstr::cst(f(1))
h <- function(x) {
lobstr::cst()
}
f(x = 1)
lobstr::cst()
mean(lobstr::cst())
mean(1, lobstr::cst())
a <- function(x) b(x)
b <- function(x) c(x)
c <- function(x) x
a(f())
?expr()
rlang::expr()
?rlang::expr()
?tidyr
?dplyr::mutate_each
rlang::expr(a + b)
