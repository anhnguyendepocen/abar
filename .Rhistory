438.590/60
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
getModelInfo("ranger")
install.packages("arules")
# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
# Set global knitr chunk options
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
message = FALSE,
collapse = TRUE,
fig.align = "center",
fig.height = 3.5
)
library(rsample)      # data splitting
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # automating the tuning process
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
knitr::include_graphics("illustrations/bagging.png")
knitr::include_graphics("illustrations/tree-correlation-1.png")
knitr::include_graphics("illustrations/oob-error-compare-1.svg")
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(20, 35, by = 5),
min.node.size   = seq(3, 9, by = 3),
sample.fraction = c(.632, .80),
splitrule       = c("variance", "extratrees"),
OOB_RMSE        = 0
)
dim(hyper_grid)
for(i in seq_len(nrow(hyper_grid))) {
# train model
model <- ranger(
formula         = Sale_Price ~ .,
data            = ames_train,
num.trees       = 500,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
sample.fraction = hyper_grid$sample.fraction[i],
splitrule       = hyper_grid$splitrule[i],
seed            = 123
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
hyper_grid %>%
dplyr::arrange(OOB_RMSE) %>%
head(10)
caret::getModelInfo("ranger")
caret::getModelInfo("ranger")$ranger$parameter
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(20, 35, by = 5),
min.node.size   = seq(3, 9, by = 3),
splitrule       = c("variance", "extratrees")
)
system.time({tuned_rf <- train(
x = subset(ames_train, select = -Sale_Price),
y = ames_train$Sale_Price,
method = "ranger",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)})
375/60/60
375/60
warnings()
tuned_rf$bestTune
ggplot(tuned_rf)
tuned_rf$results
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
formula = Sale_Price ~ .,
data            = ames_train,
num.trees       = 500,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .80,
splitrule       = "variance",
importance      = 'impurity',
verbose         = FALSE,
seed            = 123
)
# re-run model with permutation-based variable importance
rf_permutation <- ranger(
formula = Sale_Price ~ .,
data            = ames_train,
num.trees       = 500,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .80,
splitrule       = "variance",
importance      = 'permutation',
verbose         = FALSE,
seed            = 123
)
p1 <- vip(rf_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE) + ggtitle("Impurity-based variable importance")
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE) + ggtitle("Permutation-based variable importance")
gridExtra::grid.arrange(p1, p2, nrow = 1)
# partial dependence of Sale_Price on Gr_Liv_Area
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
library(vip)
library(pdp)
# partial dependence of Sale_Price on Gr_Liv_Area
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50) %>%
autoplot(rug = TRUE, train = ames_train)
rf_impurity %>%
partial(pred.var = "Overall_Qual", train = as.data.frame(ames_train)) %>%
autoplot()
rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.2) +
ggtitle("Non-centered ICE plot")
ice1 <- rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.1) +
ggtitle("Non-centered ICE plot")
ice2 <- rf_impurity %>%
partial(pred.var = "Gr_Liv_Area", grid.resolution = 50, ice = TRUE) %>%
autoplot(rug = TRUE, train = ames_train, alpha = 0.1, center = TRUE) +
ggtitle("Centered ICE plot")
gridExtra::grid.arrange(ice1, ice2, nrow = 1)
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# get attrition data
df <- rsample::attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
df
# get attrition data
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
dim(df)
sqrt(31)
seq(3, 12, by = 3)
seq(1, 9, by = 3)
seq(1, 10, by = 3)
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 12, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("variance", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
?ranger
hyper_grid <- expand.grid(
mtry            = seq(3, 12, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# best model
tuned_rf$bestTune
ggplot(tuned_rf)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
ggplot(tuned_rf)
summary(resamples(list(
rf_model = tuned_rf
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
tuned_rf
nrow(hyper_grid)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
set.seed(123)
# cross validated model
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
# train regularized logistic regression model
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# extract out of sample performance measures
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
# train logistic regression model
set.seed(123)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
# train regularized logistic regression model
set.seed(123)
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# extract out of sample performance measures
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
glm_mod
penalized_mod
tuned_mars
tuned_rf
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10,
seed = 123
)
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
seed = 123
)
tuned_rf
# create a tuning grid
hyper_grid <- expand.grid(
mtry            = seq(3, 18, by = 3),
min.node.size   = seq(1, 10, by = 3),
splitrule       = c("gini", "extratrees")
)
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
# cross validated model
tuned_rf <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "ranger",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid,
num.trees = 500,
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
seed = 123
)
warnings()
# train logistic regression model
glm_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
glm_mod
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10,
seed = 123
)
# train regularized logistic regression model
penalized_mod <- train(
Attrition ~ .,
data = churn_train,
method = "glmnet",
family = "binomial",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
penalized_mod
# train mars model
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
tuned_mars <- train(
x = subset(churn_train, select = -Attrition),
y = churn_train$Attrition,
method = "earth",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
tuned_mars
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf,
)))
summary(resamples(list(
Logistic_model = glm_mod,
Elastic_net = penalized_mod,
MARS_model = tuned_mars,
RF_model = tuned_rf
)))$statistics$Accuracy %>%
kableExtra::kable() %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
install.packages("pBrackets")
?bookdown::html_document2
install.packages("outliers")
install.packages("ISLR")
