<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 5 Linear Regression | Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</title>
  <meta name="description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 5 Linear Regression | Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Linear Regression | Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  
  <meta name="twitter:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  

<meta name="author" content="Bradley C. Boehmke and Brandon M. Greenwell">


<meta name="date" content="2019-01-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="unsupervised.html">
<link rel="next" href="logistic-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="extra.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Business Analytics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-not-for"><i class="fa fa-check"></i>Who this book is not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-really-not-for"><i class="fa fa-check"></i>Who this book is really not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i>Why R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-this-book-is-organized"><i class="fa fa-check"></i>How this book is organized</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#using-code-examples"><i class="fa fa-check"></i>Using code examples</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-information"><i class="fa fa-check"></i>Software information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>Who are these Guys?</a><ul>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#bradley-c.-boehmke"><i class="fa fa-check"></i>Bradley C. Boehmke</a></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#brandon-m.-greenwell"><i class="fa fa-check"></i>Brandon M. Greenwell</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>1.1</b> Descriptive</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#predictive"><i class="fa fa-check"></i><b>1.2</b> Predictive</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#prescriptive"><i class="fa fa-check"></i><b>1.3</b> Prescriptive</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.4</b> Data sets</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#ames-iowa-housing-data"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#employee-attrition-data"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Descriptive Analytics</b></span><ul>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#prerequisites"><i class="fa fa-check"></i><b>1.5</b> Prerequisites</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#measures-of-location"><i class="fa fa-check"></i><b>1.6</b> Measures of location</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#the-sample-mean"><i class="fa fa-check"></i><b>1.6.1</b> The sample mean</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#the-sample-median"><i class="fa fa-check"></i><b>1.6.2</b> The sample median</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#the-mean-or-the-median"><i class="fa fa-check"></i><b>1.6.3</b> The mean or the median</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#measures-of-spread"><i class="fa fa-check"></i><b>1.7</b> Measures of spread</a><ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a href="intro.html#empirical-rule"><i class="fa fa-check"></i><b>1.7.1</b> The empirical rule</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#percentiles"><i class="fa fa-check"></i><b>1.8</b> Percentiles</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#robust-measures-of-spread"><i class="fa fa-check"></i><b>1.9</b> Robust measures of spread</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#outliers"><i class="fa fa-check"></i><b>1.10</b> Outlier detection</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#categorical"><i class="fa fa-check"></i><b>1.11</b> Describing categorical data</a><ul>
<li class="chapter" data-level="1.11.1" data-path="intro.html"><a href="intro.html#contingency-tables"><i class="fa fa-check"></i><b>1.11.1</b> Contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>2</b> Visual data exploration</a><ul>
<li class="chapter" data-level="2.1" data-path="visualization.html"><a href="visualization.html#prerequisites-1"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="visualization.html"><a href="visualization.html#univariate-data"><i class="fa fa-check"></i><b>2.2</b> Univariate data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="visualization.html"><a href="visualization.html#continuous-variables"><i class="fa fa-check"></i><b>2.2.1</b> Continuous Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="visualization.html"><a href="visualization.html#categorical-variables"><i class="fa fa-check"></i><b>2.2.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="visualization.html"><a href="visualization.html#bivariate-data"><i class="fa fa-check"></i><b>2.3</b> Bivariate data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="visualization.html"><a href="visualization.html#scatter-plots"><i class="fa fa-check"></i><b>2.3.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="visualization.html"><a href="visualization.html#multivariate-data"><i class="fa fa-check"></i><b>2.4</b> Multivariate data</a><ul>
<li class="chapter" data-level="2.4.1" data-path="visualization.html"><a href="visualization.html#facetting"><i class="fa fa-check"></i><b>2.4.1</b> Facetting</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="visualization.html"><a href="visualization.html#data-quality"><i class="fa fa-check"></i><b>2.5</b> Data quality</a></li>
<li class="chapter" data-level="2.6" data-path="visualization.html"><a href="visualization.html#further-reading"><i class="fa fa-check"></i><b>2.6</b> Further reading</a></li>
<li class="chapter" data-level="2.7" data-path="visualization.html"><a href="visualization.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-frequentist-approach"><i class="fa fa-check"></i><b>3.1</b> The frequentist approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.1.1</b> The central limit theorem</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#one-sided-versus-two-sided-tests"><i class="fa fa-check"></i><b>3.1.3</b> One-sided versus two-sided tests</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>3.1.4</b> Type I and type II errors</a></li>
<li class="chapter" data-level="3.1.5" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>3.1.5</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="3.1.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>3.1.6</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#one--and-two-sample-t-tests"><i class="fa fa-check"></i><b>3.2</b> One- and two-sample t-tests</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inference.html"><a href="inference.html#one-sample-t-test"><i class="fa fa-check"></i><b>3.2.1</b> One-sample t-test</a></li>
<li class="chapter" data-level="3.2.2" data-path="inference.html"><a href="inference.html#two-sample-t-test"><i class="fa fa-check"></i><b>3.2.2</b> Two-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#tests-involving-more-than-two-means-anova-models"><i class="fa fa-check"></i><b>3.3</b> Tests involving more than two means: ANOVA models</a></li>
<li class="chapter" data-level="3.4" data-path="inference.html"><a href="inference.html#testing-for-association-in-contingency-tables"><i class="fa fa-check"></i><b>3.4</b> Testing for association in contingency tables</a></li>
<li class="chapter" data-level="3.5" data-path="inference.html"><a href="inference.html#nonparametric-tests"><i class="fa fa-check"></i><b>3.5</b> Nonparametric tests</a></li>
<li class="chapter" data-level="3.6" data-path="inference.html"><a href="inference.html#bootstrap"><i class="fa fa-check"></i><b>3.6</b> The nonparametric bootstrap</a><ul>
<li class="chapter" data-level="3.6.1" data-path="inference.html"><a href="inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>3.6.1</b> Bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="inference.html"><a href="inference.html#further-reading-1"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="inference.html"><a href="inference.html#exercises-2"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>4</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="unsupervised.html"><a href="unsupervised.html#prerequisites-2"><i class="fa fa-check"></i><b>4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.2" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>4.2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Finding principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised.html"><a href="unsupervised.html#performing-pca-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Performing PCA in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="unsupervised.html"><a href="unsupervised.html#selecting-the-number-of-principal-components"><i class="fa fa-check"></i><b>4.2.3</b> Selecting the Number of Principal Components</a></li>
<li class="chapter" data-level="4.2.4" data-path="unsupervised.html"><a href="unsupervised.html#extracting-additional-insights"><i class="fa fa-check"></i><b>4.2.4</b> Extracting additional insights</a></li>
<li class="chapter" data-level="4.2.5" data-path="unsupervised.html"><a href="unsupervised.html#pca-with-mixed-data"><i class="fa fa-check"></i><b>4.2.5</b> PCA with mixed data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised.html"><a href="unsupervised.html#cluster-analysis"><i class="fa fa-check"></i><b>4.3</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised.html"><a href="unsupervised.html#clustering-distance-measures"><i class="fa fa-check"></i><b>4.3.1</b> Clustering distance measures</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>4.3.2</b> K-means clustering</a></li>
<li class="chapter" data-level="4.3.3" data-path="unsupervised.html"><a href="unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.3.3</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="4.3.4" data-path="unsupervised.html"><a href="unsupervised.html#clustering-with-mixed-data"><i class="fa fa-check"></i><b>4.3.4</b> Clustering with mixed data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Predictive Analytics</b></span><ul>
<li class="chapter" data-level="4.4" data-path="unsupervised.html"><a href="unsupervised.html#regression-problems"><i class="fa fa-check"></i><b>4.4</b> Regression problems</a></li>
<li class="chapter" data-level="4.5" data-path="unsupervised.html"><a href="unsupervised.html#classification-problems"><i class="fa fa-check"></i><b>4.5</b> Classification problems</a></li>
<li class="chapter" data-level="4.6" data-path="unsupervised.html"><a href="unsupervised.html#algorithm-comparison-guide"><i class="fa fa-check"></i><b>4.6</b> Algorithm Comparison Guide</a></li>
<li class="chapter" data-level="4.7" data-path="unsupervised.html"><a href="unsupervised.html#general-modeling-process"><i class="fa fa-check"></i><b>4.7</b> General modeling process</a><ul>
<li class="chapter" data-level="4.7.1" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_prereq"><i class="fa fa-check"></i><b>4.7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.7.2" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-split"><i class="fa fa-check"></i><b>4.7.2</b> Data splitting</a></li>
<li class="chapter" data-level="4.7.3" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_feat"><i class="fa fa-check"></i><b>4.7.3</b> Feature engineering</a></li>
<li class="chapter" data-level="4.7.4" data-path="unsupervised.html"><a href="unsupervised.html#model-form"><i class="fa fa-check"></i><b>4.7.4</b> Basic model formulation</a></li>
<li class="chapter" data-level="4.7.5" data-path="unsupervised.html"><a href="unsupervised.html#tune"><i class="fa fa-check"></i><b>4.7.5</b> Model tuning</a></li>
<li class="chapter" data-level="4.7.6" data-path="unsupervised.html"><a href="unsupervised.html#cv"><i class="fa fa-check"></i><b>4.7.6</b> Cross Validation for Generalization</a></li>
<li class="chapter" data-level="4.7.7" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-eval"><i class="fa fa-check"></i><b>4.7.7</b> Model evaluation</a></li>
<li class="chapter" data-level="4.7.8" data-path="unsupervised.html"><a href="unsupervised.html#interpreting-predictive-models"><i class="fa fa-check"></i><b>4.7.8</b> Interpreting predictive models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#prerequisites-3"><i class="fa fa-check"></i><b>5.1</b> Prerequisites</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Simple linear regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimation"><i class="fa fa-check"></i><b>5.2.1</b> Estimation</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-regression.html"><a href="linear-regression.html#inference-1"><i class="fa fa-check"></i><b>5.2.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#multi-lm"><i class="fa fa-check"></i><b>5.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression.html"><a href="linear-regression.html#todo"><i class="fa fa-check"></i><b>5.3.1</b> TODO:</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.4</b> Assessing Model Accuracy</a></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#model-concerns"><i class="fa fa-check"></i><b>5.5</b> Model concerns</a></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#principal-component-regression"><i class="fa fa-check"></i><b>5.6</b> Principal component regression</a></li>
<li class="chapter" data-level="5.7" data-path="linear-regression.html"><a href="linear-regression.html#partial-least-squares"><i class="fa fa-check"></i><b>5.7</b> Partial least squares</a></li>
<li class="chapter" data-level="5.8" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-interp"><i class="fa fa-check"></i><b>5.8</b> Feature Interpretation</a></li>
<li class="chapter" data-level="5.9" data-path="linear-regression.html"><a href="linear-regression.html#final-thoughts"><i class="fa fa-check"></i><b>5.9</b> Final thoughts</a></li>
<li class="chapter" data-level="5.10" data-path="linear-regression.html"><a href="linear-regression.html#learning-more"><i class="fa fa-check"></i><b>5.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic regression</a><ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#prerequisites-4"><i class="fa fa-check"></i><b>6.1</b> Prerequisites</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Why logistic regression</a></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> Simple logistic regression</a></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>6.4</b> Multiple logistic regression</a></li>
<li class="chapter" data-level="6.5" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>6.5</b> Assessing model accuracy</a></li>
<li class="chapter" data-level="6.6" data-path="logistic-regression.html"><a href="logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>6.6</b> Feature interpretation</a></li>
<li class="chapter" data-level="6.7" data-path="logistic-regression.html"><a href="logistic-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>6.7</b> Final thoughts</a></li>
<li class="chapter" data-level="6.8" data-path="logistic-regression.html"><a href="logistic-regression.html#learning-more-1"><i class="fa fa-check"></i><b>6.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>7</b> Regularized regression</a><ul>
<li class="chapter" data-level="7.1" data-path="regularized-regression.html"><a href="regularized-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regularized-regression.html"><a href="regularized-regression.html#why"><i class="fa fa-check"></i><b>7.2</b> Why Regularize</a><ul>
<li class="chapter" data-level="7.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#ridge"><i class="fa fa-check"></i><b>7.2.1</b> Ridge penalty</a></li>
<li class="chapter" data-level="7.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#lasso"><i class="fa fa-check"></i><b>7.2.2</b> Lasso penalty</a></li>
<li class="chapter" data-level="7.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#elastic"><i class="fa fa-check"></i><b>7.2.3</b> Elastic nets</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regularized-regression.html"><a href="regularized-regression.html#implementation"><i class="fa fa-check"></i><b>7.3</b> Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-tune"><i class="fa fa-check"></i><b>7.4</b> Tuning</a></li>
<li class="chapter" data-level="7.5" data-path="regularized-regression.html"><a href="regularized-regression.html#lm-features"><i class="fa fa-check"></i><b>7.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="7.6" data-path="regularized-regression.html"><a href="regularized-regression.html#attrition-data"><i class="fa fa-check"></i><b>7.6</b> Attrition data</a></li>
<li class="chapter" data-level="7.7" data-path="regularized-regression.html"><a href="regularized-regression.html#final-thoughts-2"><i class="fa fa-check"></i><b>7.7</b> Final thoughts</a></li>
<li class="chapter" data-level="7.8" data-path="regularized-regression.html"><a href="regularized-regression.html#learning-more-2"><i class="fa fa-check"></i><b>7.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MARS.html"><a href="MARS.html"><i class="fa fa-check"></i><b>8</b> Multivariate Adaptive Regression Splines</a><ul>
<li class="chapter" data-level="8.1" data-path="MARS.html"><a href="MARS.html#prerequisites-6"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="MARS.html"><a href="MARS.html#the-basic-idea"><i class="fa fa-check"></i><b>8.2</b> The basic idea</a><ul>
<li class="chapter" data-level="8.2.1" data-path="MARS.html"><a href="MARS.html#multivariate-regression-splines"><i class="fa fa-check"></i><b>8.2.1</b> Multivariate regression splines</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="MARS.html"><a href="MARS.html#fitting-a-basic-mars-model"><i class="fa fa-check"></i><b>8.3</b> Fitting a basic MARS model</a></li>
<li class="chapter" data-level="8.4" data-path="MARS.html"><a href="MARS.html#tuning"><i class="fa fa-check"></i><b>8.4</b> Tuning</a></li>
<li class="chapter" data-level="8.5" data-path="MARS.html"><a href="MARS.html#feature-interpretation-1"><i class="fa fa-check"></i><b>8.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="MARS.html"><a href="MARS.html#attrition-data-1"><i class="fa fa-check"></i><b>8.6</b> Attrition data</a></li>
<li class="chapter" data-level="8.7" data-path="MARS.html"><a href="MARS.html#final-thoughts-3"><i class="fa fa-check"></i><b>8.7</b> Final thoughts</a></li>
<li class="chapter" data-level="8.8" data-path="MARS.html"><a href="MARS.html#learning-more-3"><i class="fa fa-check"></i><b>8.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="RF.html"><a href="RF.html"><i class="fa fa-check"></i><b>9</b> Random Forests</a><ul>
<li class="chapter" data-level="9.1" data-path="RF.html"><a href="RF.html#prerequisites-7"><i class="fa fa-check"></i><b>9.1</b> Prerequisites</a></li>
<li class="chapter" data-level="9.2" data-path="RF.html"><a href="RF.html#decision-trees"><i class="fa fa-check"></i><b>9.2</b> Decision trees</a><ul>
<li class="chapter" data-level="9.2.1" data-path="RF.html"><a href="RF.html#partitioning"><i class="fa fa-check"></i><b>9.2.1</b> Partitioning</a></li>
<li class="chapter" data-level="9.2.2" data-path="RF.html"><a href="RF.html#minimizing-overfitting"><i class="fa fa-check"></i><b>9.2.2</b> Minimizing overfitting</a></li>
<li class="chapter" data-level="9.2.3" data-path="RF.html"><a href="RF.html#a-simple-regression-tree-example"><i class="fa fa-check"></i><b>9.2.3</b> A simple regression tree example</a></li>
<li class="chapter" data-level="9.2.4" data-path="RF.html"><a href="RF.html#deciding-on-splits"><i class="fa fa-check"></i><b>9.2.4</b> Deciding on splits</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="RF.html"><a href="RF.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="RF.html"><a href="RF.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random forests</a><ul>
<li class="chapter" data-level="9.4.1" data-path="RF.html"><a href="RF.html#oob-error-vs.test-set-error"><i class="fa fa-check"></i><b>9.4.1</b> OOB error vs. test set error</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="RF.html"><a href="RF.html#fitting-a-basic-random-forest-model"><i class="fa fa-check"></i><b>9.5</b> Fitting a basic random forest model</a></li>
<li class="chapter" data-level="9.6" data-path="RF.html"><a href="RF.html#tuning-1"><i class="fa fa-check"></i><b>9.6</b> Tuning</a><ul>
<li class="chapter" data-level="9.6.1" data-path="RF.html"><a href="RF.html#tuning-via-ranger"><i class="fa fa-check"></i><b>9.6.1</b> Tuning via ranger</a></li>
<li class="chapter" data-level="9.6.2" data-path="RF.html"><a href="RF.html#tuning-via-caret"><i class="fa fa-check"></i><b>9.6.2</b> Tuning via caret</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="RF.html"><a href="RF.html#feature-interpretation-2"><i class="fa fa-check"></i><b>9.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="9.8" data-path="RF.html"><a href="RF.html#attrition-data-2"><i class="fa fa-check"></i><b>9.8</b> Attrition data</a></li>
<li class="chapter" data-level="9.9" data-path="RF.html"><a href="RF.html#final-thoughts-4"><i class="fa fa-check"></i><b>9.9</b> Final thoughts</a></li>
<li class="chapter" data-level="9.10" data-path="RF.html"><a href="RF.html#learning-more-4"><i class="fa fa-check"></i><b>9.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="GBM.html"><a href="GBM.html"><i class="fa fa-check"></i><b>10</b> Gradient Boosting Machines</a><ul>
<li class="chapter" data-level="10.1" data-path="GBM.html"><a href="GBM.html#prerequisites-8"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="GBM.html"><a href="GBM.html#the-basic-idea-1"><i class="fa fa-check"></i><b>10.2</b> The basic idea</a></li>
<li class="chapter" data-level="10.3" data-path="GBM.html"><a href="GBM.html#gbm-gradient"><i class="fa fa-check"></i><b>10.3</b> Gradient descent</a></li>
<li class="chapter" data-level="10.4" data-path="GBM.html"><a href="GBM.html#gbm-tuning"><i class="fa fa-check"></i><b>10.4</b> Tuning</a></li>
<li class="chapter" data-level="10.5" data-path="GBM.html"><a href="GBM.html#fitting-a-basic-gbm"><i class="fa fa-check"></i><b>10.5</b> Fitting a basic GBM</a></li>
<li class="chapter" data-level="10.6" data-path="GBM.html"><a href="GBM.html#tuning-2"><i class="fa fa-check"></i><b>10.6</b> Tuning</a></li>
<li class="chapter" data-level="10.7" data-path="GBM.html"><a href="GBM.html#feature-interpretation-3"><i class="fa fa-check"></i><b>10.7</b> Feature Interpretation</a></li>
<li class="chapter" data-level="10.8" data-path="GBM.html"><a href="GBM.html#attrition-data-3"><i class="fa fa-check"></i><b>10.8</b> Attrition data</a></li>
<li class="chapter" data-level="10.9" data-path="GBM.html"><a href="GBM.html#final-thoughts-5"><i class="fa fa-check"></i><b>10.9</b> Final thoughts</a></li>
<li class="chapter" data-level="10.10" data-path="GBM.html"><a href="GBM.html#learning-more-5"><i class="fa fa-check"></i><b>10.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="DL.html"><a href="DL.html"><i class="fa fa-check"></i><b>11</b> Deep Learning</a><ul>
<li class="chapter" data-level="11.1" data-path="DL.html"><a href="DL.html#dnn_why"><i class="fa fa-check"></i><b>11.1</b> Why deep learning</a></li>
<li class="chapter" data-level="11.2" data-path="DL.html"><a href="DL.html#dnn_ff"><i class="fa fa-check"></i><b>11.2</b> Feedforward DNNs</a></li>
<li class="chapter" data-level="11.3" data-path="DL.html"><a href="DL.html#dnn_arch"><i class="fa fa-check"></i><b>11.3</b> Network architecture</a><ul>
<li class="chapter" data-level="11.3.1" data-path="DL.html"><a href="DL.html#layers-and-nodes"><i class="fa fa-check"></i><b>11.3.1</b> Layers and nodes</a></li>
<li class="chapter" data-level="11.3.2" data-path="DL.html"><a href="DL.html#activation"><i class="fa fa-check"></i><b>11.3.2</b> Activation</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="DL.html"><a href="DL.html#dnn_back"><i class="fa fa-check"></i><b>11.4</b> Backpropagation</a></li>
<li class="chapter" data-level="11.5" data-path="DL.html"><a href="DL.html#dnn_train"><i class="fa fa-check"></i><b>11.5</b> Model training</a></li>
<li class="chapter" data-level="11.6" data-path="DL.html"><a href="DL.html#dnn_tuning"><i class="fa fa-check"></i><b>11.6</b> Model tuning</a><ul>
<li class="chapter" data-level="11.6.1" data-path="DL.html"><a href="DL.html#adjust-model-capacity"><i class="fa fa-check"></i><b>11.6.1</b> Adjust model capacity</a></li>
<li class="chapter" data-level="11.6.2" data-path="DL.html"><a href="DL.html#add-dropout"><i class="fa fa-check"></i><b>11.6.2</b> Add dropout</a></li>
<li class="chapter" data-level="11.6.3" data-path="DL.html"><a href="DL.html#add-weight-regularization"><i class="fa fa-check"></i><b>11.6.3</b> Add weight regularization</a></li>
<li class="chapter" data-level="11.6.4" data-path="DL.html"><a href="DL.html#adjust-learning-rate"><i class="fa fa-check"></i><b>11.6.4</b> Adjust learning rate</a></li>
<li class="chapter" data-level="11.6.5" data-path="DL.html"><a href="DL.html#automate-the-tuning-process"><i class="fa fa-check"></i><b>11.6.5</b> Automate the tuning process</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="DL.html"><a href="DL.html#feature-interpretation-4"><i class="fa fa-check"></i><b>11.7</b> Feature Interpretation</a></li>
<li class="chapter" data-level="11.8" data-path="DL.html"><a href="DL.html#final-thoughts-6"><i class="fa fa-check"></i><b>11.8</b> Final thoughts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="12" data-path="appendix-data.html"><a href="appendix-data.html"><i class="fa fa-check"></i><b>12</b> (APPENDIX) Appendix {-}</a><ul>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#ames-iowa-housing-data-1"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#employee-attrition-data-1"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Linear Regression</h1>
<p>Linear regression, a staple of statistical modeling from the precomputer age of statistics, is one of the simplest algorithms for supervised learning. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later chapters, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches; as we will see in later chapters, many fancy statistical learning approaches can be seen as generalizations to or extensions of linear regression. Consequently, it is importance to have a good understanding of linear regression before studying more complex learning methods. This chapter introduces linear regression with an emphasis on predictiction, rather than inference. An excellent and comprehensive overview of linear regression is provided in <span class="citation">Kutner et al. (<a href="#ref-kutner-2005-applied">2005</a>)</span>. See <span class="citation">Faraway (<a href="#ref-faraway-2016-linear">2016</a>)</span> for a discussion of linear regression in R (the book’s website also provides Python scripts).</p>
<div id="prerequisites-3" class="section level2">
<h2><span class="header-section-number">5.1</span> Prerequisites</h2>
<p>In this chapter, we’ll make use of the following packages:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)    <span class="co"># for cross-validation and model training functions</span>
<span class="kw">library</span>(dplyr)    <span class="co"># for data manipulation</span>
<span class="kw">library</span>(ggplot2)  <span class="co"># for awesome graphics</span>
<span class="kw">library</span>(rsample)  <span class="co"># for data splitting</span>
<span class="kw">library</span>(vip)      <span class="co"># for variable importance plots</span>
<span class="kw">library</span>(modelr)</code></pre>
<p>To illustrate linear regression concepts, we’ll continue with the Ames housing data, where our intent is to predict <code>Sale_Price</code> (and <code>log(Sale_Price)</code>). As discussed in the <em>Data splitting</em> section <a href="unsupervised.html#reg-perf-split">4.7.2</a>, we’ll set aside part of our data for training and another part to assess generalizability error.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Construct train (70%) and test (30%) sets Ames housing data.</span>
ames &lt;-<span class="st"> </span>AmesHousing<span class="op">::</span><span class="kw">make_ames</span>()  <span class="co"># Ames housing data</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span>
ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames, <span class="dt">prop =</span> <span class="fl">0.7</span>, <span class="dt">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)
trn &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)  <span class="co"># training data</span>
tst  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)  <span class="co"># test data</span></code></pre>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">5.2</span> Simple linear regression</h2>
<p>In Section <a href="#correlation"><strong>??</strong></a> we discussed the use of Pearson’s correlation coefficient to quantify the strength of the linear association between two continuous variables. In this section, we seek to fully characterize that linear relationship. <em>Simple linear regression</em> (SLR) assumes that the statistical relationship between two continuous variables is (at least approximately) linear:</p>
<p><span class="math display" id="eq:lm">\[\begin{equation}
\tag{5.1}
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad \text{for } i = 1, 2, \dots, n,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> represents the <em>i</em>-th response value, <span class="math inline">\(X_i\)</span> represents the <em>i</em>-th feature value, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are fixed, but unknown constants (commonly referred to as coefficients or parameters) that represent the intercept and slope of the regression line, respectively, and <span class="math inline">\(\epsilon_i\)</span> represent noise or random error. In this Chapter, we’ll assume that the errors are normally distirbuted with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>, denoted <span class="math inline">\(\stackrel{iid}{\sim} \left(0, \sigma^2\right)\)</span>. Since the random errors are centered around zero (i.e., <span class="math inline">\(E\left(\epsilon\right) = 0\)</span>), linear regression is really a problem of estimating a <em>conditional mean</em>:</p>
<p><span class="math display">\[\begin{equation}
  E\left(Y_i | X_i\right) = \beta_0 + \beta_1 X_i.
\end{equation}\]</span></p>
<p>For brevity, we often drop the conditional pice and write <span class="math inline">\(E\left(Y | X\right) = E\left(Y\right)\)</span>. Consequently, the interpretation of the coefficients are in terms of the average, or mean reponse. For example, the intercept <span class="math inline">\(\beta_0\)</span> represents the average response value when <span class="math inline">\(X = 0\)</span> (it is often not meaningful or of interest and is is sometimes referred to as a <em>bias term</em>). The slope <span class="math inline">\(\beta_1\)</span> represents the increase in the average response per one-unit increase in <span class="math inline">\(X\)</span> (i.e., it is a <em>rate of change</em>).</p>
<div id="estimation" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Estimation</h3>
<p>Ideally, we want estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that give us the “best fitting” line. But what is meant by “best fitting”? The most common approach is to use the method of <em>least squares</em> (LS) estimation; this form of linear regression is often referred to ordinary least squares (OLS) regression. There are multiple ways to measure “best fitting”, but the LS criterion finds the “best fitting” line by minimizing the <em>residual sum of squares</em> (RSS):</p>
<p><span class="math display" id="eq:least-squares-simple">\[\begin{equation}
\tag{5.2}
  RSS\left(\beta_0, \beta_1\right) = \sum_{i=1}^n\left[Y_i - \left(\beta_0 + \beta_1 X_i\right)\right]^2 = \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1 X_i\right)^2.
\end{equation}\]</span></p>
<p>The LS estimated of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are denoted as <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>, respectively. Once obtained, we can generate predicted values, say at <span class="math inline">\(X = X_{new}\)</span>, using the estimaed regression equation:</p>
<p><span class="math display">\[\begin{equation}
  \widehat{Y}_{new} = \widehat{\beta}_0 + \widehat{\beta}_1 X_{new},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\widehat{Y}_{new} = \widehat{E\left(Y_{new} | X = X_{new}\right)}\)</span> is the estimated mean response at <span class="math inline">\(X = X_{new}\)</span>.</p>
<p>With the Ames housing data, suppose we wanted to model a linear relationship between the year the house was built (<code>Year_Built</code>) and sale price (<code>Sale_Price</code>). To perform an OLS regression model in R we can use the <code>lm()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r">model1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Gr_Liv_Area, <span class="dt">data =</span> trn)</code></pre>
<p>The fitted model (<code>model1</code>) is displayed in the left plot in Figure <a href="linear-regression.html#fig:07-visualize-model1">5.1</a> where the points represent the values of <code>Sale_Price</code> in the training data. In the right plot of Figure <a href="linear-regression.html#fig:07-visualize-model1">5.1</a>, the vertical lines represent the individual errors, called <em>residuals</em>, associated with each observation. The OLS criterion <a href="#eq:rss">(<strong>??</strong>)</a> identifies the “best fitting” line that minimizes the sum of squares of these residuals.</p>
<div class="figure" style="text-align: center"><span id="fig:07-visualize-model1"></span>
<img src="abar_files/figure-html/07-visualize-model1-1.png" alt="The least squares fit from regressing `Sale_Price` on `Gr_Liv_Area` for the the Ames housing data. _Left_: Fitted regresison line. _Right_: Fitted regression line with vertical grey bars representing the residuals." width="960" />
<p class="caption">
Figure 5.1: The least squares fit from regressing <code>Sale_Price</code> on <code>Gr_Liv_Area</code> for the the Ames housing data. <em>Left</em>: Fitted regresison line. <em>Right</em>: Fitted regression line with vertical grey bars representing the residuals.
</p>
</div>
<p>The <code>coef()</code> function extracts the estimated coefficientes from the model. We can also use <code>summary()</code> to get a more detailed report of the model results.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model1)
<span class="co">## </span>
<span class="co">## Call:</span>
<span class="co">## lm(formula = Sale_Price ~ Gr_Liv_Area, data = trn)</span>
<span class="co">## </span>
<span class="co">## Residuals:</span>
<span class="co">##     Min      1Q  Median      3Q     Max </span>
<span class="co">## -467327  -30799   -1432   22339  338467 </span>
<span class="co">## </span>
<span class="co">## Coefficients:</span>
<span class="co">##             Estimate Std. Error t value             Pr(&gt;|t|)    </span>
<span class="co">## (Intercept) 17797.07    3916.11   4.545           0.00000582 ***</span>
<span class="co">## Gr_Liv_Area   108.03       2.47  43.743 &lt; 0.0000000000000002 ***</span>
<span class="co">## ---</span>
<span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span>
<span class="co">## </span>
<span class="co">## Residual standard error: 56850 on 2052 degrees of freedom</span>
<span class="co">## Multiple R-squared:  0.4825, Adjusted R-squared:  0.4823 </span>
<span class="co">## F-statistic:  1913 on 1 and 2052 DF,  p-value: &lt; 0.00000000000000022</span></code></pre>
<p>The estimated coefficients from our model are <span class="math inline">\(\widehat{\beta}_0 =\)</span> 17797.07 and <span class="math inline">\(\widehat{\beta}_1 =\)</span> 108.03. To interpret, we estimate that the mean selling price increases by 108.03 for each additional one square foot of above ground living space. This simple description of the relationship between the sale price and square footage using a single number (i.e., the slope) is what makes linear regression such an intuitive and popular modeling tool.</p>
<p>One drawback of the LS procedure in linear regression is that it only provides estimates of the coefficents; it does not provide an estimate of the error variance <span class="math inline">\(\sigma^2\)</span>! Note that LS makes no assumptions about the random errors. These assumptions are important for inference and in estimating the error variance which we’re assuming is constant with a value of <span class="math inline">\(\sigma^2\)</span>. One way to estimate <span class="math inline">\(\sigma^2\)</span> (which is required for characterizing the variability of our fitted model), is to use the method of <em>maximum likelihood</em> (ML) estimation (see <span class="citation">(Kutner et al. <a href="#ref-kutner-2005-applied">2005</a> sec 1.7)</span> for details). The ML procedure requires that we assume a particular distribution for the random errors. Most often, we assume the errors to be normally distributed. In practice, under the usual assumptions stated above, an unbiased estimate of the error variance is given as the sum of the squared residuals divided by <span class="math inline">\(n - p\)</span> (where <span class="math inline">\(p\)</span> is the number of regression coefficients or parameters in the model):</p>
<p><span class="math display">\[\begin{equation}
  \widehat{\sigma}^2 = \frac{1}{n - p}\sum_{i = 1} ^ n r_i ^ 2,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(r_i = \left(Y_i - \widehat{Y}_i\right)\)</span> is referred to as the <em>i</em>-th residual (i.e., the difference between the <em>i</em>-th observed and predicted response value). The quantity <span class="math inline">\(\widehat{\sigma}^2\)</span> is also referred to as the <em>mean square error</em> (MSE) and is often used for comparing regression models (typically, the MSEs are computed on a separate validation set or using cross-validation). It’s square root, denoted RMSE (for root mean square error) is also useful as it contains the same units as the response variable. In R, the RMSE of a linear model can be extracted using the <code>sigma()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sigma</span>(model1)  <span class="co"># RMSE</span>
<span class="co">## [1] 56845.44</span>
<span class="kw">sigma</span>(model1)<span class="op">^</span><span class="dv">2</span>  <span class="co"># MSE</span>
<span class="co">## [1] 3231404480</span></code></pre>
<p>Note that the RMSE is also reported as the <code>Residual standard error</code> in the output from <code>summary()</code>.</p>
</div>
<div id="inference-1" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Inference</h3>
<p>How accurate are the LS of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? Point estimates by themselves are not very useful. It is often desirable to associate some measure of an estimates variability. The variability of an estimate is often measured by its <em>standard error</em> (SE)—the square root of its variance. If we assume that the errors in the linear regression model are <span class="math inline">\(\stackrel{iid}{\sim} \left(0, \sigma^2\right)\)</span>, then simple expressions for the SEs of the estimated coefficients exist and are displayed in the column labeled <code>Std. Error</code> in the output from <code>summary()</code>. From this, we can also derive simple <span class="math inline">\(t\)</span>-tests to understand if the individual coefficients are statistically significant from zero. The <em>t</em>-statistics for such a test are nothing more than the estimated coefficients divided by their corresponding estimated standard errors (i.e., in the output from <code>summary()</code>, <code>t value</code> = <code>Estimate</code> / <code>Std. Error</code>). The reported <em>t</em>-statistics measure the number of standard deviations each coefficient is away from 0. Thus, large <em>t</em>-statistics (greater than two in absolute value, say) roughly indicate statistical significance at the <span class="math inline">\(\alpha = 0.05\)</span> level. The <em>p</em>-values for these tests are also reported by <code>summary()</code> in the column labeled <code>Pr(&gt;|t|)</code>.</p>
<p>Under the same assumptions, we can also derive confidence intervals for the coefficients. The formula for the traditional <span class="math inline">\(100\left(1 - \alpha\right)\)</span>% confidence interval for <span class="math inline">\(\beta_j\)</span> is</p>
<p><span class="math display" id="eq:conf-int">\[\begin{equation}
  \widehat{\beta}_j \pm t_{1 - \alpha / 2, n - p} \widehat{SE}\left(\widehat{\beta}_j\right),
  \tag{5.3}
\end{equation}\]</span></p>
<p>In R, we can construct such (one-at-a-time) confidence intervals for each coefficient using <code>confint()</code>. For example, a 95% confidence intervals for the coefficients in our SLR example can be computed using</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(model1, <span class="dt">level =</span> <span class="fl">0.95</span>)
<span class="co">##                 2.5 %     97.5 %</span>
<span class="co">## (Intercept) 10117.107 25477.0385</span>
<span class="co">## Gr_Liv_Area   103.191   112.8779</span></code></pre>
<p>To interpret, we estimate with 90% confidence that the mean selling price increases between 103.19 and 112.88 for each additional one square foot of above ground living space. We can also conclude that the slope <span class="math inline">\(\beta_1\)</span> is statistically significant from zero (or any other pre-specified value not included in the interval) at the <span class="math inline">\(\alpha = 0.05\)</span> level. This is also supported by the output from <code>summary()</code>.</p>

<div class="note">
<p>Most statistical software, including R, will include estimated standard errors, <em>t</em>-statistics, etc. as part of its regression output. However, it is important to remember that such quantities depend on three major assumptions of the linear regresion model:</p>
<ol style="list-style-type: decimal">
<li>Independent observations</li>
<li>The random errors have mean zero, and constant variance</li>
<li>The random errors are normally distributed</li>
</ol>
If any or all of these assumptions are violated, then remdial measures nned to be taken. For instance, <em>weghted least squares</em> (and other procedures) can be used the constant variance assumption is violated. Transformations (of both the response and features) can also help to correct departures from these assumptions.
</div>

</div>
</div>
<div id="multi-lm" class="section level2">
<h2><span class="header-section-number">5.3</span> Multiple linear regression</h2>
<div id="todo" class="section level3">
<h3><span class="header-section-number">5.3.1</span> TODO:</h3>
<ol style="list-style-type: decimal">
<li>Qualitative variables/factors</li>
<li>Interactions</li>
<li>Residual analysis</li>
</ol>
<p>In practice, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (<code>Gr_Liv_Area</code>) and the year the house was built (<code>Year_Built</code>) are (linearly) related to sales price (<code>Sale_Price</code>). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as a <em>multiple linear regression</em> (MLR) model. With two predictors, the MLR model becomes:</p>
<p><span class="math display">\[\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are features of interest. In our Ames housing exmple, <span class="math inline">\(X_1\)</span> represents <code>Gr_Liv_Area</code> and <span class="math inline">\(X_2\)</span> represents <code>Year_Built</code>.</p>
<p>In R, multiple linear regression models can be fit by separating all the features of interest with a <code>+</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">(model2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built, <span class="dt">data =</span> trn))
<span class="co">## </span>
<span class="co">## Call:</span>
<span class="co">## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = trn)</span>
<span class="co">## </span>
<span class="co">## Coefficients:</span>
<span class="co">## (Intercept)  Gr_Liv_Area   Year_Built  </span>
<span class="co">## -2121292.13        91.73      1097.66</span></code></pre>
<p>Alternatively, we can use <code>update()</code> to update the model formula used in <code>model</code>. The new formula can use a <code>.</code> as short hand for keep everything on either the left or right hand side of the formula, and a <code>+</code> or <code>-</code> can be used to add or remove terms from the original model, respectively. In the case of adding <code>Year_Built</code> to to <code>model1</code>, we could’ve used:</p>
<pre class="sourceCode r"><code class="sourceCode r">(model2 &lt;-<span class="st"> </span><span class="kw">update</span>(model1, . <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> </span>Year_Built))
<span class="co">## </span>
<span class="co">## Call:</span>
<span class="co">## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = trn)</span>
<span class="co">## </span>
<span class="co">## Coefficients:</span>
<span class="co">## (Intercept)  Gr_Liv_Area   Year_Built  </span>
<span class="co">## -2121292.13        91.73      1097.66</span></code></pre>
<p>The LS estimates of the regression coefficients are <span class="math inline">\(\widehat{\beta}_1 =\)</span> 91.728 and <span class="math inline">\(\widehat{\beta}_2 =\)</span> 1097.664 (the estimated intercept is -2121292.13. In other words, every one square foot increase to above ground square footage is associated with an additional $91.73 in <strong>mean selling price</strong> when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of $1,097.66 in selling price when holding the above ground square footage constant.</p>
<p>A contour plot of the fitted regression surface is displayed in the left side of Figure <a href="linear-regression.html#fig:07-mlr-fit">5.2</a> below. Note how the fitted regression surface is flat (i.e., it does not twist or bend). This is true for all linear models that include only <em>main effects</em> (i.e., terms involving only a single predictor). One way to model curvature is to include <em>interaction effects</em>. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. In linear regression, interactions are captured via products of features (i.e., <span class="math inline">\(X1 \times X_2\)</span>). A model with two main effects can also include a two-way interaction. For example, to include an interaction between <span class="math inline">\(X_1 =\)</span> <code>Gr_Liv_Area</code> and <span class="math inline">\(X_2 =\)</span> <code>Year_Built</code>, we would include an additional product term:</p>
<p><span class="math display">\[\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon.
\end{equation}\]</span></p>
<p>Note that in R, we use the <code>:</code> operator to include an interaction (techincally, we could use <code>*</code> as well, but <code>x1 * x2</code> is shortcut for <code>x1 + x2 + x1:x2</code> so is slightly redundant):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(Sale_Price <span class="op">~</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built <span class="op">+</span><span class="st"> </span>Gr_Liv_Area <span class="op">:</span><span class="st"> </span>Year_Built, <span class="dt">data =</span> trn)
<span class="co">## </span>
<span class="co">## Call:</span>
<span class="co">## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, </span>
<span class="co">##     data = trn)</span>
<span class="co">## </span>
<span class="co">## Coefficients:</span>
<span class="co">##            (Intercept)             Gr_Liv_Area              Year_Built  </span>
<span class="co">##           -719379.7373               -803.2844                386.1098  </span>
<span class="co">## Gr_Liv_Area:Year_Built  </span>
<span class="co">##                 0.4537</span></code></pre>
<p>A contour plot of the fitted regression surface with interaction is displayed in the right side of Figure <a href="linear-regression.html#fig:07-mlr-fit">5.2</a>. Note the curvature in the contour lines.</p>
<div class="note">
<p>
Interaction effects are quite prevelant in predictive modeling. Since linear models are an example of parametric modeling, it is up to the analyst to decide if and when to include interaction effects. In later chapters, we’ll discuss algorithms that can automatically detect and incoproarte interaction effects (albeit in different ways). It is also important to understand a concept called the <em>hierachy principle</em>—which demands that all lower-order terms corresponding to an interaction be retained in the model—when concidering interaction effects in linear regression models.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:07-mlr-fit"></span>
<img src="abar_files/figure-html/07-mlr-fit-1.png" alt="In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane)." width="960" />
<p class="caption">
Figure 5.2: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The ‘best-fit’ plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane).
</p>
</div>
<p>In general, we can include as many predicotrs as we want, as long as we have more rows than parmaters! The general multiple linear regression model with <em>p</em> distinct predictors is</p>
<p><span class="math display">\[\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> for <span class="math inline">\(i = 1, 2, \dots, p\)</span> are the predictors of interest. Note some of these may represent interactions (e.g., <span class="math inline">\(X_3 = X_1 \times X_2\)</span>) between or transformations<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (e.g., <span class="math inline">\(X_4 = \sqrt{X_1}\)</span>) of the original features. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. The below creates a third model where we use all features in our data set to predict <code>Sale_Price</code>.</p>
<div class="tip">
<p>
When fitting an MLR model in R with may terms, we can make use of the <code>.</code> notation for conveniance. For example, the dot in <code>lm(Sale_Price ~ ., data = trn)</code> is shorthand for regressing <code>Sale_Price</code> onto all other columns <code>trn</code> whereas <code>lm(Sale_Price ~ .^2, data = trn)</code> will regress <code>Sale_Price</code> on all other columns in <code>trn</code> while also including all possible two-way interactions effects. The use of <code>.</code> within <code>lm()</code> is illustrated below for the Ames housing example where we now include all possible main effects.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">model3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Sale_Price <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> trn)  <span class="co"># include all possible main effects</span>
broom<span class="op">::</span><span class="kw">tidy</span>(model3)  <span class="co"># print estimated coefficients in a tidy data frame</span>
<span class="co">## # A tibble: 291 x 5</span>
<span class="co">##    term                                estimate std.error statistic p.value</span>
<span class="co">##    &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;</span>
<span class="co">##  1 (Intercept)                          -1.51e7 10738681.   -1.41    0.159 </span>
<span class="co">##  2 MS_SubClassOne_Story_1945_and_Old…    1.92e3     3619.    0.530   0.596 </span>
<span class="co">##  3 MS_SubClassOne_Story_with_Finishe…    1.19e4    11239.    1.06    0.290 </span>
<span class="co">##  4 MS_SubClassOne_and_Half_Story_Unf…    1.99e4    17296.    1.15    0.251 </span>
<span class="co">##  5 MS_SubClassOne_and_Half_Story_Fin…    6.09e3     6270.    0.972   0.331 </span>
<span class="co">##  6 MS_SubClassTwo_Story_1946_and_New…   -2.99e2     5439.   -0.0550  0.956 </span>
<span class="co">##  7 MS_SubClassTwo_Story_1945_and_Old…    7.89e3     6075.    1.30    0.194 </span>
<span class="co">##  8 MS_SubClassTwo_and_Half_Story_All…   -1.90e4    10395.   -1.82    0.0683</span>
<span class="co">##  9 MS_SubClassSplit_or_Multilevel       -1.10e4    11925.   -0.922   0.356 </span>
<span class="co">## 10 MS_SubClassSplit_Foyer               -8.82e3     7896.   -1.12    0.264 </span>
<span class="co">## # ... with 281 more rows</span></code></pre>
</div>
</div>
<div id="assessing-model-accuracy" class="section level2">
<h2><span class="header-section-number">5.4</span> Assessing Model Accuracy</h2>
<p>We’ve fit three main effects models to the Ames housing data: a single predicor, two two predictors, and all possible predictors. But the question remains, which model is “best”? To answer this question we have to define what we mean by “best”. In our case, we’ll use the RMSE metric and cross-validation (Section <a href="unsupervised.html#cv">4.7.6</a>) to determine the “best” model. We can use the <code>caret::train()</code> function to to train a linear model (i.e., <code>method = &quot;lm&quot;</code>) using cross-validation (or a variety of other validation methods). In practice, a number of factors should be considered in determining a “best” model (e.g., time constraints, model production cost, predictive accuracy, etc.). The benefit of <strong>caret</strong> is that it provides built-in cross-validation capabilities, whereas the <code>lm()</code> function does not<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. The following code chunk uses <code>caret::train()</code> to refit <code>model1</code> using 10-fold cross-validation:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use caret package to train model using 10-fold cross-validation</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)  <span class="co"># for reproducibility</span>
(cv_model1 &lt;-<span class="st"> </span><span class="kw">train</span>(
  <span class="dt">form =</span> Sale_Price <span class="op">~</span><span class="st"> </span>Gr_Liv_Area, 
  <span class="dt">data =</span> trn, 
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
))
<span class="co">## Linear Regression </span>
<span class="co">## </span>
<span class="co">## 2054 samples</span>
<span class="co">##    1 predictor</span>
<span class="co">## </span>
<span class="co">## No pre-processing</span>
<span class="co">## Resampling: Cross-Validated (10 fold) </span>
<span class="co">## Summary of sample sizes: 1849, 1849, 1849, 1848, 1849, 1849, ... </span>
<span class="co">## Resampling results:</span>
<span class="co">## </span>
<span class="co">##   RMSE      Rsquared  MAE     </span>
<span class="co">##   56872.53  0.488359  38678.32</span>
<span class="co">## </span>
<span class="co">## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</span></code></pre>
<p>The resulting cross-validated RMSE is $56,872.53 (this is the average RMSE across the 10 cross-validation folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $56,873 off from the actual sale price.</p>
<p>We can perform cross validation on the other two models in a similar fashion, which we do in the code chunk below.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model2 &lt;-<span class="st"> </span><span class="kw">train</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>Gr_Liv_Area <span class="op">+</span><span class="st"> </span>Year_Built, 
  <span class="dt">data =</span> trn, 
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
)

<span class="co">#</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model3 &lt;-<span class="st"> </span><span class="kw">train</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> trn, 
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
)

<span class="co"># Extract out of sample performance measures</span>
<span class="kw">summary</span>(<span class="kw">resamples</span>(<span class="kw">list</span>(
  <span class="dt">model1 =</span> cv_model1, 
  <span class="dt">model2 =</span> cv_model2, 
  <span class="dt">model3 =</span> cv_model3
)))
<span class="co">## </span>
<span class="co">## Call:</span>
<span class="co">## summary.resamples(object = resamples(list(model1 = cv_model1, model2</span>
<span class="co">##  = cv_model2, model3 = cv_model3)))</span>
<span class="co">## </span>
<span class="co">## Models: model1, model2, model3 </span>
<span class="co">## Number of resamples: 10 </span>
<span class="co">## </span>
<span class="co">## MAE </span>
<span class="co">##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s</span>
<span class="co">## model1 35890.22 36623.61 38444.68 38678.32 40566.07 41819.36    0</span>
<span class="co">## model2 29242.66 30263.41 31201.89 31400.67 31895.10 34505.50    0</span>
<span class="co">## model3 14944.20 15851.63 17626.49 17700.57 18883.13 21980.35    0</span>
<span class="co">## </span>
<span class="co">## RMSE </span>
<span class="co">##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s</span>
<span class="co">## model1 49534.21 53293.60 56878.36 56872.53 58485.32 66579.00    0</span>
<span class="co">## model2 39812.27 43131.99 46900.17 46886.43 49341.44 55657.04    0</span>
<span class="co">## model3 21304.89 24402.89 46474.70 41438.00 53957.79 63246.61    0</span>
<span class="co">## </span>
<span class="co">## Rsquared </span>
<span class="co">##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s</span>
<span class="co">## model1 0.3125667 0.3965549 0.5179845 0.4883590 0.5607495 0.6351056    0</span>
<span class="co">## model2 0.5059805 0.5995888 0.6663934 0.6491582 0.7090147 0.7415758    0</span>
<span class="co">## model3 0.5456242 0.6428724 0.7123521 0.7523838 0.9058276 0.9250331    0</span></code></pre>
<p>Extracting the results for each model, we see that by adding more information via more predictors, we are able to improve the out-of-sample cross validation performance metrics. Specifically, our average prediction RMSE reduces from $46,886.43 (the model with two predictors) down to $41,438.00 (for our full model). In this case, the model with all possible main effects performs the “best” (comapred with the other two).</p>
</div>
<div id="model-concerns" class="section level2">
<h2><span class="header-section-number">5.5</span> Model concerns</h2>
<p>As previously stated, linear regression has been a popular modeling tool due to the ease of interpreting the coefficients. However, linear regression makes several strong assumptions that are often violated as we include more predictors in our model. Violation of these assumptions can lead to flawed interpretation of the coefficients and prediction results.</p>
<p><strong>1. Linear relationship:</strong> Linear regression assumes a linear relationship between the predictor and the response variable. When a linear relationship does not hold then the coefficient estimate makes a flawed assumption that a constant relationship holds. However, as discussed in Chapter <a href="intro.html#descriptive">1.1</a>, non-linear relationships can be made linear (or near-linear) by applying power transformations to the response and/or predictor. For example, Figure <a href="linear-regression.html#fig:07-linear-relationship">5.3</a> illustrates the relationship between sale price and the year a home was built. The left plot illustrates the non-linear relationship that exists. However, we can achieve a near-linear relationship by log transforming sale price; although some non-linearity still exists for older homes.</p>
<pre class="sourceCode r"><code class="sourceCode r">p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(trn, <span class="kw">aes</span>(Year_Built, Sale_Price)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Sale price&quot;</span>, <span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Year built&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Non-transformed variables with a </span><span class="ch">\n</span><span class="st">non-linear relationship.&quot;</span>)

p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(trn, <span class="kw">aes</span>(Year_Built, Sale_Price)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="st">&quot;Sale price&quot;</span>, <span class="dt">labels =</span> scales<span class="op">::</span>dollar, <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">400000</span>, <span class="dt">by =</span> <span class="dv">100000</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Year built&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Transforming variables can provide a </span><span class="ch">\n</span><span class="st">near-linear relationship.&quot;</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:07-linear-relationship"></span>
<img src="abar_files/figure-html/07-linear-relationship-1.png" alt="Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s)." width="768" />
<p class="caption">
Figure 5.3: Linear regression assumes a linear relationship between the predictor(s) and the response variable; however, non-linear relationships can often be altered to be near-linear by appying a transformation to the variable(s).
</p>
</div>
<p><strong>2. Constant variance among residuals:</strong> Linear regression assumes the variance among error terms (<span class="math inline">\(\epsilon_1, \epsilon_2, ..., \epsilon_p\)</span>) are constant (also referred to as homoscedasticity). When residuals are not constant, the <em>p</em>-values and confidence intervals of the coefficients are invalid resulting in invalid prediction estimates and confidence intervals. Similar to the linear relationships assumption, non-constant variance can often be resolved with variable transformations or by including additional predictors. For example, Figure <a href="linear-regression.html#fig:07-homoskedasticity">5.4</a> shows residuals across predicted values for our linear regression models <code>model1</code> and <code>model3</code>. <code>model1</code> displays a classic violation of constant variance with cone-shaped residuals. However, <code>model3</code> appears to have near-constant variance.</p>
<div class="tip">
<p>
The <code>broom::augment</code> function is an easy way to add model results to each observation (i.e. predicted values, residuals).
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">
df1 &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">augment</span>(cv_model1<span class="op">$</span>finalModel, <span class="dt">data =</span> trn)

p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df1, <span class="kw">aes</span>(.fitted, .resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Predicted values&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Model 1&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Sale_Price ~ Gr_Liv_Area&quot;</span>)

df2 &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">augment</span>(cv_model3<span class="op">$</span>finalModel, <span class="dt">data =</span> trn)

p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df2, <span class="kw">aes</span>(.fitted, .resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>)  <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Predicted values&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Model 3&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Sale_Price ~ .&quot;</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:07-homoskedasticity"></span>
<img src="abar_files/figure-html/07-homoskedasticity-1.png" alt="Linear regression assumes constant variance among the residuals. `model1` (left) shows definitive signs of heteroskedasticity whereas `model3` (right) appears to have constant variance." width="768" />
<p class="caption">
Figure 5.4: Linear regression assumes constant variance among the residuals. <code>model1</code> (left) shows definitive signs of heteroskedasticity whereas <code>model3</code> (right) appears to have constant variance.
</p>
</div>
<p><strong>3. No autocorrelation:</strong> Linear regression assumes the error terms are also independent and uncorrelated. If in fact, there is correlation among the error terms, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be. For example, the left plot in Figure <a href="linear-regression.html#fig:07-autocorrelation">5.5</a> displays the residuals (y-axis) to the observation ID (x-axis) for <code>model1</code>. A clear pattern exists suggesting that information about <span class="math inline">\(\epsilon_1\)</span> provides information about <span class="math inline">\(\epsilon_2\)</span>.</p>
<p>This pattern is a result of the data being ordered by neighborhood, which we have not accounted for in this model. Consequently, the residuals for homes in the same neighborhood are correlated (homes within a neighborhood are typically the same size and can often contain similar features). Since the <code>Neighborhood</code> predictor is included in <code>model3</code> (right plot), our errors are no longer correlated.</p>
<pre class="sourceCode r"><code class="sourceCode r">
df1 &lt;-<span class="st"> </span><span class="kw">mutate</span>(df1, <span class="dt">id =</span> <span class="kw">row_number</span>())
df2 &lt;-<span class="st"> </span><span class="kw">mutate</span>(df2, <span class="dt">id =</span> <span class="kw">row_number</span>())

p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df1, <span class="kw">aes</span>(id, .resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Row ID&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Model 1&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Correlated residuals.&quot;</span>)

p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df2, <span class="kw">aes</span>(id, .resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Row ID&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Model 3&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Uncorrelated residuals.&quot;</span>)

gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:07-autocorrelation"></span>
<img src="abar_files/figure-html/07-autocorrelation-1.png" alt="Linear regression assumes uncorrelated errors. The residuals in `model1` (left) have a distinct pattern suggesting that information about $\epsilon_1$ provides information about $\epsilon_2$. Whereas residuals in `model3` have no signs of autocorrelation." width="768" />
<p class="caption">
Figure 5.5: Linear regression assumes uncorrelated errors. The residuals in <code>model1</code> (left) have a distinct pattern suggesting that information about <span class="math inline">\(\epsilon_1\)</span> provides information about <span class="math inline">\(\epsilon_2\)</span>. Whereas residuals in <code>model3</code> have no signs of autocorrelation.
</p>
</div>
<p><strong>4. More observations than predictors:</strong> Although not an issue with the Ames housing data, when the number of features exceed the number of observations (<span class="math inline">\(p &gt; n\)</span>), the OLS solution matrix is not invertible. This causes significant issues because it means the least-squares estimates are not unique. In fact, there are an infinite set of solutions available so we lose our ability to meaningfully interpret coefficients. Consequently, to resolve this issue an analyst can remove variables until <span class="math inline">\(p &lt; n\)</span> and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach <span class="citation">(Kuhn and Johnson <a href="#ref-apm">2013</a>, 26:43–47)</span>, it can be cumbersome and prone to errors. Alternatively, we will introduce regularized regression in Chapter <a href="#regularize"><strong>??</strong></a> which provides you an alternative linear regression technique when <span class="math inline">\(p &gt; n\)</span>.</p>
<p><strong>5. No or little multicollinearity:</strong> <em>Collinearity</em> refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This, obviously, leads to inaccurate interpretation of coefficients and identifying influential predictors.</p>
<p>For example, in our data, <code>Garage_Area</code> and <code>Garage_Cars</code> are two variables that have a correlation of 0.89 and both variables are strongly correlated to our response variable (<code>Sale_Price</code>). Looking at our full model where both of these variables are included, we see that <code>Garage_Area</code> is found to be statistically significant but <code>Garage_Cars</code> is not.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit with two strongly correlated variables</span>
<span class="kw">summary</span>(cv_model3) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tidy</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Garage_Area&quot;</span>, <span class="st">&quot;Garage_Cars&quot;</span>))
<span class="co">## # A tibble: 2 x 5</span>
<span class="co">##   term        estimate std.error statistic  p.value</span>
<span class="co">##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span>
<span class="co">## 1 Garage_Cars   2131.    1761.        1.21 0.226   </span>
<span class="co">## 2 Garage_Area     19.5      5.88      3.31 0.000939</span></code></pre>
<p>However, if we refit the full model without <code>Garage_Area</code>, the coefficient estimate for <code>Garage_Cars</code> increases three fold and becomes statistically significant.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># model without Garage_Area</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
mod_wo_Garage_Area &lt;-<span class="st"> </span><span class="kw">train</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> <span class="kw">select</span>(trn, <span class="op">-</span>Garage_Area), 
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
  )

<span class="kw">summary</span>(mod_wo_Garage_Area) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tidy</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;Garage_Cars&quot;</span>)
<span class="co">## # A tibble: 1 x 5</span>
<span class="co">##   term        estimate std.error statistic     p.value</span>
<span class="co">##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;</span>
<span class="co">## 1 Garage_Cars    6342.     1222.      5.19 0.000000236</span></code></pre>
<p>This reflects the instability in the linear regression model caused by the between-predictor relationships and this instability gets propagated directly to the model predictions. Considering 16 of our 34 numeric predictors have medium to strong correlation (Section <a href="unsupervised.html#pca">4.2</a>), the biased coefficients of these predictors are likely restricting the predictive accuracy of our model. How can we control for this problem? One option is to manually remove one of the offending predictors. However, when the number of predictors is large such as in our case, this becomes difficult. Moreover, relationships between predictors can become complex and involve many predictors. In these cases, manual removal of specific predictors may not be possible. Consequently, the following sections offers two simple extensions of linear regression where dimension reduction is applied prior to performing linear regression. Chapter <a href="#regularize"><strong>??</strong></a> offers a modified regression approach that helps to deal with the problem. And future chapters provide alternative methods that are not effected by multicollinearity.</p>
</div>
<div id="principal-component-regression" class="section level2">
<h2><span class="header-section-number">5.6</span> Principal component regression</h2>
<p>As discussed in Chapter <a href="unsupervised.html#unsupervised">4</a>, principal components analysis can be used to represent correlated variables in a lower dimension and the resulting components can be used as predictors in the linear regression model. This two-step process is known as <strong>principal component regression</strong> (PCR) <span class="citation">(Massy <a href="#ref-massy1965principal">1965</a>)</span>.</p>
<p>Performing PCR with <strong>caret</strong> is an easy extension from our previous model. We simply change the <code>method</code> to “pcr” within <code>train</code> to perform PCA on all our numeric predictors prior to applying the multiple regression. Often, we can greatly improve performance by only using a small subset of all principal components as predictors. Consequently, you can think of the number of principal components as a tuning parameter (see Section <a href="unsupervised.html#tune">4.7.5</a>). The following performs cross validated PCR with <span class="math inline">\(1, 2, \dots, 20\)</span> principal components, and Figure <a href="linear-regression.html#fig:pcr-regression">5.6</a> illustrates the cross-validated RMSE. You can see a significant drop in prediction error using just five principal components followed by a gradual decrease. Using 17 principal components provided the lowest RMSE of $35,769.99 (see <code>cv_model_pcr</code> for a comparison of the cross-validated results).</p>
<div class="warning">
<p>
Per Section <span class="citation">(<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>)</span>(pca), don’t forget to center and scale your predictors, which you can do by incorporating the <code>preProcess</code> argument.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># perform 10-fold cross validation on a PCR model tuning the number of</span>
<span class="co"># principal components to use as predictors from 1-20</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model_pcr &lt;-<span class="st"> </span><span class="kw">train</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> trn, 
  <span class="dt">method =</span> <span class="st">&quot;pcr&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),
  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;zv&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">tuneLength =</span> <span class="dv">20</span>
  )

<span class="co"># model with lowest RMSE</span>
cv_model_pcr<span class="op">$</span>bestTune
<span class="co">##    ncomp</span>
<span class="co">## 17    17</span>

<span class="co"># plot cross-validated RMSE</span>
<span class="kw">plot</span>(cv_model_pcr)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:pcr-regression"></span>
<img src="abar_files/figure-html/pcr-regression-1.png" alt="The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components." width="576" />
<p class="caption">
Figure 5.6: The 10-fold cross valdation RMSE obtained using PCR with 1-20 principal components.
</p>
</div>
<p>By controlling for multicollinearity with PCR, we saw significant improvement in our predictive accuary (reducing out-of-sample RMSE from 41438 down to 35770). However, since PCR is a two step process, the PCA step does not consider any aspects of the response when it selects the components. Consequently, the new predictors produced by the PCA step are not designed to maximize the relationship with the response. Instead, it simply seeks to reduce the variability present throughout the predictor space. If that variability happens to be related to the response variability, then PCR has a good chance to identify a predictive relationship, as in our case. If, however, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exists (i.e. we may actually experience a decrease in our predictive accuracy). Thus, an alternative approach to reduce the impact of multicollinearity is partial least squares.</p>
</div>
<div id="partial-least-squares" class="section level2">
<h2><span class="header-section-number">5.7</span> Partial least squares</h2>
<p><em>Partial least squares (PLS)</em> can be viewed as a supervised dimension reduction procedure <span class="citation">(Kuhn and Johnson <a href="#ref-apm">2013</a>)</span>. Similar to PCR this technique also constructs a set of linear combinations of the inputs for regression, but unlike PCR it uses the response variable to aid the construction of the principal components. Thus, we can think of PLS as a supervised dimension reduction procedure that finds new features that not only appromxate the old features well, but also that are related to the response.</p>
<p><strong>TODO: IMAGE of PCR vs PLS</strong></p>
<p>Referring back to Equation <a href="#eq:pca1">(<strong>??</strong>)</a>, PLS will compute the first principal (<span class="math inline">\(z_1\)</span>) by setting each <span class="math inline">\(\phi_{j1}\)</span> to the coefficient from a SLR model of <span class="math inline">\(y\)</span> onto that respective <span class="math inline">\(x_j\)</span>. One can show that this coefficient is proportional to the correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_j\)</span>. Hence, in computing <span class="math inline">\(z_1 = \sum^p_{j=1} \phi_{j1}x_j\)</span>, PLS places the highest weight on the variables that are most strongly related to the response.</p>
<p>To compute the second principal (<span class="math inline">\(z_2\)</span>), we first regress each variable on <span class="math inline">\(z_1\)</span>. The residuals from this regression captures the remaining signal that has not been explained by the first principal. We substitute these residual values for the predictor values in Equation <a href="#eq:pca2">(<strong>??</strong>)</a>. This process continues until all <span class="math inline">\(m\)</span> components have been computed and then we use OLS to regress the response on <span class="math inline">\(z_1, \dots, z_m\)</span>.</p>
<div class="note">
<p>
See <span class="citation"><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></span> and <span class="citation"><span class="citation">Geladi and Kowalski (<a href="#ref-geladi1986partial">1986</a>)</span></span> for a thorough discussion of PLS.
</p>
</div>
<p>Similar to PCR, we can easily fit a PLS model by changing the <code>method</code> argument in <code>caret::train</code>. As with PCR, the number of principal components to use is a tuning parameter that is determined by the model that maximize predictive accuracy (minimizes RMSE in this case). The following performs cross validated PLS with <span class="math inline">\(1, 2, \dots, 20\)</span> principal components, and Figure <a href="linear-regression.html#fig:pls-regression">5.7</a> illustrates the cross-validated RMSE. You can see a greater drop in prediction error than PCR. Using PLS with <span class="math inline">\(m = 10\)</span> principal components provided the lowest RMSE of $31,522.47.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># perform 10-fold cross validation on a PLS model tuning the number of</span>
<span class="co"># principal components to use as predictors from 1-20</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv_model_pls &lt;-<span class="st"> </span><span class="kw">train</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> trn, 
  <span class="dt">method =</span> <span class="st">&quot;pls&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),
  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;zv&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
  <span class="dt">tuneLength =</span> <span class="dv">20</span>
  )

<span class="co"># model with lowest RMSE</span>
cv_model_pls<span class="op">$</span>bestTune
<span class="co">##    ncomp</span>
<span class="co">## 10    10</span>

<span class="co"># plot cross-validated RMSE</span>
<span class="kw">plot</span>(cv_model_pls)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:pls-regression"></span>
<img src="abar_files/figure-html/pls-regression-1.png" alt="The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components." width="576" />
<p class="caption">
Figure 5.7: The 10-fold cross valdation RMSE obtained using PLS with 1-20 principal components.
</p>
</div>
</div>
<div id="lm-model-interp" class="section level2">
<h2><span class="header-section-number">5.8</span> Feature Interpretation</h2>
<p>Once we’ve found the model that minimizes the predictive accuracy, our next goal is to interpret the model structure. Linear regression models provide a very intuitive model structure as they assume a <strong><em>monotonic linear relationship</em></strong> between the predictor variables and the response. The <em>linear</em> relationship part of that statement just means, for a given predictor variable, it assumes for every one unit change in a given predictor variable there is a constant change in the response. As discussed earlier in the chapter, this constant change is provided by the given coefficient for a predictor. The <em>monotonic</em> relationship means that a given predictor variable will always have a positive or negative relationship. But how do we determine the most influential variables?</p>
<p>Variable importance seeks to identify those variables that are most influential in our model. For linear regression models, this is most often measured by the absolute value of the <em>t</em>-statistic (Equation <a href="#eq:tstat">(<strong>??</strong>)</a>) for each model parameter used. For a PLS model, variable importance is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the RSS across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the RSS.</p>
<p>We can use <code>vip::vip</code> to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important). Figure <a href="linear-regression.html#fig:pls-vip">5.8</a> illustrates that the top 4 most important variables are <code>Gr_liv_Area</code>, <code>First_Flr_SF</code>, <code>Garage_Area</code>, and <code>Garage_Cars</code> respectively.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vip</span>(cv_model_pls, <span class="dt">num_features =</span> <span class="dv">20</span>, <span class="dt">method =</span> <span class="st">&quot;model&quot;</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:pls-vip"></span>
<img src="abar_files/figure-html/pls-vip-1.png" alt="Top 20 most important variables for the PLS model." width="672" />
<p class="caption">
Figure 5.8: Top 20 most important variables for the PLS model.
</p>
</div>
<p>As stated earlier, linear regression models assume a monotonic linear relationship. To illustrate this, we can apply partial dependence plots (PDPs). PDPs plot the change in the average predicted value (<span class="math inline">\(\hat y\)</span>) as specified feature(s) vary over their marginal distribution. As you will see in later chapters, PDPs become more useful when non-linear relationships are present. However, PDPs of linear models help illustrate how a fixed change in <span class="math inline">\(x_i\)</span> relates to a fixed linear change in <span class="math inline">\(\hat y_i\)</span>.</p>
<div class="tip">
<p>
The <strong>pdp</strong> package <span class="citation"><span class="citation">(Greenwell <a href="#ref-pkg-pdp">2017</a>)</span></span> provides convenient functions for computing and plotting PDPs. For example, the following code chunk would plot the PDP for the <code>Gr_Liv_Area</code> predictor.
</p>
<p>
<code>pdp::partial(cv_model_pls, pred.var = “Gr_Liv_Area”, grid.resolution = 20) %&gt;% autoplot()</code>
</p>
</div>
<p>All four of the most important predictors have a positive relationship with sale price; however, we see that the slope (<span class="math inline">\(\beta_i\)</span>) is steepest for the most important predictor and gradually decreases for lessor important variables.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="abar_files/figure-html/unnamed-chunk-9-1.png" alt="Partial dependence plots for the first four most important variables." width="672" />
<p class="caption">
Figure 5.9: Partial dependence plots for the first four most important variables.
</p>
</div>
</div>
<div id="final-thoughts" class="section level2">
<h2><span class="header-section-number">5.9</span> Final thoughts</h2>
<p>Linear regression is a great starting point in learning more advanced predictive analytic approaches because, in its simplest form, it is very intuitive and easy to interpret. Training a linear regression model is very easy and computationally efficient. However, due to the many assumptions required, the disadvantages of linear regression often outweigh their benefits. In our example, we saw how multicollinearity was interferring with predictive accuracy. By controlling multicollinearity with PCR and PLS we were able to improve predictive accuracy. Later chapters will build on the concepts illustrated in this chapter and will compare cross-validated performance results to identify the best predictive model. The following summarizes some of the advantages and disadvantages discussed regarding linear regression.</p>
<p><strong>FIXME: refine this section</strong></p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Normal linear regression has no hyperparameters to tune and PCR and PLS have only one hyperparameter to tune; making these methods very simple to train.</li>
<li>Computationally efficient - relatively fast compared to other algorithms in this book and does not require large memory.</li>
<li>Easy to interpret results.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Makes strong asssumptions about the data.</li>
<li>Does not handle missing data - must impute or remove observations with missing values.</li>
<li>Not robust to outliers as they can still bias the coefficients.</li>
<li>Assumes relationships between predictors and response variable to be monotonic linear (always increasing or decreasing in a linear fashion).</li>
<li>Typically does not perform as well as more advanced methods that allow non-monotonic and non-linear relationships (i.e. random forests, gradient boosting machines, neural networks).</li>
<li>Most large data sets violate one of the several assumptions made for linear regression to hold, which cause instability in the modeling results.</li>
</ul>
</div>
<div id="learning-more" class="section level2">
<h2><span class="header-section-number">5.10</span> Learning more</h2>
<p>This will get you up and running with linear regression. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:</p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
<li><a href="https://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kutner-2005-applied">
<p>Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li. 2005. <em>Applied Linear Statistical Models</em>. 5th ed. McGraw Hill.</p>
</div>
<div id="ref-faraway-2016-linear">
<p>Faraway, Julian J. 2016. <em>Linear Models with R</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-apm">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Vol. 26. Springer.</p>
</div>
<div id="ref-massy1965principal">
<p>Massy, William F. 1965. “Principal Components Regression in Exploratory Statistical Research.” <em>Journal of the American Statistical Association</em> 60 (309). Taylor &amp; Francis Group: 234–56.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA:</p>
</div>
<div id="ref-geladi1986partial">
<p>Geladi, Paul, and Bruce R Kowalski. 1986. “Partial Least-Squares Regression: A Tutorial.” <em>Analytica Chimica Acta</em> 185. Elsevier: 1–17.</p>
</div>
<div id="ref-pkg-pdp">
<p>Greenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” <em>The R Journal</em> 9 (1): 421–36. <a href="https://journal.r-project.org/archive/2017/RJ-2017-016/index.html">https://journal.r-project.org/archive/2017/RJ-2017-016/index.html</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Transofrmations of the features serve a number of purposes (e.g., modeling nonlinear relationships or alleviating departures from common regression assumptions). See <span class="citation">Kutner et al. (<a href="#ref-kutner-2005-applied">2005</a>)</span> for details.<a href="linear-regression.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Although general cross-validation is not avilable in <code>lm()</code> alone, a simple metric calles the <em>PRESS</em> statistic, for __PRE__dictive __S__um of __S__quare, (equivalent to a <em>leave-one-out</em> cross-validated RMSE) can be computed by summing the PRESS residuals which are available using <code>rstandard(&lt;lm-model-name&gt;, type = &quot;predictive&quot;)</code>. See <code>?rstandard</code> for details.<a href="linear-regression.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsupervised.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["abar.pdf", "abar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
