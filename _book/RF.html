<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</title>
  <meta name="description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  
  <meta name="twitter:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  

<meta name="author" content="Bradley C. Boehmke and Brandon M. Greenwell">


<meta name="date" content="2018-09-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="MARS.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="extra.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Business Analytics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-not-for"><i class="fa fa-check"></i>Who this book is not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-really-not-for"><i class="fa fa-check"></i>Who this book is really not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i>Why R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-this-book-is-organized"><i class="fa fa-check"></i>How this book is organized</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#using-code-examples"><i class="fa fa-check"></i>Using code examples</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-information"><i class="fa fa-check"></i>Software information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>Who are these Guys?</a><ul>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#bradley-c.-boehmke"><i class="fa fa-check"></i>Bradley C. Boehmke</a></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#brandon-m.-greenwell"><i class="fa fa-check"></i>Brandon M. Greenwell</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>1.1</b> Descriptive</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#predictive"><i class="fa fa-check"></i><b>1.2</b> Predictive</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#prescriptive"><i class="fa fa-check"></i><b>1.3</b> Prescriptive</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.4</b> Data sets</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#ames-iowa-housing-data"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#employee-attrition-data"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Descriptive Analytics</b></span><ul>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#prerequisites"><i class="fa fa-check"></i><b>1.5</b> Prerequisites</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#measures-of-location"><i class="fa fa-check"></i><b>1.6</b> Measures of location</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#the-sample-mean"><i class="fa fa-check"></i><b>1.6.1</b> The sample mean</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#the-sample-median"><i class="fa fa-check"></i><b>1.6.2</b> The sample median</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#the-mean-or-the-median"><i class="fa fa-check"></i><b>1.6.3</b> The mean or the median</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#measures-of-spread"><i class="fa fa-check"></i><b>1.7</b> Measures of spread</a><ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a href="intro.html#empirical-rule"><i class="fa fa-check"></i><b>1.7.1</b> The empirical rule</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#percentiles"><i class="fa fa-check"></i><b>1.8</b> Percentiles</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#robust-measures-of-spread"><i class="fa fa-check"></i><b>1.9</b> Robust measures of spread</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#outliers"><i class="fa fa-check"></i><b>1.10</b> Outlier detection</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#categorical"><i class="fa fa-check"></i><b>1.11</b> Describing categorical data</a><ul>
<li class="chapter" data-level="1.11.1" data-path="intro.html"><a href="intro.html#contingency-tables"><i class="fa fa-check"></i><b>1.11.1</b> Contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>2</b> Visual data exploration</a><ul>
<li class="chapter" data-level="2.1" data-path="visualization.html"><a href="visualization.html#prerequisites-1"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="visualization.html"><a href="visualization.html#univariate-data"><i class="fa fa-check"></i><b>2.2</b> Univariate data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="visualization.html"><a href="visualization.html#continuous-variables"><i class="fa fa-check"></i><b>2.2.1</b> Continuous Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="visualization.html"><a href="visualization.html#categorical-variables"><i class="fa fa-check"></i><b>2.2.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="visualization.html"><a href="visualization.html#bivariate-data"><i class="fa fa-check"></i><b>2.3</b> Bivariate data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="visualization.html"><a href="visualization.html#scatter-plots"><i class="fa fa-check"></i><b>2.3.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="visualization.html"><a href="visualization.html#multivariate-data"><i class="fa fa-check"></i><b>2.4</b> Multivariate data</a><ul>
<li class="chapter" data-level="2.4.1" data-path="visualization.html"><a href="visualization.html#facetting"><i class="fa fa-check"></i><b>2.4.1</b> Facetting</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="visualization.html"><a href="visualization.html#data-quality"><i class="fa fa-check"></i><b>2.5</b> Data quality</a></li>
<li class="chapter" data-level="2.6" data-path="visualization.html"><a href="visualization.html#further-reading"><i class="fa fa-check"></i><b>2.6</b> Further reading</a></li>
<li class="chapter" data-level="2.7" data-path="visualization.html"><a href="visualization.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-frequentist-approach"><i class="fa fa-check"></i><b>3.1</b> The frequentist approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>3.1.1</b> The central limit theorem</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#one-sided-versus-two-sided-tests"><i class="fa fa-check"></i><b>3.1.3</b> One-sided versus two-sided tests</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>3.1.4</b> Type I and type II errors</a></li>
<li class="chapter" data-level="3.1.5" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>3.1.5</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="3.1.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>3.1.6</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#one--and-two-sample-t-tests"><i class="fa fa-check"></i><b>3.2</b> One- and two-sample t-tests</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inference.html"><a href="inference.html#one-sample-t-test"><i class="fa fa-check"></i><b>3.2.1</b> One-sample t-test</a></li>
<li class="chapter" data-level="3.2.2" data-path="inference.html"><a href="inference.html#two-sample-t-test"><i class="fa fa-check"></i><b>3.2.2</b> Two-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#tests-involving-more-than-two-means-anova-models"><i class="fa fa-check"></i><b>3.3</b> Tests involving more than two means: ANOVA models</a></li>
<li class="chapter" data-level="3.4" data-path="inference.html"><a href="inference.html#testing-for-association-in-contingency-tables"><i class="fa fa-check"></i><b>3.4</b> Testing for association in contingency tables</a></li>
<li class="chapter" data-level="3.5" data-path="inference.html"><a href="inference.html#nonparametric-tests"><i class="fa fa-check"></i><b>3.5</b> Nonparametric tests</a></li>
<li class="chapter" data-level="3.6" data-path="inference.html"><a href="inference.html#bootstrap"><i class="fa fa-check"></i><b>3.6</b> The nonparametric bootstrap</a><ul>
<li class="chapter" data-level="3.6.1" data-path="inference.html"><a href="inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>3.6.1</b> Bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="inference.html"><a href="inference.html#further-reading-1"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="inference.html"><a href="inference.html#exercises-2"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>4</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="4.1" data-path="unsupervised.html"><a href="unsupervised.html#prerequisites-2"><i class="fa fa-check"></i><b>4.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.2" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>4.2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="4.2.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Finding principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="unsupervised.html"><a href="unsupervised.html#performing-pca-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Performing PCA in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="unsupervised.html"><a href="unsupervised.html#selecting-the-number-of-principal-components"><i class="fa fa-check"></i><b>4.2.3</b> Selecting the Number of Principal Components</a></li>
<li class="chapter" data-level="4.2.4" data-path="unsupervised.html"><a href="unsupervised.html#extracting-additional-insights"><i class="fa fa-check"></i><b>4.2.4</b> Extracting additional insights</a></li>
<li class="chapter" data-level="4.2.5" data-path="unsupervised.html"><a href="unsupervised.html#pca-with-mixed-data"><i class="fa fa-check"></i><b>4.2.5</b> PCA with mixed data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="unsupervised.html"><a href="unsupervised.html#cluster-analysis"><i class="fa fa-check"></i><b>4.3</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised.html"><a href="unsupervised.html#clustering-distance-measures"><i class="fa fa-check"></i><b>4.3.1</b> Clustering distance measures</a></li>
<li class="chapter" data-level="4.3.2" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>4.3.2</b> K-means clustering</a></li>
<li class="chapter" data-level="4.3.3" data-path="unsupervised.html"><a href="unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.3.3</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="4.3.4" data-path="unsupervised.html"><a href="unsupervised.html#clustering-with-mixed-data"><i class="fa fa-check"></i><b>4.3.4</b> Clustering with mixed data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Predictive Analytics</b></span><ul>
<li class="chapter" data-level="4.4" data-path="unsupervised.html"><a href="unsupervised.html#regression-problems"><i class="fa fa-check"></i><b>4.4</b> Regression problems</a></li>
<li class="chapter" data-level="4.5" data-path="unsupervised.html"><a href="unsupervised.html#classification-problems"><i class="fa fa-check"></i><b>4.5</b> Classification problems</a></li>
<li class="chapter" data-level="4.6" data-path="unsupervised.html"><a href="unsupervised.html#algorithm-comparison-guide"><i class="fa fa-check"></i><b>4.6</b> Algorithm Comparison Guide</a></li>
<li class="chapter" data-level="4.7" data-path="unsupervised.html"><a href="unsupervised.html#general-modeling-process"><i class="fa fa-check"></i><b>4.7</b> General modeling process</a><ul>
<li class="chapter" data-level="4.7.1" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_prereq"><i class="fa fa-check"></i><b>4.7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="4.7.2" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-split"><i class="fa fa-check"></i><b>4.7.2</b> Data splitting</a></li>
<li class="chapter" data-level="4.7.3" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_feat"><i class="fa fa-check"></i><b>4.7.3</b> Feature engineering</a></li>
<li class="chapter" data-level="4.7.4" data-path="unsupervised.html"><a href="unsupervised.html#model-form"><i class="fa fa-check"></i><b>4.7.4</b> Basic model formulation</a></li>
<li class="chapter" data-level="4.7.5" data-path="unsupervised.html"><a href="unsupervised.html#tune"><i class="fa fa-check"></i><b>4.7.5</b> Model tuning</a></li>
<li class="chapter" data-level="4.7.6" data-path="unsupervised.html"><a href="unsupervised.html#cv"><i class="fa fa-check"></i><b>4.7.6</b> Cross Validation for Generalization</a></li>
<li class="chapter" data-level="4.7.7" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-eval"><i class="fa fa-check"></i><b>4.7.7</b> Model evaluation</a></li>
<li class="chapter" data-level="4.7.8" data-path="unsupervised.html"><a href="unsupervised.html#interpreting-predictive-models"><i class="fa fa-check"></i><b>4.7.8</b> Interpreting predictive models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#prerequisites-3"><i class="fa fa-check"></i><b>5.1</b> Prerequisites</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#multi-lm"><i class="fa fa-check"></i><b>5.3</b> Multiple linear regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>5.4</b> Assessing Model Accuracy</a></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#model-concerns"><i class="fa fa-check"></i><b>5.5</b> Model concerns</a></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#principal-component-regression"><i class="fa fa-check"></i><b>5.6</b> Principal component regression</a></li>
<li class="chapter" data-level="5.7" data-path="linear-regression.html"><a href="linear-regression.html#partial-least-squares"><i class="fa fa-check"></i><b>5.7</b> Partial least squares</a></li>
<li class="chapter" data-level="5.8" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-interp"><i class="fa fa-check"></i><b>5.8</b> Feature Interpretation</a></li>
<li class="chapter" data-level="5.9" data-path="linear-regression.html"><a href="linear-regression.html#final-thoughts"><i class="fa fa-check"></i><b>5.9</b> Final thoughts</a></li>
<li class="chapter" data-level="5.10" data-path="linear-regression.html"><a href="linear-regression.html#learning-more"><i class="fa fa-check"></i><b>5.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic regression</a><ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#prerequisites-4"><i class="fa fa-check"></i><b>6.1</b> Prerequisites</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Why logistic regression</a></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>6.3</b> Simple logistic regression</a></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>6.4</b> Multiple logistic regression</a></li>
<li class="chapter" data-level="6.5" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>6.5</b> Assessing model accuracy</a></li>
<li class="chapter" data-level="6.6" data-path="logistic-regression.html"><a href="logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>6.6</b> Feature interpretation</a></li>
<li class="chapter" data-level="6.7" data-path="logistic-regression.html"><a href="logistic-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>6.7</b> Final thoughts</a></li>
<li class="chapter" data-level="6.8" data-path="logistic-regression.html"><a href="logistic-regression.html#learning-more-1"><i class="fa fa-check"></i><b>6.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>7</b> Regularized regression</a><ul>
<li class="chapter" data-level="7.1" data-path="regularized-regression.html"><a href="regularized-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="regularized-regression.html"><a href="regularized-regression.html#why"><i class="fa fa-check"></i><b>7.2</b> Why Regularize</a><ul>
<li class="chapter" data-level="7.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#ridge"><i class="fa fa-check"></i><b>7.2.1</b> Ridge penalty</a></li>
<li class="chapter" data-level="7.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#lasso"><i class="fa fa-check"></i><b>7.2.2</b> Lasso penalty</a></li>
<li class="chapter" data-level="7.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#elastic"><i class="fa fa-check"></i><b>7.2.3</b> Elastic nets</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regularized-regression.html"><a href="regularized-regression.html#implementation"><i class="fa fa-check"></i><b>7.3</b> Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-tune"><i class="fa fa-check"></i><b>7.4</b> Tuning</a></li>
<li class="chapter" data-level="7.5" data-path="regularized-regression.html"><a href="regularized-regression.html#lm-features"><i class="fa fa-check"></i><b>7.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="7.6" data-path="regularized-regression.html"><a href="regularized-regression.html#attrition-data"><i class="fa fa-check"></i><b>7.6</b> Attrition data</a></li>
<li class="chapter" data-level="7.7" data-path="regularized-regression.html"><a href="regularized-regression.html#final-thoughts-2"><i class="fa fa-check"></i><b>7.7</b> Final thoughts</a></li>
<li class="chapter" data-level="7.8" data-path="regularized-regression.html"><a href="regularized-regression.html#learning-more-2"><i class="fa fa-check"></i><b>7.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MARS.html"><a href="MARS.html"><i class="fa fa-check"></i><b>8</b> Multivariate Adaptive Regression Splines</a><ul>
<li class="chapter" data-level="8.1" data-path="MARS.html"><a href="MARS.html#prerequisites-6"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="MARS.html"><a href="MARS.html#the-basic-idea"><i class="fa fa-check"></i><b>8.2</b> The basic idea</a><ul>
<li class="chapter" data-level="8.2.1" data-path="MARS.html"><a href="MARS.html#multivariate-regression-splines"><i class="fa fa-check"></i><b>8.2.1</b> Multivariate regression splines</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="MARS.html"><a href="MARS.html#fitting-a-basic-mars-model"><i class="fa fa-check"></i><b>8.3</b> Fitting a basic MARS model</a></li>
<li class="chapter" data-level="8.4" data-path="MARS.html"><a href="MARS.html#tuning"><i class="fa fa-check"></i><b>8.4</b> Tuning</a></li>
<li class="chapter" data-level="8.5" data-path="MARS.html"><a href="MARS.html#feature-interpretation-1"><i class="fa fa-check"></i><b>8.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="MARS.html"><a href="MARS.html#attrition-data-1"><i class="fa fa-check"></i><b>8.6</b> Attrition data</a></li>
<li class="chapter" data-level="8.7" data-path="MARS.html"><a href="MARS.html#final-thoughts-3"><i class="fa fa-check"></i><b>8.7</b> Final thoughts</a></li>
<li class="chapter" data-level="8.8" data-path="MARS.html"><a href="MARS.html#learning-more-3"><i class="fa fa-check"></i><b>8.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="RF.html"><a href="RF.html"><i class="fa fa-check"></i><b>9</b> Random Forests</a><ul>
<li class="chapter" data-level="9.1" data-path="RF.html"><a href="RF.html#prerequisites-7"><i class="fa fa-check"></i><b>9.1</b> Prerequisites</a></li>
<li class="chapter" data-level="9.2" data-path="RF.html"><a href="RF.html#decision-trees"><i class="fa fa-check"></i><b>9.2</b> Decision trees</a><ul>
<li class="chapter" data-level="9.2.1" data-path="RF.html"><a href="RF.html#a-simple-regression-tree-example"><i class="fa fa-check"></i><b>9.2.1</b> A simple regression tree example</a></li>
<li class="chapter" data-level="9.2.2" data-path="RF.html"><a href="RF.html#deciding-on-splits"><i class="fa fa-check"></i><b>9.2.2</b> Deciding on splits</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="RF.html"><a href="RF.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="RF.html"><a href="RF.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random forests</a><ul>
<li class="chapter" data-level="9.4.1" data-path="RF.html"><a href="RF.html#oob-error-vs.test-set-error"><i class="fa fa-check"></i><b>9.4.1</b> OOB error vs. test set error</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="RF.html"><a href="RF.html#fitting-a-basic-random-forest-model"><i class="fa fa-check"></i><b>9.5</b> Fitting a basic random forest model</a></li>
<li class="chapter" data-level="9.6" data-path="RF.html"><a href="RF.html#tuning-1"><i class="fa fa-check"></i><b>9.6</b> Tuning</a><ul>
<li class="chapter" data-level="9.6.1" data-path="RF.html"><a href="RF.html#tuning-via-ranger"><i class="fa fa-check"></i><b>9.6.1</b> Tuning via ranger</a></li>
<li class="chapter" data-level="9.6.2" data-path="RF.html"><a href="RF.html#tuning-via-caret"><i class="fa fa-check"></i><b>9.6.2</b> Tuning via caret</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="RF.html"><a href="RF.html#feature-interpretation-2"><i class="fa fa-check"></i><b>9.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="9.8" data-path="RF.html"><a href="RF.html#attrition-data-2"><i class="fa fa-check"></i><b>9.8</b> Attrition data</a></li>
<li class="chapter" data-level="9.9" data-path="RF.html"><a href="RF.html#final-thoughts-4"><i class="fa fa-check"></i><b>9.9</b> Final thoughts</a></li>
<li class="chapter" data-level="9.10" data-path="RF.html"><a href="RF.html#learning-more-4"><i class="fa fa-check"></i><b>9.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="10" data-path="appendix-data.html"><a href="appendix-data.html"><i class="fa fa-check"></i><b>10</b> (APPENDIX) Appendix {-}</a><ul>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#ames-iowa-housing-data-1"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#employee-attrition-data-1"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="RF" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Random Forests</h1>
<p>The previous chapters covered models where the algorithm is based on a linear expansions in simple basis functions of the form <span class="math inline">\(\sum_{i=1}^p\beta_ih_i\left(\boldsymbol{x}\right)\)</span>, where the <span class="math inline">\(\beta_i\)</span> are unknown coefficients to be estimated and the <span class="math inline">\(h_i\left(\cdot\right)\)</span> are transformations applied to the features <span class="math inline">\(\boldsymbol{x}\)</span>. For ordinalry linear regression (with or without regularization), these transformations are supplied by the user; hence, these are parametric models. For example, the prediction equation <span class="math inline">\(f\left(\boldsymbol{x}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + \beta_4x_1^2\)</span> has</p>
<p><span class="math display">\[h_1\left(\boldsymbol{x}\right) = x_1 \quad \text{(main effect})\]</span></p>
<p><span class="math display">\[h_2\left(\boldsymbol{x}\right) = x_2 \quad \text{(main effect})\]</span></p>
<p><span class="math display">\[h_3\left(\boldsymbol{x}\right) = x_2x_4 \quad \text{(two-way interaction effect)}\]</span></p>
<p><span class="math display">\[h_2\left(\boldsymbol{x}\right) = x_2^2 \quad \text{(quadratic effect})\]</span></p>
<p>MARS, on the other hand, uses a specific algorithm to find the transformations to use automatically; hence, MARS is a nonparametric model.</p>
<p><em>Tree-based models</em>, are also nonparametric, but they work very differently. Tree-based models use algorithms to partition the feature space into a number of smaller (non-overlapping) regions based on a set of splitting rules and then fits a simpler model (e.g., a constant) in each region. Such <em>divide-and-conquor</em> methods (e.g., a single decision tree) can produce simple rules that are easy to interpret and visualize (e.g., with a <em>tree diagram</em>). Although fitted tree-based models can still be written as a linear expansions in simple basis functions (here the basis functions define the feature space partitioning), there is no benfit to doing so as other techniques, like <em>tree diagrams</em>, are better and conveying the information. Simple decision trees typically lack in predictive performance compared to more complex algorithms like neural networks and MARS. More sophisticated tree-based models, such as random forests and gradient boosting machines, are less interpretable, but tend to have very good predictive accuracy. This chapter will get you familiar with the basics behind decision trees, and two ways inwhich to combine them into a more accurate ensemble; namely, bagging and random forests. In the next chapter, we’ll cover (stochastic) gradient boosting machines, which is another powerful way of combining decision trees into a more accurate ensemble.</p>
<div id="prerequisites-7" class="section level2">
<h2><span class="header-section-number">9.1</span> Prerequisites</h2>
<p>For this chapter we’ll use the following packages:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(caret)         <span class="co"># for classification and regression training</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(randomForest)  <span class="co"># for Breiman and Cutler&#39;s random forest</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">library</span>(ranger)        <span class="co"># for fast implementation of random forests</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw">library</span>(rpart)         <span class="co"># for fitting CART-like decision trees</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">library</span>(rpart.plot)    <span class="co"># for flexible decision tree diagrams</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="kw">library</span>(rsample)       <span class="co"># for data splitting </span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">library</span>(pdp)           <span class="co"># for partial dependence plots</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="kw">library</span>(vip)           <span class="co"># for variable importance plots</span></a></code></pre></div>
<p>To illustrate the various concepts we’ll use the Ames Housing data (regression); however, at the end of the chapter we’ll also apply fit a random forest model to the employee attrition data (classification).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="co"># Use set.seed for reproducibility</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(AmesHousing<span class="op">::</span><span class="kw">make_ames</span>(), <span class="dt">prop =</span> <span class="fl">.7</span>)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">ames_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)</a></code></pre></div>
</div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">9.2</span> Decision trees</h2>
<p>There are many methodologies for constructing decision trees but the most well-known is the <strong>c</strong>lassification <strong>a</strong>nd <strong>r</strong>egression <strong>t</strong>ree (CART©) algorithm proposed in <span class="citation">Breiman (<a href="#ref-breiman2017classification">1984</a>)</span>. Basic decision trees partition the training data into homogenious subgroups and then fit a simple <em>constant</em> in each (e.g., the mean of the within group response values for regression). The partitioning is achieved by recursive binary partitions formed by asking yes-or-no questions about each feature (e.g., is <code>age &lt; 18</code>?). This is done a number of times until a suitable stopping critera is satiscfied (e.g., a maximum depth of the tree is reached). After all the partitioning has been done, the model predicts the output based on (1) the average response values for all observations that fall in that subgroup (regression problem), or (2) the class that has majority representation (classification problem). For classification, predicted probabilites can be obtained using the proportion of each class within the subgroups.</p>
<div class="tip">
<p>
The basic idea behind decision trees is to ask simple yes-or-no questions about each feature in order partion the training data into subgroups with similar response rates. Ideally, the repsonses within each subgroup will be as similar or homegenous as possible, while responses across subgroups will be as different or as heterogenous as possible.
</p>
</div>
<div id="a-simple-regression-tree-example" class="section level3">
<h3><span class="header-section-number">9.2.1</span> A simple regression tree example</h3>
<p>For example, suppose we want to use a decision tree to predict the miles per gallon a car will average (<code>mpg</code>) based on two features: the number of cylinders (<code>cyl</code>) and the horsepower (<code>hp</code>); such data are available in the <code>mtcars</code> data frame which is part of the standard <strong>datasets</strong> package. A (regression) tree is built using the <strong>rpart</strong> package <span class="citation">(Therneau and Atkinson <a href="#ref-R-rpart">2018</a>)</span> (see the code chunk below and Figure <a href="#rf-decision-tree-example-image"><strong>??</strong></a> which was produced using the <strong>rpart.plot</strong> package <span class="citation">(Milborrow <a href="#ref-R-rpart.plot">2018</a>)</span>), and all the training data are passed down this tree. Whenever an obervation reaches a partiular node in the tree (i.e., a yes-or-no question about one of the features; in this case, <code>cyl</code> or <code>hp</code>), it proceeds either to the left (if the answer is “yes”) or to the right (if the answer is “no”). This tree has only three terminal nodes. To start traversing the tree, all observations that have 6 or 8 cylinders go to the left branch, all other observations proceed to the right branch. Next, the left branch is further partitioned by <code>hp</code>. Of all the observations with 6 or 8 cylinders with <code>hp</code> equal to or greater than 192 proceed to the left branch; those with less than 192 <code>hp</code> proceed to the right. These branches lead to <em>terminal nodes</em> or <em>leafs</em> which contain the predicted response value; in this case, the average mpg of cars that fall within that terminal node. In short, cars with less than 5 cylinders (region <span class="math inline">\(R_1\)</span>) average 27 mpg, cars with <code>cyl</code> <span class="math inline">\(\ge 5\)</span> and <code>hp</code> <span class="math inline">\(&lt; 193\)</span> (region <span class="math inline">\(R_2\)</span>) average 18 mpg, and all other cars (region <span class="math inline">\(R_3\)</span>) in the training data average 13 mph.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(mpg <span class="op">~</span><span class="st"> </span>cyl <span class="op">+</span><span class="st"> </span>hp, <span class="dt">data =</span> mtcars)  <span class="co"># CART-like regression tree</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">rpart.plot</span>(tree)  <span class="co"># tree diagram</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-decision-tree-example"></span>
<img src="abar_files/figure-html/rf-decision-tree-example-1.png" alt="Using a CART-like decision tree to predict `mpg` based on `cyl` and `hp`." width="70%" />
<p class="caption">
Figure 9.1: Using a CART-like decision tree to predict <code>mpg</code> based on <code>cyl</code> and <code>hp</code>.
</p>
</div>
<p>This simple example can be generalized as follows. We have a given response variable <span class="math inline">\(Y\)</span> and two inputs <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The recursive partitioning results in three non-verlapping regions denoted <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span>, and <span class="math inline">\(R_3\)</span>. In region <span class="math inline">\(R_m\)</span> <span class="math inline">\(\left(m = 1, 2, 3\right)\)</span>, the model predicts <em>Y</em> with a constant <span class="math inline">\(c_m\)</span>:</p>
<p><strong>FIXME:</strong> This doesn’t seem right?</p>
<p><span class="math display">\[\begin{equation}
  \widehat{f}\left(X_1, X_2\right) = \sum^3_{m=1} c_m I{\left(X_1, X_2 \in R_m\right)}.
\end{equation}\]</span></p>
<p>However, an the question remains of how to grow a decision tree; that is, select the partioning to give regions <span class="math inline">\(R_j\)</span> for <span class="math inline">\(j = 1, 2, \dots\)</span>.</p>
</div>
<div id="deciding-on-splits" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Deciding on splits</h3>
<p><strong>FIXME:</strong> Continue with the previous example?</p>
<p>First, it’s important to realize that the partitioning of the feature space is done in a top-down, <em>greedy</em> fashion. This means that a any partion in the tree dpeends on the previous partitions. But how are these partions made? The algorithm begins with the entire training data set <em>S</em> and searches every distinct value of every input variable to find the “best” feature/split combination that partitions the data into two regions (<span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>) such that the overall error is minimized (typically, MSE for regression problems, and cross-entropy or the Gini index for classification problems–see Section <a href="unsupervised.html#reg-perf-eval">4.7.7</a>).</p>
<p>Having found the best feature/split combination, the data are partitioned into two regions and the splitting process is repeated on each of the two regions. This process is continued until some stopping criterion is reached (e.g., a mxaimu depth is reached or the tree becomes “too complex”). What results is, typically, a very deep, complex tree that may produce good predictions on the training set, but is likely to generalize well to new unseen data (i.e., overfitting), leading to poor generalization performance.</p>
<p>For example, using the well-known Boston housing data <span class="citation">(Harrison Jr and Rubinfeld <a href="#ref-harrison1978hedonic">1978</a>)</span>, three decision trees are created based on three different samples of the data. You can see that the first few partitions are fairly similar at the top of each tree; however, they tend to differ substantially closer to the terminal nodes. These deeper nodes tend to overfit to specific attributes of the training data; consequently, slightly different samples will result in highly variable predicted values in the terminal nodes. For this reason, CART-like decision tree algorithms are often considered high variance (i.e., noisy) models fitting procedures.</p>
<p><strong>FIXME:</strong> Brad, can we replace this with the code that produced the results; either hidden or shown?</p>
<div class="figure" style="text-align: center"><span id="fig:rf-tree-variance-image"></span>
<img src="illustrations/tree-variance-1.svg" alt="Three decision trees based on slightly different samples." width="70%" height="70%" />
<p class="caption">
Figure 9.2: Three decision trees based on slightly different samples.
</p>
</div>
</div>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">9.3</span> Bagging</h2>
<p>Although pruning the tree helps reduce the variance<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> of a single tree (i.e., by helping to avoid overfitting), there are alternative methods that exploit this variance in a way that can significantly improve performance over and above that of single trees. <em>Bootstrap aggregating</em> (bagging) <span class="citation">(Breiman <a href="#ref-breiman1996bagging">1996</a>)</span> is one such approach.</p>
<p>Bagging combines and averages multiple models.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance. Bagging follows three simple steps:</p>
<ol style="list-style-type: decimal">
<li>Create <em>m</em> bootstrap samples (Section <a href="inference.html#bootstrap">3.6</a>) from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.</li>
<li>For each bootstrap sample, train a single, unpruned decision tree.</li>
<li>Average individual predictions from each tree to create an overall average predicted value.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:rf-bagging-image"></span>
<img src="illustrations/bagging.png" alt="The bagging process." width="70%" height="70%" />
<p class="caption">
Figure 9.3: The bagging process.
</p>
</div>
<p>This process can actually be applied to any regression or classification model; however, it provides the greatest improvement for models that have high variance. For example, more stable parametric models such as linear regression and multi-adaptive regression splines tend to experience less improvement in predictive performance when bagging.</p>
<p>Although bagging trees can help reduce the variance of a single tree’s prediction and improve predictive performance, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships.</p>
<p>For example, if we create six decision trees with different bootstrapped samples of the Boston housing data, we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both <code>lstat</code> and <code>rm</code> variables driving the first few splits.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="illustrations/tree-correlation-1.png" alt="Six decision trees based on different bootstrap samples." width="70%" height="70%" />
<p class="caption">
Figure 9.4: Six decision trees based on different bootstrap samples.
</p>
</div>
<p>This characteristic is known as <em>tree correlation</em> and prevents bagging from optimally reducing variance of the predicted values. In order to reduce variance further, we need to minimize the amount of correlation between the trees.</p>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">9.4</span> Random forests</h2>
<p>Random forests are an extension of bagging and injects more randomness into the tree-growing process. Random forests achieve this in two ways:</p>
<ol style="list-style-type: decimal">
<li><strong>Bootstrap</strong>: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and <em>somewhat</em> decorrelates them.</li>
<li><strong>Split-variable randomization</strong>: each time a split is to be performed, the search for the split variable is limited to a random subset of <em>m</em> of the <em>p</em> variables. Typical default values for <span class="math inline">\(m\)</span> are <span class="math inline">\(m = \frac{p}{3}\)</span> (regression problems) and <span class="math inline">\(m = \sqrt{p}\)</span> for classification models. However, this should be considered a tuning parameter. When <span class="math inline">\(m = p\)</span>, the randomization amounts to using only step 1 and is the same as <em>bagging</em>.</li>
</ol>
<p>The basic algorithm for a random forest model can be generalized to the following:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="fl">1.</span>  Given training data set</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="fl">2.</span>  Select number of trees to <span class="kw">build</span> (ntrees)</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="fl">3.</span>  <span class="cf">for</span> i =<span class="st"> </span><span class="dv">1</span> to ntrees do</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="fl">4.</span>  <span class="op">|</span><span class="st">  </span>Generate a bootstrap sample of the original data</a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="fl">5.</span>  <span class="op">|</span><span class="st">  </span>Grow a regression or classification tree to the bootstrapped data</a>
<a class="sourceLine" id="cb4-6" data-line-number="6"><span class="fl">6.</span>  <span class="op">|</span><span class="st">  </span><span class="cf">for</span> each split do</a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="fl">7.</span>  <span class="op">|</span><span class="st">  </span><span class="er">|</span><span class="st"> </span>Select m variables at random from all p variables</a>
<a class="sourceLine" id="cb4-8" data-line-number="8"><span class="fl">8.</span>  <span class="op">|</span><span class="st">  </span><span class="er">|</span><span class="st"> </span>Pick the best variable<span class="op">/</span>split<span class="op">-</span>point among the m</a>
<a class="sourceLine" id="cb4-9" data-line-number="9"><span class="fl">9.</span>  <span class="op">|</span><span class="st">  </span><span class="er">|</span><span class="st"> </span>Split the node into two child nodes</a>
<a class="sourceLine" id="cb4-10" data-line-number="10"><span class="fl">10.</span> <span class="op">|</span><span class="st">  </span>end</a>
<a class="sourceLine" id="cb4-11" data-line-number="11"><span class="fl">11.</span> <span class="op">|</span><span class="st"> </span>Use typical tree model stopping criteria to determine when a tree is <span class="kw">complete</span> (but do not prune)</a>
<a class="sourceLine" id="cb4-12" data-line-number="12"><span class="fl">12.</span> end</a></code></pre></div>
<p>Since the algorithm randomly selects a bootstrap sample to train on <strong><em>and</em></strong> predictors to use at each split, tree correlation will be lessened beyond bagged trees.</p>
<div id="oob-error-vs.test-set-error" class="section level3">
<h3><span class="header-section-number">9.4.1</span> OOB error vs. test set error</h3>
<p>One benefit of bagging (and thus also random forests) is that, on average, a bootstrap sample will contain 63% of the training data. This leaves about 37% of the data out of the bootstrapped sample. We call this the out-of-bag (OOB) sample. We can use the OOB observations to estimate the model’s accuracy, creating a natural cross-validation process, which allows you to not need to sacrifice any of your training data to use for validation. This makes identifying the number of trees required to stablize the error rate during tuning more efficient; however, as illustrated below some difference between the OOB error and test error are expected.</p>
<div class="figure" style="text-align: center"><span id="fig:rf-oob-example-image"></span>
<img src="illustrations/oob-error-compare-1.svg" alt="Random forest out-of-bag error versus validation error." width="70%" height="70%" />
<p class="caption">
Figure 9.5: Random forest out-of-bag error versus validation error.
</p>
</div>
<p>Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, you’d want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation.</p>
</div>
</div>
<div id="fitting-a-basic-random-forest-model" class="section level2">
<h2><span class="header-section-number">9.5</span> Fitting a basic random forest model</h2>
<p>There are over 20 random forest packages in R.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> To demonstrate the basic implementation we illustrate the use of the <strong>randomForest</strong> package <span class="citation">(Breiman et al. <a href="#ref-R-randomForest">2018</a>)</span>, the oldest and most well known implementation of the Random Forest algorithm in R.</p>
<div class="tip">
<p>
However, as your data set grows in size <strong>randomForest</strong> does not scale well (although you can parallelize with <strong>foreach</strong>).
</p>
</div>
<p><code>randomForest()</code> can use the formula or separate x, y matrix notation for specifying our model. Below we apply the default <strong>randomForest</strong> model using the formulaic specification. The default random forest performs 500 trees and <span class="math inline">\(\frac{features}{3} = 26\)</span> randomly selected predictor variables at each split. Averaging across all 500 trees provides an OOB <span class="math inline">\(MSE = 661089658\)</span> (<span class="math inline">\(RMSE = \$25,711\)</span>).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># for reproduciblity</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb5-3" data-line-number="3"></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="co"># default RF model</span></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">rf1 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">  <span class="dt">data =</span> ames_train</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb5-9" data-line-number="9"></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">rf1</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">## </a>
<a class="sourceLine" id="cb5-12" data-line-number="12">## Call:</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">##  randomForest(formula = Sale_Price ~ ., data = ames_train) </a>
<a class="sourceLine" id="cb5-14" data-line-number="14">##                Type of random forest: regression</a>
<a class="sourceLine" id="cb5-15" data-line-number="15">##                      Number of trees: 500</a>
<a class="sourceLine" id="cb5-16" data-line-number="16">## No. of variables tried at each split: 26</a>
<a class="sourceLine" id="cb5-17" data-line-number="17">## </a>
<a class="sourceLine" id="cb5-18" data-line-number="18">##           Mean of squared residuals: 661089658</a>
<a class="sourceLine" id="cb5-19" data-line-number="19">##                     % Var explained: 89.8</a></code></pre></div>
<p>Plotting the model will illustrate the OOB error rate as we average across more trees and shows that our error rate stabalizes with around 100 trees but continues to decrease slowly until around 300 or so trees.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">plot</span>(rf1)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-basic-model-plot"></span>
<img src="abar_files/figure-html/rf-basic-model-plot-1.png" alt="OOB error (MSE) as a function of the number of trees.  We see the MSE reduces quickly for the first 100 trees and then slowly thereafter.  We want to make sure that we are providing enough trees so that our OOB error has stabalized or flatlined." width="480" />
<p class="caption">
Figure 9.6: OOB error (MSE) as a function of the number of trees. We see the MSE reduces quickly for the first 100 trees and then slowly thereafter. We want to make sure that we are providing enough trees so that our OOB error has stabalized or flatlined.
</p>
</div>
<p>The plotted error rate above is based on the OOB sample error and can be accessed directly at <code>rf1$mse</code>. Thus, we can find which number of trees provides the lowest error rate, which is 447 trees providing an average home sales price error of $25,649.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># number of trees with lowest MSE</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">which.min</span>(rf1<span class="op">$</span>mse)</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">## [1] 447</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="co"># RMSE of this optimal random forest</span></a>
<a class="sourceLine" id="cb7-6" data-line-number="6"><span class="kw">sqrt</span>(rf1<span class="op">$</span>mse[<span class="kw">which.min</span>(rf1<span class="op">$</span>mse)])</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">## [1] 25648.78</a></code></pre></div>
<p>Random forests are one of the best “out-of-the-box” machine learning algorithms. They typically perform remarkably well with very little tuning required. As illustrated above, we were able to get an RMSE of $25,649 without any tuning which is nearly as good as the best, fully tuned model we’ve explored thus far. However, we can still seek improvement by tuning hyperparameters in our random forest model.</p>
</div>
<div id="tuning-1" class="section level2">
<h2><span class="header-section-number">9.6</span> Tuning</h2>
<p>Compared to the algorithms explored in the previous chapters, random forests have more hyperparameters to tune. However, compared to gradient boosting machines and neural networks, which we explore in future chapters, random forests are much easier to tune. Typically, the primary concern when starting out is tuning the number of candidate variables to select from at each split.</p>
<div class="tip">
<p>
The two primary tuning parameters you should always tune in a random forest model are:
</p>
<ol style="list-style-type: decimal">
<li>
Number of trees as you want to ensure you apply enough trees to minimize and stabalize the error rate.
</li>
<li>
Number of candidate variables to select from at each split.
</li>
</ol>
</div>
<p>However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present:</p>
<ul>
<li><strong>Number of trees</strong>: We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets.</li>
<li><strong>Number of variables to randomly sample as candidates at each split</strong>: Commonly referred to as “mtry”. When <code>mtry</code> <span class="math inline">\(=p\)</span> the model equates to bagging. When <code>mtry</code> <span class="math inline">\(=1\)</span> the split variable is completely random, so all variables get a chance but can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to <em>p</em>.</li>
<li><strong>Sample size to train on</strong>: The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range.</li>
<li><strong>Minimum number of samples within the terminal nodes</strong>: Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and larger node size results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data).</li>
<li><strong>Maximum number of terminal nodes</strong>: Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees.</li>
<li><strong>Split rule</strong>:As stated in the introduction, the most traditional splitting rules are based on minimizing the variance or MSE across the terminal nodes for regression problems and Cross-entropy or Gini index for classification problems. However, additional splitrules have been developed that can offer improved predictive accuracy. For example, the extra trees split rule chooses cut-points fully at random and uses the whole learning sample (rather than a bootstrap replica) to grow the trees <span class="citation">(Geurts, Ernst, and Wehenkel <a href="#ref-geurts2006extremely">2006</a>)</span>.</li>
</ul>
<p>Tuning a larger set of hyperparameters requires a larger grid search than we’ve performed thus far. Unfortunately, this is where <strong>randomForest</strong> becomes quite inefficient since it does not scale well. Instead, we can use <strong>ranger</strong> <span class="citation">(Wright, Wager, and Probst <a href="#ref-R-ranger">2018</a>)</span> which is a C++ implementation of Brieman’s random forest algorithm and, as the following illustrates, is over 27 times faster than <strong>randomForest</strong>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># names of features</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">features &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">names</span>(ames_train), <span class="st">&quot;Sale_Price&quot;</span>)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3"></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="co"># randomForest speed</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="kw">system.time</span>(</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">  ames_randomForest &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">    <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb8-8" data-line-number="8">    <span class="dt">data    =</span> ames_train, </a>
<a class="sourceLine" id="cb8-9" data-line-number="9">    <span class="dt">ntree   =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb8-10" data-line-number="10">    <span class="dt">mtry    =</span> <span class="kw">floor</span>(<span class="kw">length</span>(features) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb8-11" data-line-number="11">  )</a>
<a class="sourceLine" id="cb8-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb8-13" data-line-number="13">##    user  system elapsed </a>
<a class="sourceLine" id="cb8-14" data-line-number="14">##  35.321   0.157  35.487</a>
<a class="sourceLine" id="cb8-15" data-line-number="15"></a>
<a class="sourceLine" id="cb8-16" data-line-number="16"><span class="co"># ranger speed</span></a>
<a class="sourceLine" id="cb8-17" data-line-number="17"><span class="kw">system.time</span>(</a>
<a class="sourceLine" id="cb8-18" data-line-number="18">  ames_ranger &lt;-<span class="st"> </span><span class="kw">ranger</span>(</a>
<a class="sourceLine" id="cb8-19" data-line-number="19">    <span class="dt">formula   =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb8-20" data-line-number="20">    <span class="dt">data      =</span> ames_train, </a>
<a class="sourceLine" id="cb8-21" data-line-number="21">    <span class="dt">num.trees =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb8-22" data-line-number="22">    <span class="dt">mtry      =</span> <span class="kw">floor</span>(<span class="kw">length</span>(features) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb8-23" data-line-number="23">  )</a>
<a class="sourceLine" id="cb8-24" data-line-number="24">)</a>
<a class="sourceLine" id="cb8-25" data-line-number="25">##    user  system elapsed </a>
<a class="sourceLine" id="cb8-26" data-line-number="26">##   7.009   0.132   1.087</a></code></pre></div>
<div id="tuning-via-ranger" class="section level3">
<h3><span class="header-section-number">9.6.1</span> Tuning via ranger</h3>
<p>There are two approaches to tuning a <strong>ranger</strong> model. The first is to tune <strong>ranger</strong> manually using a <code>for</code> loop. To perform a manual grid search, first we want to construct our grid of hyperparameters. We’re going to search across 48 different models with varying mtry, minimum node size, sample size, and trying different split rules.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># create a tuning grid</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2">hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">  <span class="dt">mtry            =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">35</span>, <span class="dt">by =</span> <span class="dv">5</span>),</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">  <span class="dt">min.node.size   =</span> <span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">9</span>, <span class="dt">by =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">  <span class="dt">sample.fraction =</span> <span class="kw">c</span>(.<span class="dv">632</span>, <span class="fl">.80</span>),</a>
<a class="sourceLine" id="cb9-6" data-line-number="6">  <span class="dt">splitrule       =</span> <span class="kw">c</span>(<span class="st">&quot;variance&quot;</span>, <span class="st">&quot;extratrees&quot;</span>),</a>
<a class="sourceLine" id="cb9-7" data-line-number="7">  <span class="dt">OOB_RMSE        =</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb9-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb9-9" data-line-number="9"></a>
<a class="sourceLine" id="cb9-10" data-line-number="10"><span class="kw">dim</span>(hyper_grid)</a>
<a class="sourceLine" id="cb9-11" data-line-number="11">## [1] 48  5</a></code></pre></div>
<p>We loop through each hyperparameter combination and apply 500 trees since our previous examples illustrated that 500 was plenty to achieve a stable error rate. Also note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. Our OOB RMSE ranges between ~25,900-28,500. Our top 10 performing models all have RMSE values right around 26,000 and the results show that models with larger sample sizes (80%) and a variance splitrule perform best. However, no definitive evidence suggests that certain values of <code>mtry</code> or <code>min.node.size</code> are better than other values.</p>
<div class="warning">
<p>
This grid search took 69 seconds to complete.
</p>
</div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">nrow</span>(hyper_grid))) {</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">  </a>
<a class="sourceLine" id="cb10-3" data-line-number="3">  <span class="co"># train model</span></a>
<a class="sourceLine" id="cb10-4" data-line-number="4">  model &lt;-<span class="st"> </span><span class="kw">ranger</span>(</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">    <span class="dt">formula         =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb10-6" data-line-number="6">    <span class="dt">data            =</span> ames_train, </a>
<a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="dt">num.trees       =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">    <span class="dt">mtry            =</span> hyper_grid<span class="op">$</span>mtry[i],</a>
<a class="sourceLine" id="cb10-9" data-line-number="9">    <span class="dt">min.node.size   =</span> hyper_grid<span class="op">$</span>min.node.size[i],</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">    <span class="dt">sample.fraction =</span> hyper_grid<span class="op">$</span>sample.fraction[i],</a>
<a class="sourceLine" id="cb10-11" data-line-number="11">    <span class="dt">splitrule       =</span> hyper_grid<span class="op">$</span>splitrule[i],</a>
<a class="sourceLine" id="cb10-12" data-line-number="12">    <span class="dt">seed            =</span> <span class="dv">123</span></a>
<a class="sourceLine" id="cb10-13" data-line-number="13">  )</a>
<a class="sourceLine" id="cb10-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb10-15" data-line-number="15">  <span class="co"># add OOB error to grid</span></a>
<a class="sourceLine" id="cb10-16" data-line-number="16">  hyper_grid<span class="op">$</span>OOB_RMSE[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(model<span class="op">$</span>prediction.error)</a>
<a class="sourceLine" id="cb10-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb10-18" data-line-number="18"></a>
<a class="sourceLine" id="cb10-19" data-line-number="19">hyper_grid <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb10-20" data-line-number="20"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">arrange</span>(OOB_RMSE) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb10-21" data-line-number="21"><span class="st">  </span><span class="kw">head</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb10-22" data-line-number="22">##    mtry min.node.size sample.fraction splitrule OOB_RMSE</a>
<a class="sourceLine" id="cb10-23" data-line-number="23">## 1    20             3             0.8  variance 25963.96</a>
<a class="sourceLine" id="cb10-24" data-line-number="24">## 2    25             3             0.8  variance 25981.28</a>
<a class="sourceLine" id="cb10-25" data-line-number="25">## 3    20             6             0.8  variance 25981.66</a>
<a class="sourceLine" id="cb10-26" data-line-number="26">## 4    20             9             0.8  variance 26091.08</a>
<a class="sourceLine" id="cb10-27" data-line-number="27">## 5    30             3             0.8  variance 26123.07</a>
<a class="sourceLine" id="cb10-28" data-line-number="28">## 6    25             6             0.8  variance 26145.15</a>
<a class="sourceLine" id="cb10-29" data-line-number="29">## 7    30             6             0.8  variance 26167.42</a>
<a class="sourceLine" id="cb10-30" data-line-number="30">## 8    35             3             0.8  variance 26175.66</a>
<a class="sourceLine" id="cb10-31" data-line-number="31">## 9    35             6             0.8  variance 26189.23</a>
<a class="sourceLine" id="cb10-32" data-line-number="32">## 10   25             9             0.8  variance 26210.44</a></code></pre></div>
<p>However, using this approach does not provide us with a cross validated measure of error. To get a k-fold CV error, we would have to expand our <code>for</code> loop approach or use an alternative approach. One such approach follows.</p>
</div>
<div id="tuning-via-caret" class="section level3">
<h3><span class="header-section-number">9.6.2</span> Tuning via caret</h3>
<p>The second tuning approach is to use the <strong>caret</strong> package. <strong>caret</strong> only allows you to tune some, not all, of the available <strong>ranger</strong> hyperparameters (<code>mtry</code>, <code>splitrule</code>, <code>min.node.size</code>). However, <strong>caret</strong> will allow us to get a CV measure of error to compare to our previous models (i.e. regularized regression, MARS). The following creates a similar tuning grid as before but with only those hyperparameters that <strong>caret</strong> will accept.</p>
<div class="tip">
<p>
If you do not know what hyperparameters <strong>caret</strong> allows you to tune for a specific model you can find that info at <a href="https://topepo.github.io/caret/train-models-by-tag.html" class="uri">https://topepo.github.io/caret/train-models-by-tag.html</a> or with <code>caret::getModelInfo</code>. For example, we can find the parameters available for a <code>ranger</code> with <code>caret::getModelInfo(“ranger”)<span class="math inline">\(ranger\)</span>parameter</code>.
</p>
</div>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="co"># create a tuning grid</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2">hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">  <span class="dt">mtry            =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">35</span>, <span class="dt">by =</span> <span class="dv">5</span>),</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">  <span class="dt">min.node.size   =</span> <span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">9</span>, <span class="dt">by =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb11-5" data-line-number="5">  <span class="dt">splitrule       =</span> <span class="kw">c</span>(<span class="st">&quot;variance&quot;</span>, <span class="st">&quot;extratrees&quot;</span>)</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">  )</a></code></pre></div>
<p>Tuning with <strong>caret</strong> provides similar results as our <strong>ranger</strong> grid search. Both results suggest <code>mtry = 20</code> and <code>min.node.size = 3</code>. With <strong>ranger</strong>, our OOB RMSE was 25963.96 and with <strong>caret</strong> our 10-fold CV RMSE was 25531.47.</p>
<div class="warning">
<p>
This grid search took a little over 6 minutes to complete.
</p>
</div>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># cross validated model</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">tuned_rf &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">  <span class="dt">x =</span> <span class="kw">subset</span>(ames_train, <span class="dt">select =</span> <span class="op">-</span>Sale_Price),</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">  <span class="dt">y =</span> ames_train<span class="op">$</span>Sale_Price,</a>
<a class="sourceLine" id="cb12-5" data-line-number="5">  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb12-6" data-line-number="6">  <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</a>
<a class="sourceLine" id="cb12-7" data-line-number="7">  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb12-8" data-line-number="8">  <span class="dt">tuneGrid =</span> hyper_grid,</a>
<a class="sourceLine" id="cb12-9" data-line-number="9">  <span class="dt">num.trees =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb12-10" data-line-number="10">  <span class="dt">seed =</span> <span class="dv">123</span></a>
<a class="sourceLine" id="cb12-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb12-12" data-line-number="12"></a>
<a class="sourceLine" id="cb12-13" data-line-number="13"><span class="co"># best model</span></a>
<a class="sourceLine" id="cb12-14" data-line-number="14">tuned_rf<span class="op">$</span>bestTune</a>
<a class="sourceLine" id="cb12-15" data-line-number="15">##   mtry splitrule min.node.size</a>
<a class="sourceLine" id="cb12-16" data-line-number="16">## 7   25  variance             3</a>
<a class="sourceLine" id="cb12-17" data-line-number="17"></a>
<a class="sourceLine" id="cb12-18" data-line-number="18"><span class="co"># plot results</span></a>
<a class="sourceLine" id="cb12-19" data-line-number="19"><span class="kw">ggplot</span>(tuned_rf)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-grid-search2"></span>
<img src="abar_files/figure-html/rf-grid-search2-1.png" alt="Cross validated RMSE for the __caret__ grid search." width="672" />
<p class="caption">
Figure 9.7: Cross validated RMSE for the <strong>caret</strong> grid search.
</p>
</div>
</div>
</div>
<div id="feature-interpretation-2" class="section level2">
<h2><span class="header-section-number">9.7</span> Feature interpretation</h2>
<p>Whereas many of the linear models discussed previously use the standardized coefficients to signal importance, random forests have, historically, applied two different approaches to measure variable importance.</p>
<ol style="list-style-type: decimal">
<li><strong>Impurity</strong>: At each split in each tree, compute the improvement in the split-criterion (MSE for regression Gini for classification). Then average the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in the error metric are considered most important.</li>
<li><strong>Permutation</strong>: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important.</li>
</ol>
<p>To compute these variable importance measures with <strong>ranger</strong>, you must include the importance argument.</p>
<div class="note">
<p>
Once you’ve identified the optimal parameter values from the grid search, you will want to re-run your model with these hyperparameter values.
</p>
</div>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># re-run model with impurity-based variable importance</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">rf_impurity &lt;-<span class="st"> </span><span class="kw">ranger</span>(</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb13-4" data-line-number="4">  <span class="dt">data            =</span> ames_train, </a>
<a class="sourceLine" id="cb13-5" data-line-number="5">  <span class="dt">num.trees       =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">  <span class="dt">mtry            =</span> <span class="dv">20</span>,</a>
<a class="sourceLine" id="cb13-7" data-line-number="7">  <span class="dt">min.node.size   =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb13-8" data-line-number="8">  <span class="dt">sample.fraction =</span> <span class="fl">.80</span>,</a>
<a class="sourceLine" id="cb13-9" data-line-number="9">  <span class="dt">splitrule       =</span> <span class="st">&quot;variance&quot;</span>,</a>
<a class="sourceLine" id="cb13-10" data-line-number="10">  <span class="dt">importance      =</span> <span class="st">&#39;impurity&#39;</span>,</a>
<a class="sourceLine" id="cb13-11" data-line-number="11">  <span class="dt">verbose         =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb13-12" data-line-number="12">  <span class="dt">seed            =</span> <span class="dv">123</span></a>
<a class="sourceLine" id="cb13-13" data-line-number="13">  )</a>
<a class="sourceLine" id="cb13-14" data-line-number="14"></a>
<a class="sourceLine" id="cb13-15" data-line-number="15"><span class="co"># re-run model with permutation-based variable importance</span></a>
<a class="sourceLine" id="cb13-16" data-line-number="16">rf_permutation &lt;-<span class="st"> </span><span class="kw">ranger</span>(</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb13-18" data-line-number="18">  <span class="dt">data            =</span> ames_train, </a>
<a class="sourceLine" id="cb13-19" data-line-number="19">  <span class="dt">num.trees       =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb13-20" data-line-number="20">  <span class="dt">mtry            =</span> <span class="dv">20</span>,</a>
<a class="sourceLine" id="cb13-21" data-line-number="21">  <span class="dt">min.node.size   =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb13-22" data-line-number="22">  <span class="dt">sample.fraction =</span> <span class="fl">.80</span>,</a>
<a class="sourceLine" id="cb13-23" data-line-number="23">  <span class="dt">splitrule       =</span> <span class="st">&quot;variance&quot;</span>,</a>
<a class="sourceLine" id="cb13-24" data-line-number="24">  <span class="dt">importance      =</span> <span class="st">&#39;permutation&#39;</span>,</a>
<a class="sourceLine" id="cb13-25" data-line-number="25">  <span class="dt">verbose         =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb13-26" data-line-number="26">  <span class="dt">seed            =</span> <span class="dv">123</span></a>
<a class="sourceLine" id="cb13-27" data-line-number="27">  )</a></code></pre></div>
<p>For both options, you can directly access the variable importance values with <code>model_name$variable.importance</code>. However, here we’ll plot the variable importance using the <code>vip</code> package. Typically, you will not see the same variable importance order between the two options; however, you will often see similar variables at the top of the plots. Consquently, in this example, we can comfortably state that there appears to be enough evidence to suggest that two variables stand out as most influential:</p>
<ul>
<li><code>Overall_Qual</code></li>
<li><code>Gr_Liv_Area</code></li>
</ul>
<p>Looking at the next ~10 variables in both plots, you will also see some commonality in influential variables (i.e. <code>Garage_Cars</code>, <code>Bsmt_Qual</code>, <code>Year_Built</code>, <code>Exter_Qual</code>).</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">p1 &lt;-<span class="st"> </span><span class="kw">vip</span>(rf_impurity, <span class="dt">num_features =</span> <span class="dv">25</span>, <span class="dt">bar =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Impurity-based variable importance&quot;</span>)</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">p2 &lt;-<span class="st"> </span><span class="kw">vip</span>(rf_permutation, <span class="dt">num_features =</span> <span class="dv">25</span>, <span class="dt">bar =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Permutation-based variable importance&quot;</span>)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3"></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-vip-plots"></span>
<img src="abar_files/figure-html/rf-vip-plots-1.png" alt="Top 25 most important variables based on impurity (left) and permutation (right)." width="864" />
<p class="caption">
Figure 9.8: Top 25 most important variables based on impurity (left) and permutation (right).
</p>
</div>
<p>To better understand the relationship between these important features and <code>Sale_Price</code>, we can create partial dependence plots (PDPs). If you recall in the linear and regularized regression sections, we saw that the linear model assumed a continously increasing relationship between <code>Gr_Liv_Area</code> and <code>Sale_Price</code>. In the MARS chapter, we saw as homes exceed 2,945 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less than 2,945 square feet. However, in between the knots of a MARS model, the relationship will remain linear. However, the PDP plot below displays how random forest models can capture unique non-linear and non-monotonic relationships between predictors and the target. In this case, <code>Sale_Price</code> appears to not be influenced by <code>Gr_Liv_Area</code> values below 750 sqft or above 3500 sqft. This change in realtionship was not well captured by the prior parametric models.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># partial dependence of Sale_Price on Gr_Liv_Area</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2">rf_impurity <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="st">  </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, <span class="dt">grid.resolution =</span> <span class="dv">50</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb15-4" data-line-number="4"><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_train)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-pdp-GrLiv-Area"></span>
<img src="abar_files/figure-html/rf-pdp-GrLiv-Area-1.png" alt="The mean predicted sale price as the above ground living area increases." width="384" />
<p class="caption">
Figure 9.9: The mean predicted sale price as the above ground living area increases.
</p>
</div>
<p>Additionally, if we assess the relationship between the <code>Overall_Qual</code> predictor and <code>Sale_Price</code>, we see a continual increase as the overall quality increases. This provides a more comprehensive understanding of the relationship than the results we saw in the regularized regression section (<a href="regularized-regression.html#lm-features">7.5</a>). We see that the largest impact on <code>Sale_Price</code> occurs when houses go from “Good” overall quality to “Very Good”.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># partial dependence of Sale_Price on Overall_Qual</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">rf_impurity <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"><span class="st">  </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Overall_Qual&quot;</span>, <span class="dt">train =</span> <span class="kw">as.data.frame</span>(ames_train)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="st">  </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-pdp-Overall-Qual"></span>
<img src="abar_files/figure-html/rf-pdp-Overall-Qual-1.png" alt="The mean predicted sale price for each level of the overall quality variable." width="768" />
<p class="caption">
Figure 9.10: The mean predicted sale price for each level of the overall quality variable.
</p>
</div>
<p>Individual conditional expectation (ICE) curves <span class="citation">(Goldstein et al. <a href="#ref-goldstein2015peeking">2015</a>)</span> are an extension of PDP plots but, rather than plot the <em>average</em> marginal effect on the response variable, we plot the change in the predicted response variable <strong><em>for each observation</em></strong> as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results.</p>
<p>The plots below show that marginal changes in <code>Gr_Liv_Area</code> have a fairly homogenous effect on our response variable. As <code>Gr_Liv_Area</code> increases, the vast majority of observations show a similar increasing effect on the predicted <code>Sale_Price</code> value. The primary differences is in the magnitude of the increasing effect. However, in the centered ICE plot you see evidence of a few observations that display a different pattern. Some have a higher <span class="math inline">\(\hat y\)</span> value when <code>Gr_Liv_Area</code> is between 2500-4000 and some have a decreasing <span class="math inline">\(\hat y\)</span> as <code>Gr_Liv_AreA</code> increases. These results may be a sign of interaction effects and would be worth exploring more closely.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># ice curves of Sale_Price on Gr_Liv_Area</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">ice1 &lt;-<span class="st"> </span>rf_impurity <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="st">  </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, <span class="dt">grid.resolution =</span> <span class="dv">50</span>, <span class="dt">ice =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_train, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Non-centered ICE plot&quot;</span>)</a>
<a class="sourceLine" id="cb17-6" data-line-number="6"></a>
<a class="sourceLine" id="cb17-7" data-line-number="7">ice2 &lt;-<span class="st"> </span>rf_impurity <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-8" data-line-number="8"><span class="st">  </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Gr_Liv_Area&quot;</span>, <span class="dt">grid.resolution =</span> <span class="dv">50</span>, <span class="dt">ice =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9"><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> ames_train, <span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-10" data-line-number="10"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Centered ICE plot&quot;</span>)</a>
<a class="sourceLine" id="cb17-11" data-line-number="11"></a>
<a class="sourceLine" id="cb17-12" data-line-number="12">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(ice1, ice2, <span class="dt">nrow =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-ice-Gr-Liv-Area"></span>
<img src="abar_files/figure-html/rf-ice-Gr-Liv-Area-1.png" alt="Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations." width="864" />
<p class="caption">
Figure 9.11: Non-centered (left) and centered (right) individual conditional expectation curve plots illustrate how changes in above ground square footage influences predicted sale price for all observations.
</p>
</div>
<p>Both PDPs and ICE curves should be assessed for the most influential variables as they help to explain the underlying patterns in the data that the random forest model is picking up.</p>
</div>
<div id="attrition-data-2" class="section level2">
<h2><span class="header-section-number">9.8</span> Attrition data</h2>
<p>With the Ames data, the random forest models obtained predictive accuracy that was close to our best MARS model, but how about the attrition data? The following performs a grid search across 48 hyperparameter combinations.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co"># get attrition data</span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2">df &lt;-<span class="st"> </span>rsample<span class="op">::</span>attrition <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">mutate_if</span>(is.ordered, factor, <span class="dt">ordered =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb18-3" data-line-number="3"></a>
<a class="sourceLine" id="cb18-4" data-line-number="4"><span class="co"># Create training (70%) and test (30%) sets for the rsample::attrition data.</span></a>
<a class="sourceLine" id="cb18-5" data-line-number="5"><span class="co"># Use set.seed for reproducibility</span></a>
<a class="sourceLine" id="cb18-6" data-line-number="6"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb18-7" data-line-number="7">churn_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(df, <span class="dt">prop =</span> <span class="fl">.7</span>, <span class="dt">strata =</span> <span class="st">&quot;Attrition&quot;</span>)</a>
<a class="sourceLine" id="cb18-8" data-line-number="8">churn_train &lt;-<span class="st"> </span><span class="kw">training</span>(churn_split)</a>
<a class="sourceLine" id="cb18-9" data-line-number="9">churn_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(churn_split)</a>
<a class="sourceLine" id="cb18-10" data-line-number="10"></a>
<a class="sourceLine" id="cb18-11" data-line-number="11"><span class="co"># create a tuning grid</span></a>
<a class="sourceLine" id="cb18-12" data-line-number="12">hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb18-13" data-line-number="13">  <span class="dt">mtry            =</span> <span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">18</span>, <span class="dt">by =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb18-14" data-line-number="14">  <span class="dt">min.node.size   =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dt">by =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb18-15" data-line-number="15">  <span class="dt">splitrule       =</span> <span class="kw">c</span>(<span class="st">&quot;gini&quot;</span>, <span class="st">&quot;extratrees&quot;</span>)</a>
<a class="sourceLine" id="cb18-16" data-line-number="16">  )</a>
<a class="sourceLine" id="cb18-17" data-line-number="17"></a>
<a class="sourceLine" id="cb18-18" data-line-number="18"><span class="co"># cross validated model</span></a>
<a class="sourceLine" id="cb18-19" data-line-number="19">tuned_rf &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb18-20" data-line-number="20">  <span class="dt">x =</span> <span class="kw">subset</span>(churn_train, <span class="dt">select =</span> <span class="op">-</span>Attrition),</a>
<a class="sourceLine" id="cb18-21" data-line-number="21">  <span class="dt">y =</span> churn_train<span class="op">$</span>Attrition,</a>
<a class="sourceLine" id="cb18-22" data-line-number="22">  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb18-23" data-line-number="23">  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb18-24" data-line-number="24">  <span class="dt">tuneGrid =</span> hyper_grid,</a>
<a class="sourceLine" id="cb18-25" data-line-number="25">  <span class="dt">num.trees =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb18-26" data-line-number="26">  <span class="dt">seed =</span> <span class="dv">123</span></a>
<a class="sourceLine" id="cb18-27" data-line-number="27">)</a>
<a class="sourceLine" id="cb18-28" data-line-number="28"></a>
<a class="sourceLine" id="cb18-29" data-line-number="29"><span class="co"># best model</span></a>
<a class="sourceLine" id="cb18-30" data-line-number="30">tuned_rf<span class="op">$</span>bestTune</a>
<a class="sourceLine" id="cb18-31" data-line-number="31">##    mtry splitrule min.node.size</a>
<a class="sourceLine" id="cb18-32" data-line-number="32">## 31   12      gini            10</a>
<a class="sourceLine" id="cb18-33" data-line-number="33"></a>
<a class="sourceLine" id="cb18-34" data-line-number="34"><span class="co"># plot results</span></a>
<a class="sourceLine" id="cb18-35" data-line-number="35"><span class="kw">ggplot</span>(tuned_rf)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rf-tuned-rf-attrition"></span>
<img src="abar_files/figure-html/rf-tuned-rf-attrition-1.png" alt="Cross-validated accuracy rate for the 48 different hyperparameter combinations in our grid search. The optimal model uses `mtry` = 9, `splitrule` = gini, and `min.node.size` = 4, which obtained a 10-fold CV accuracy rate of 85.8%." width="672" />
<p class="caption">
Figure 9.12: Cross-validated accuracy rate for the 48 different hyperparameter combinations in our grid search. The optimal model uses <code>mtry</code> = 9, <code>splitrule</code> = gini, and <code>min.node.size</code> = 4, which obtained a 10-fold CV accuracy rate of 85.8%.
</p>
</div>
<p>Similar to the MARS model, the random forest model does not improve predictive accuracy over an above the regularized regression model.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Min.
</th>
<th style="text-align:right;">
1st Qu.
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
3rd Qu.
</th>
<th style="text-align:right;">
Max.
</th>
<th style="text-align:right;">
NA’s
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Logistic_model
</td>
<td style="text-align:right;">
0.8446602
</td>
<td style="text-align:right;">
0.8532981
</td>
<td style="text-align:right;">
0.8557692
</td>
<td style="text-align:right;">
0.8670209
</td>
<td style="text-align:right;">
0.8899914
</td>
<td style="text-align:right;">
0.8942308
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Elastic_net
</td>
<td style="text-align:right;">
0.8269231
</td>
<td style="text-align:right;">
0.8661955
</td>
<td style="text-align:right;">
0.8743932
</td>
<td style="text-align:right;">
0.8749103
</td>
<td style="text-align:right;">
0.8899914
</td>
<td style="text-align:right;">
0.9313725
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
MARS_model
</td>
<td style="text-align:right;">
0.8039216
</td>
<td style="text-align:right;">
0.8474375
</td>
<td style="text-align:right;">
0.8647311
</td>
<td style="text-align:right;">
0.8650623
</td>
<td style="text-align:right;">
0.8810680
</td>
<td style="text-align:right;">
0.9313725
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
RF_model
</td>
<td style="text-align:right;">
0.8235294
</td>
<td style="text-align:right;">
0.8409961
</td>
<td style="text-align:right;">
0.8653846
</td>
<td style="text-align:right;">
0.8612044
</td>
<td style="text-align:right;">
0.8802113
</td>
<td style="text-align:right;">
0.9019608
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
</div>
<div id="final-thoughts-4" class="section level2">
<h2><span class="header-section-number">9.9</span> Final thoughts</h2>
<p>Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. However, as we illustrated in this chapter, random forests are not guaranteed to improve predictive accuracy over and above linear models and their cousins. The following summarizes some of the advantages and disadvantages discussed regarding random forests modeling:</p>
<p><strong>TODO</strong>: may need to better tie in some of these advantages and disadvantages throughout the chapter.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Typically have very good performance.</li>
<li>Remarkably good “out-of-the box” - very little tuning required.</li>
<li>Built-in validation set - don’t need to sacrifice data for extra validation.</li>
<li>Does not overfit.</li>
<li>No data pre-processing required - often works great with categorical and numerical values as is.</li>
<li>Robust to outliers.</li>
<li>Handles missing data - imputation not required.</li>
<li>Provide automatic feature selection.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Can become slow on large data sets.</li>
<li>Although accurate, often cannot compete with the accuracy of advanced boosting algorithms.</li>
<li>Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).</li>
</ul>
</div>
<div id="learning-more-4" class="section level2">
<h2><span class="header-section-number">9.10</span> Learning more</h2>
<p>The literature behind random forests are rich and we have only touched on the fundamentals. To learn more I would start with the following resources listed in order of complexity:</p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
<li><a href="https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894">Computer Age Statistical Inference</a></li>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman2017classification">
<p>Breiman, Leo. 1984. <em>Classification and Regression Trees</em>. Routledge.</p>
</div>
<div id="ref-R-rpart">
<p>Therneau, Terry, and Beth Atkinson. 2018. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart" class="uri">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-R-rpart.plot">
<p>Milborrow, Stephen. 2018. <em>Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’Plot.rpart’</em>. <a href="https://CRAN.R-project.org/package=rpart.plot" class="uri">https://CRAN.R-project.org/package=rpart.plot</a>.</p>
</div>
<div id="ref-harrison1978hedonic">
<p>Harrison Jr, David, and Daniel L Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” <em>Journal of Environmental Economics and Management</em> 5 (1). Elsevier: 81–102.</p>
</div>
<div id="ref-breiman1996bagging">
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2). Springer: 123–40.</p>
</div>
<div id="ref-R-randomForest">
<p>Breiman, Leo, Adele Cutler, Andy Liaw, and Matthew Wiener. 2018. <em>RandomForest: Breiman and Cutler’s Random Forests for Classification and Regression</em>. <a href="https://CRAN.R-project.org/package=randomForest" class="uri">https://CRAN.R-project.org/package=randomForest</a>.</p>
</div>
<div id="ref-geurts2006extremely">
<p>Geurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. “Extremely Randomized Trees.” <em>Machine Learning</em> 63 (1). Springer: 3–42.</p>
</div>
<div id="ref-R-ranger">
<p>Wright, Marvin N., Stefan Wager, and Philipp Probst. 2018. <em>Ranger: A Fast Implementation of Random Forests</em>. <a href="https://CRAN.R-project.org/package=ranger" class="uri">https://CRAN.R-project.org/package=ranger</a>.</p>
</div>
<div id="ref-goldstein2015peeking">
<p>Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” <em>Journal of Computational and Graphical Statistics</em> 24 (1). Taylor &amp; Francis: 44–65.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>See <span class="citation">Esposito et al. (<a href="#ref-esposito1997comparative">1997</a>)</span> for various methods of pruning.<a href="RF.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Combining multiple models is referred to as <em>ensembling</em>.<a href="RF.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>See the Random Forest section in the <a href="https://CRAN.R-project.org/view=MachineLearning">Machine Learning Task View</a> on CRAN and Erin LeDell’s <a href="https://koalaverse.github.io/machine-learning-in-R/random-forest.html#random-forest-software-in-r">useR! Machine Learning Tutorial</a> for a non-comprehensive list.<a href="RF.html#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="MARS.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["abar.pdf", "abar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
