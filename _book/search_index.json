[
["MARS.html", "Chapter 8 Multivariate Adaptive Regression Splines 8.1 Prerequisites 8.2 The basic idea 8.3 Fitting MARS models 8.4 Tuning 8.5 Final thoughts 8.6 Learning more", " Chapter 8 Multivariate Adaptive Regression Splines The previous chapters discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects); however, do so you must know the specific nature of the nonlinearity a priori. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy. This chapter discusses multivariate adaptive regression splines (MARS), an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future chapters will focus on other nonlinear algorithms. 8.1 Prerequisites library(rsample) library(ggplot2) To illustrate various MARS modeling concepts we will use the Ames Housing data; however, at the end of the chapter we will also apply regularization to the employee attrition data. # Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data. # Use set.seed for reproducibility set.seed(123) ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7, strata = &quot;Sale_Price&quot;) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 8.2 The basic idea The problem with linear models…making global assumptions…ISLR Figure 8.1: Blue line represents predicted Sale_Price values as a function of Year_Built for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional nonlinear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting Year_Built into three categorical levels. 8.2.1 Basis functions 8.2.2 Regression splines 8.3 Fitting MARS models 8.4 Tuning 8.5 Final thoughts FIXME: refine this section Advantages: Disadvantages: 8.6 Learning more This will get you up and running with MARS modeling. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more: An Introduction to Statistical Learning Applied Predictive Modeling Elements of Statistical Learning "]
]
