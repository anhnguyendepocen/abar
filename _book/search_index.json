[
["descriptive.html", "Chapter 2 Descriptive Statistics 2.1 Prerequisites 2.2 Measures of location 2.3 Measures of spread 2.4 Percentiles 2.5 Robust measures of spread 2.6 Outlier detection 2.7 Describing categorical data 2.8 Exercises", " Chapter 2 Descriptive Statistics The first step in any data analysis problem is to describe the data using descriptive statistics and look at the data using appropriate graphical techniques. In this chapter, we will introduce some basic descriptive statistics (e.g., various measures of location and spread) while graphical techniques are discussed in Chapter 3. Descriptive statistics, in contrast to inferential statistics (Chapter 4), aim to describe a data sample, or sample. A sample is simply a set of data collected from some population of interest (e.g., the annual salaries of males at a particular company). In this book, we refer to the individual sample points as observations. Typically, the data are collected in a way such that the sample is representative of the population from which it came. Other times, we may have access to the entire population, in which case, our sample comprises a census. In either case, descriptive statistics seek to paint a quantitative picture of the data using simple values such as measures of location (i.e., what a typical values looks like) and dispersion (i.e., how spread out the data are). Beyond measures of location and spread, we also discuss percentiles and simple ways for detecting outliers (i.e., unusually small or large observations). Various other descriptive statistics and useful R packages are introduced in the exercises at the end of the chapter. 2.1 Prerequisites The R package stats (which is part of the standard R distribution) provides functions for many of the descriptive statistics discussed in this chapter, plus more. For a complete list of functions, type library(help = &quot;stats&quot;) into the R console. For illustration, we will use two data sets: (1) the Ames housing data set (Cock 2011) (in particular, the column labelled Sale_Price) and (2) the Adult data set (Lichman 2013); both of which are described in Chapter 1. # Ames housing data dim(ames &lt;- AmesHousing::make_ames()) # construct data and print dimensions ## [1] 2930 81 head(Sale_Price &lt;- ames$Sale_Price) # extract Sale_Price column ## [1] 215000 105000 172000 244000 189900 195500 # Adult data data(AdultUCI, package = &quot;arules&quot;) # load data from arules package dim(AdultUCI) # print dimensions ## [1] 48842 15 2.2 Measures of location The first question we might ask of the ames data is ‚ÄúWhat is a typical value for the selling price?‚Äù In particular, we are interested in some measure of central location or central tendency. Such measures try to summarize a set of values with a ‚Äútypical‚Äù number. There are a number of different measures of location, but the simplest and most commonly used is the arithmetic mean or sample mean. 2.2.1 The sample mean Suppose we have a set of \\(n\\) observations denoted \\(x_1, x_2, \\dots, x_n\\). The sample mean, denoted \\(\\bar{x}\\), is defined as the sum of the observations divided \\(n\\): \\[\\begin{equation} \\label{eqn:sample-mean} \\bar{x} = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\frac{1}{n}\\left(x_1 + x_2 + \\dots + x_n\\right), \\end{equation}\\] where \\(\\sum\\) is mathematical notation for summation and simply means ‚Äúadd up the values‚Äù. Another way to think of the sample mean is as the ``center of gravity‚Äô‚Äô for a set of observations. That is, if the observations were placed on a number line, like a teeter-totter, the sample mean would be the balancing point. For example, the sample mean of 1.7, 3.3, 7.5, 8.1, and 8.9 is \\(\\bar{x} = 5.9\\) which is displayed in Figure 2.1. Figure 2.1: The sample mean as the balancing point of a set of five observations. In R, we can obtain the sample mean of a set of observations using the mean() function mean(c(1.7, 3.3, 7.5, 8.1, 8.9)) ## [1] 5.9 For example, the sample mean of Sale_Price can be obtained using mean(Sale_Price) ## [1] 180796.1 mean(Sale_Price) # alternatively ## [1] 180796.1 Thus, a typical or central value of selling price for the ames data frame would be $180,796. Most of the descriptive statistical functions in R include an na.rm argument which defaults to FALSE. If na.rm = FALSE, the return value for these functions will be NA (i.e., R‚Äôs representation of a missing value; see ?NA for details.) whenever the sample contains at least one NA. If set to TRUE, then all NAs will be removed from the sample prior to computing the statistic. This is illustrated in the following code chunk: x &lt;- c(18, 4, 12, NA, 19) mean(x) ## [1] NA mean(x, na.rm = TRUE) ## [1] 13.25 (18 + 4 + 12 + 19) / 4 # sanity check ## [1] 13.25 2.2.2 The sample median One problem with the sample mean is that it is not robust to outliers. To illustrate, suppose we have a sample of size two: \\(x_1\\) and \\(x_2\\) so that \\(\\bar{x} = \\left(x_1 + x_2\\right) / 2\\). Then, regardless of the value of \\(x_1\\), we can change \\(x_2\\) to achieve any value for the sample mean. Since it only takes one of the \\(n\\) observations to arbitrarily change \\(\\bar{x}\\), we say that \\(\\bar{x}\\) has a finite sample breakdown point (FSBP) of \\(1 / n\\). The higher the FSBP of a sample statistic, the less affected it is to outliers (i.e., the more robust it is). The highest FSBP a sample statistic can obtain is 50%. A more robust measure of location is given by the sample median. Consider a sample of size \\(n\\): \\(x_1, x_2, \\dots, x_n\\). Let \\(x_{\\left(i\\right)}\\) denote the \\(i\\)-th observation after the sample has been sorted in ascending order. The sample median, denoted \\(M\\), is defined as \\[\\begin{equation*} M = \\begin{cases} x_{\\left(m\\right)} &amp; \\quad \\text{if } n \\text{ is odd}\\\\ \\left(x_{\\left(m\\right)} + x_{\\left(m + 1\\right)}\\right) / 2 &amp; \\quad \\text{if } n \\text{ is even}, \\end{cases} \\end{equation*}\\] where \\(m = \\left(n + 1\\right) / 2\\). In other words, if \\(n\\) is odd, the sample median is just the middle number, otherwise, we take the sample mean of the two middle numbers. Since the sample median only depends on the middle number (or middle two numbers), it is far more robust than the sample mean. In fact, the sample median has an FSBP of roughly 50%. In other words, close to 50% of the observations have to be outliers in order to affect the sample median. This makes the sample median more useful in practice when dealing with data that contain outliers or come from skewed distributions. In R, we can compute the sample median using the median() function. For the Ames housing data, the median sale price is $160,000, which can be computed using median(Sale_Price) ## [1] 160000 median(ames$Sale_Price) # alternatively ## [1] 160000 2.2.3 The mean or the median So which should be used in practice, the sample mean or the sample median? If the data are roughly symmetric, then the sample mean and sample median will be approximately equal. In fact, when the sample mean is most useful for reporting, it will typically be close to the sample median. If, however, the data are skewed to the left or right, then the sample mean will tend to get pulled in the same direction. For example, for right (or positively) skewed data, the sample mean will typically be larger than the sample median (sometimes much larger). In these cases, the sample median is a more reliable measure of location. However, there is nothing wrong with reporting both statistics. For the ames data frame, the sample median was smaller than the sample mean by $20,796.06. This is not surprising since data on housing and sale prices tend to be skewed right which can inflate the sample mean. In this case, the sample median will be a better measure of location than the sample mean. Different approaches to detecting skewness will be discussed in Chapter 3 when we talk about visualizing data. 2.3 Measures of spread Measures of location by themselves are not very useful. We often want to know how ``spread out‚Äô‚Äô the data are. This can be summarized using various measures of spread or dispersion. The most common measure of spread is the sample variance, denoted by \\(s^2\\). The sample variance of a sample is defined as \\[\\begin{equation*} s ^ 2 = \\sum_{i = 1} ^ n \\left(x_i - \\bar{x}\\right) ^ 2 / \\left(n - 1\\right). \\end{equation*}\\] That is, the sample variance is just the sum of the squared deviations of each observation from the sample mean dived by \\(n - 1\\). The \\(n - 1\\) is used to make \\(s ^ 2\\) an unbiased estimator1 of the population variance. Since the sample variance involves squaring the differences, it does not retain the original units, unlike the sample mean and variance. Oftentimes the positive square root of the sample variance, called the sample standard deviation, is used instead. The sample standard deviation, denoted \\(s\\), is more useful because is has the same units as the original observations (e.g., feet, dollars, etc.). In R, the sample variance and sample standard deviation can be computed using the functions var() and sd(), respectively. The following example illustrates their use on the ames data frame using the variable Sale_Price. var(Sale_Price) # sample variance ## [1] 6381883616 sd(Sale_Price) # sample standard deviation ## [1] 79886.69 sqrt(var(Sale_Price)) # sanity check ## [1] 79886.69 Since the sample standard deviation retains the original units, we can report this in dollar amount (e.g., the standard deviation of sale prices for homes sold from 2006‚Äì2010 is $79,886.69). 2.3.1 The empirical rule It is possible for the distribution of some of the variables in a data sets to exhibit a ‚Äúbell shape‚Äù üîî. For bell-shaped distributions, the empirical rule, also known as the 68-95-99.7 rule, states that (roughly) 68% of the observations should fall within one standard deviation of the mean, 95% should fall within two standard deviations of the mean, and 99.7% should fall within three standard deviations of the mean‚Äîthis is illustrated in Figure 2.2. Therefore, data from bell-shaped distributions can be adequately described my the sample mean and and standard deviation (if it exists). The most important takeaway is that the majority of observations from a bell-shaped distribution should be within a couple of standard deviations from the mean, and it is extremely unlikely for an observation to be beyond three standard deviations from the mean. This provides an intuitive and simple rule for identifying potential outliers (see Section 2.6). Figure 2.2: The empricial rule for bell-shaped distributions. (In progress!) To reiterate, the empirical rule applies to data that are (at least approximately) bell-shaped. To ascertain the shape of the distribution of a sample, a histogram or kernel density estimate can be used‚Äîthese are discussed in Chapter 3. A histogram of the Sale_Price data is displayed in the left side of Figure 2.3. These data are not bell-shaped, or even symmetric‚Äîin fact, Sale_Price appears to be skew right. Data that are skew right can often be transformed to appear more bell-shaped by taking a logarithm or square root transformation. A histogram of log(Sale_Price) is displayed in the right side of Figure 2.3. From the histograms, it is clear that Sale_Price is in fact skew right and that taking the logarithm makes the distribution appear more bell-shaped‚Äîsuch transformations are useful for some of the statistical inference procedures discussed in Chapter 4 which assume that the data are approximately normally distributed. FIXME: What other methods in this book assume normality (and therefore symmetry)? For example, regression. Figure 2.3: Histogram estimates of the distribution of Sale_Price (left) and log(Sale_Price) (right). 2.4 Percentiles The \\(p\\)-th percentile of a sample, denoted \\(x_p\\), is the value for which \\(p\\) percent of the observations are less than \\(x_p\\). The median, for example, is the 50-th percentile (i.e., the middle number). Names are given to special groups of percentiles. Deciles, for example, divide a sample into ten equally sized buckets. Quartiles are the values that divide the data into four equally sized groups‚Äîin other words, the quartiles of a sample consists of the 25-th, 50-th, and 75-th percentiles. The quartiles, denoted \\(Q_1\\), \\(Q_2\\), and \\(Q_3\\) (\\(Q_1\\) and \\(Q_3\\) are also referred to as the lower and upper quartiles, respectively), play an important part in many descriptive and graphical analyses. Together with measures of location and spread, the percentiles help describe the shape of the sample (i.e., what its distribution looks like). In fact, one of the most useful graphics for describing a sample is the boxplot which is described in Chapter 3. The boxplot is a simple visualization capturing the quartiles, median, as well as the maximum and minimum values and is extremely effective at showing the shape of a set of data (these five summary statistics are collectively known as Tukey‚Äôs five-number summary). A modern alternative to boxplots, called violin plots, will also be discussed in Chapter 3. The formula for computing the \\(p\\)-th percentile for a sample is not unique and many definitions exist. In fact, R includes nine different algorithms (controlled via the type argument) for computing percentiles! Therefore, it is important to realize that different software may produce slightly different results when computing percentiles. To reproduce the same quantiles provided by SAS2, specify type = 3 in the call to quantile(). FIXME: This needs to be verified! The R function for computing percentiles is quantile(). Quantiles are essentially the same as percentiles, but specified using decimals rather than percentages. For example, the 5-th percentile is equivalent to the 0.05 quantile. The following code chunk computes the quartiles of Sale_Price from the ames data frame. We use the default algorithm (i.e., type = 7); for specifics, see the help page ?stats::quantile. quantile(Sale_Price, probs = c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 129500 160000 213500 In other words, 25% of the sale prices were below $129,500, 25% between $129,500 and $160,000, 25% between $160,000 and $213,500, and the rest greater than $213,500. 2.5 Robust measures of spread Since the sample standard deviation relies on squared deviations from the sample mean (a non-robust measure of location), it is also sensitive to outliers. A measure of spread less affected by outliers is the interquartile range (IQR). The IQR is defined as the difference between the upper and lower quartiles: \\[\\begin{equation*} \\text{IQR} = Q_3 - Q_1. \\end{equation*}\\] The IQR describes the variability of the middle 50% of the data and has an FSBP of 25%. Therefore, the IQR is a more robust measure of spread than the sample standard deviation. Perhaps a more useful, but less often used, robust measure of spread is the median absolute deviation (MAD). For a sample \\(x_1, x_2, \\dots, x_n\\) with sample median \\(M\\), the MAD is given by the median of the absolute value of the deviations from \\(M\\): \\[\\begin{equation*} \\text{MAD} = median\\left(\\left|x_1 - M\\right|, \\left|x_2 - M\\right|, \\dots, \\left|x_n - M\\right|\\right). \\end{equation*}\\] The MAD, like the median, has an FSBP of 50%, meaning nearly half the observations could be outliers without affecting the MAD. Some software, including R by default, actually computes an adjusted version of MAD, called MADN, by multiplying by the constant \\(1.4826\\): \\(\\text{MADN} = 1.4826 \\times \\text{MAD}\\). This adjustment is to make MAD asymptotically normally consistent for the population standard deviation \\(\\sigma\\). In other words, as the sample size \\(n\\) gets larger, MADN is a good estimator of \\(\\sigma\\) for a normally distributed population. In practice, MADN is typically used/reported. To compute the IQR or MAD(N) in R, we can use the IQR() and mad() functions, respectively. For the ames data frame, the IQR and MAD(N) for Sale_Price are computed below. Note that since the IQR is based on the 25-th and 75-th percentiles, the IQR() function also includes the option type for specifying which algorithm to use for computing \\(Q_1\\) and \\(Q_3\\). IQR(Sale_Price) ## [1] 84000 mad(Sale_Price) # MADN ## [1] 54856.2 mad(Sale_Price, constant = 1) # MAD ## [1] 37000 mad(Sale_Price, constant = 1) * 1.4826 # sanity check ## [1] 54856.2 In practice, it is common (and important) to report some measure of spread whenever reporting measures of location (and vice versa). The standard deviation is often reported with the mean. Whenever the median is used, the IQR or MAD(N) is often reported as well. 2.6 Outlier detection In this section, we present a few simple rules for detecting potential outliers in univariate data; that is, data on a single variable. In later chapters, we present more sophisticated methods for detecting potential outliers and anomalies in multivariate data (i.e., data on more than one variable). The empirical rule (Section 2.3.1) probably offers the simplest method for detecting outliers, at least for reasonably bell-shaped data. Recall that for approximately bell-shaped distributions, 95% of the observations should lie within two standard deviations of the mean. Therefore, for approximately bell-shaped distributions, \\(z\\)-scores greater than two in absolute value might be considered unusual (a cutoff of 2.24 has been shown to be more useful in practice (Wilcox 2003)). To make this rule more robust, we can compute a modified \\(z\\)-score based on the median and MADN. Below, we define a simple function for detecting outliers according to the empirical rule: detect_outliers &lt;- function(x, robust = FALSE, cutoff = 2) { z_score &lt;- if (robust) { (x - median(x)) / mad(x) # modified z-score } else { (x - mean(x)) / sd(x) # z-score } sort(x[abs(z_score) &gt; cutoff]) } Next, we simulate some data from a standard normal distribution (i.e., a bell-shaped distribution with mean zero and standard deviation one) with two outliers (5 and -100) and test out our detect_outliers() function. For a standard normal distribution, we would expect any observations greater than two in absolute value to be unlikely and should be flagged as potential outliers. set.seed(101) # for reproducibility x &lt;- c(rnorm(100), 5, -100) detect_outliers(x) ## [1] -100 detect_outliers(x, robust = TRUE) ## [1] -100.000000 -2.319327 -2.073106 -2.050308 5.000000 detect_outliers(x, cutoff = 2.24) # following Wilcox (2003) ## [1] -100 Notice that five does not get flagged as an outlier when using the standard \\(z\\)-score based on the sample mean and standard deviation. This is due to the fact that the extreme value -100 skews these descriptive statistics. Using the robust method, however, returns reasonable results. Using the empirical rule is rather limited in practice since distributions are often not bell-shaped, or even symmetric‚Äîthough, as seen with Sale_Price, some can be transformed to appear more bell-shaped. A better outlier detection rule can be constructed using the IQR. This is the same method used to flag outliers in boxplots (see Section 3.2.1.3). The general rule is to define an observation an outlier if it lies outside the interval \\(\\left(Q_1 - 1.5 \\times IQR, Q_2 + 1.5 \\times IQR\\right)\\). While it would be simple to write our own function for detecting outliers using the boxplot method, we can use R‚Äôs built-in function boxplot.stats() (see ?grDevices::boxplot.stats for details). Notice how this method happens to catch the true outliers! boxplot.stats(x)$out ## [1] 5 -100 We leave it to Exercise ?? to further explore outliers for the Sale_Price variable in the ames housing data frame. 2.7 Describing categorical data All of the descriptive statistics discussed thus far are appropriate for continuous variables3‚Äîthat is, variables that can be measured on a continuum (e.g., the weight of an object measured in grams). Technically, no variable can be truly measured on a continuous scale due to precision limitations with how all variables are measured. So, in a sense, continuous variables in practice are numeric variables that can take on a lot of values. Oftentimes the data we are describing contains categorical variables. A categorical variable is a variable whose measurement scale consists of a set of categories (e.g., manufacturer and gender). Such variables are easy to describe using contingency or cross-classification tables (e.g., tables of frequencies and proportions). Categorical variables fall into one of two types: nominal and ordinal. Nominal variables are categorical variables whose categories do no have a natural ordering. The categories of an ordinal variable do have a natural ordering, but no defined distance between the categories. For example, the AdultUCI data frame contains the columns income (with unique categories &quot;small&quot; and &quot;large&quot;) and sex (with unique categories &quot;Female&quot; and &quot;Male&quot;). income would be an example of an ordered variable (since &quot;small&quot; &lt; &quot;large&quot;, but &quot;large&quot; - &quot;small&quot; has no meaningful interpretation) while sex is nominal. In R, categorical variables are typically represented using the &quot;factor&quot; class, but can also be represented by character strings (i.e., the more general &quot;character&quot; class). The AdultUCI data set contains several factors: which(sapply(AdultUCI, is.factor)) # positions and names of factor columns ## workclass education marital-status occupation relationship ## 2 4 6 7 8 ## race sex native-country income ## 9 10 14 15 which(sapply(AdultUCI, is.character)) # sanity check ## named integer(0) which(sapply(AdultUCI, is.ordered)) # check specifically for ordered factors ## education income ## 4 15 To coerce a variable into a nominal or ordinal factor, we can use the functions as.factor() and as.ordered(), respectively. Some of the techniques discussed in this book can take ordinality into account, so it is good practice to make sure such variables are coerced to ordered factors. 2.7.1 Contingency tables A contingency table is a rectangular table containing the frequencies (or proportions) of observations within the different categories of one or more categorical variables. Contingency tables typically cross-classifiy two categorical variables, but can be used to cross-classifiy more than two. R contains a number of functions for creating such tables, the most useful probably being xtabs() since it has a formula interface. To illustrate, we construct a \\(2 \\times 2\\) table cross-classifying income and sex from the AdultUCI data frame: (tab &lt;- xtabs(~ sex + income, data = AdultUCI)) ## income ## sex small large ## Female 9592 1179 ## Male 15128 6662 This table shows some disparity in income between females and males. In Chapter 4, we discuss ways of testing for association between a set of categorical variables. We can also request that margins (i.e., row/column summaries) be added to the table using the addmargins() function: addmargins(tab) # defaults to adding row/column totals ## income ## sex small large Sum ## Female 9592 1179 10771 ## Male 15128 6662 21790 ## Sum 24720 7841 32561 addmargins(tab, FUN = mean) # add sample means to the margins instead ## Margins computed over dimensions ## in the following order: ## 1: sex ## 2: income ## income ## sex small large mean ## Female 9592.00 1179.00 5385.50 ## Male 15128.00 6662.00 10895.00 ## mean 12360.00 3920.50 8140.25 We can can convert the frequencies to proportions using the prop.table() function: prop.table(tab) ## income ## sex small large ## Female 0.29458555 0.03620896 ## Male 0.46460490 0.20460060 In Section ??, we discuss a statistic that can be used to quantify the strength of the association between two categorical variables. 2.8 Exercises Exercise 2.1 (Groupwise descriptive statistics) For the ames data set, we computed various measures of spread and location for the variable Sale_Price. However, these data span multiple years (2006‚Äì2010). Therefore, it might be of interest to see how various descriptive statistics change over time. For this exercise, compute the sample median and MADN for Sale_Price stratified by the year in which the house was sold (i.e., Year_Sold). Hint: use the built-in functions tapply() or by(); see ?tapply and ?by for example usage. Do typical sale prices seem to be different across the five years? Exercise 2.2 (Outlier detection) Use the boxplot outlier detection method on the variable Sale_Price from the ames housing example. How many outliers are detected using this method? How many outliers are detected using the empirical rule? Does the empirical rule seem appropriate for these data? Why or why not? Exercise 2.3 (Kurtosis and skewness) TBD. Exercise 2.4 (The apply() function) The votes.repub data set from the standard R package cluster (Maechler et al. 2016) contains the percentages of votes given to the republican candidate in presidential elections from 1856 to 1976. The rows represent the 50 US states, and the columns represent the 31 elections. The data can be loaded into R using data(votes.repub, package = &quot;cluster&quot;). In this case, we may be interested in computing various descriptive statistics for each state across all years. Since the data are stored in rectangular format (i.e., state are in rows and years are in columns) we can efficiently compute descriptive statistics for each state using R‚Äôs built-in apply() function (see ?apply for details). Using apply(), compute the sample mean percentage of votes and standard deviation for each state, taking care to handle NA‚Äôs appropriately. Which state had the highest average percentage of votes? Which had the lowest? Exercise 2.5 (Trimmed mean) TBD. Exercise 2.6 (Sample variance r emo::ji('scream')) Show that the formula for the sample is equivalent to \\[ s^2 = \\left(\\frac{1}{n} \\sum_{i = 1}^n x_i^2\\right) - \\bar{x}^2. \\] References "]
]
