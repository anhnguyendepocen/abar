# Gradient Boosting Machines {#GBM}

```{r gbm-ch12-setup, include=FALSE}

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE, 
  message = FALSE, 
  collapse = TRUE, 
  fig.align = "center",
  fig.height = 3.5
)
```

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests (Chapter \@ref(RF)) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This chapter will cover the fundamentals to understanding and implementing GBMs.

## Prerequisites

For this chapter we’ll use the following packages:

```{r gbm-prereq-pkgs}
library(rsample)  # data splitting
library(gbm)
library(caret)
library(vip)
library(pdp)
```


To illustrate the various concepts we’ll continue focusing on the Ames Housing data (regression); however, at the end of the chapter we’ll also fit a GBM model to the employee attrition data (classification).

```{r gbm-prereq-data}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


## The basic idea

Thus far, we have discussed several supervised machine learning algorithms that are founded on a single predictive model such as linear ($\S$ \@ref(linear-regression)) and logistic regression ($\S$ \@ref(logistic-regression)), regularized regression ($\S$ \@ref(regularized-regression)), and multivariate adaptive regression splines ($\S$ \@ref(MARS)). We have also introduced other approaches such as bagging and random forests ($\S$ \@ref(RF)) that are built on the idea of building an ensemble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values. The family of boosting methods is based on a different, constructive strategy of ensemble formation.

The main idea of boosting is to add new models to the ensemble ___sequentially___. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.  

```{r sequential-fig, echo=FALSE, fig.align='center', fig.cap="Sequential ensemble approach.", out.height="75%", out.width="75%"}
knitr::include_graphics("illustrations/boosted-trees-process.png")
```

Let's discuss each component of the previous sentence in closer detail because they are important.

__Base-learning models__:  Boosting is a framework that iteratively improves _any_ weak learning model.  Many gradient boosting applications allow you to "plug in" various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this chapter will discuss boosting in the context of decision trees.

__Training weak models__: A weak model is one whose error rate is only slightly better than random guessing.  The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors.  With regards to decision trees, shallow trees represent a weak learner.  Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits:

- Speed: Constructing weak models is computationally cheap. 
- Accuracy improvement: Weak models allow the algorithm to _learn slowly_; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.
- Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).

__Sequential training with respect to errors__: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where _x_ represents our features and _y_ represents our response:

1. Fit a decision tree to the data: $F_1(x) = y$,
2. We then fit the next decision tree to the residuals of the previous: $h_1(x) = y - F_1(x)$,
3. Add this new tree to our algorithm: $F_2(x) = F_1(x) + h_1(x)$,
4. Fit the next decision tree to the residuals of $F_2$: $h_2(x) = y - F_2(x)$,
5. Add this new tree to our algorithm: $F_3(x) = F_2(x) + h_1(x)$,
6. Continue this process until some mechanism (i.e. cross validation) tells us to stop.

The basic algorithm for boosted decision trees can be generalized to the following where the final model is simply a stagewise additive model of *b* individual trees:

$$ f(x) =  \sum^B_{b=1}f^b(x) \tag{1} $$

To illustrate the behavior, assume the following *x* and *y* observations.  The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise).  The boosted prediction (red) illustrates the adjusted predictions after additional sequential trees are added to the algorithm.  Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. If enough trees are added, the algorithm can overfit. As Figure \@ref(fig:boosted-tree) shows, the predicted values approximately converge to the true underlying function at around 50-100 iterations; however, we definitely see that at larger iterations (500-1000) the predicted value becomes highly variable to the noise in the data.

```{r boosted-tree, echo=FALSE, fig.align='center', fig.cap="Boosted regression tree predictions illustrating how gradient boosted trees sequentially reduce the error of a model based on the sin function.", fig.height=6, fig.width=9}
# Nicer color palette
cols <- RColorBrewer::brewer.pal(9, "Set1")

# Function to implement gradient boosting with squared-error loss. Based on
# algorithm 17.2 on page 333 of Computer Age Statistical Inference, by Bradley
# Efron amd Trevor Hastie
rpartBoost <- function(X, y, data, num_trees = 100, learn_rate = 0.1, 
                       tree_depth = 6, verbose = FALSE) {
  require(rpart)
  G_b_hat <- matrix(0, nrow = length(y), ncol = num_trees + 1)
  r <- y
  for (tree in seq_len(num_trees)) {
    if (verbose) {
      message("iter ", tree, " of ", num_trees)
    }
    g_b_tilde <- rpart(r ~ X, control = list(cp = 0, maxdepth = tree_depth))
    g_b_hat <- learn_rate * predict(g_b_tilde)
    G_b_hat[, tree + 1] <- G_b_hat[, tree] + matrix(g_b_hat)
    r <- r - g_b_hat
  }
  colnames(G_b_hat) <- paste0("tree_", c(0, seq_len(num_trees)))
  G_b_hat
}

# Function to plot the predictions from a particular boosting iteration
plotIter <- function(object, iter, show_legend = FALSE, ...) {
  plot(x, y, ...)
  lines(x, sin(x), lwd = 3, col = cols[2L])
  lines(x, object[, iter + 1], lwd = 3, col = cols[1L])
  if (show_legend) {
    legend("topright", legend = c("Boosted prediction", "True function"),
           lty = 1L, lwd = 3L, col = cols[1L:2L], inset = 0.01)
  }
}

# Simulate some sine wave data
set.seed(101)
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
#plot(x, y)

# gradient boosted decision trees
bst <- rpartBoost(X = x, y = y, num_trees = 1000, learn_rate = 0.1, 
                  tree_depth = 3, verbose = FALSE)

# Plot first 15 iterations
# par(mfrow = c(3, 3))
# for (i in c(0, 5, 10, 15, 25, 50, 100, 500, 1000)) {
#   plotIter(bst, iter = i, main = paste("Iter:", i))
# }

# plot 9 different iterations
sin_data <- data.frame(x, y, sin = sin(x))

as.data.frame(bst) %>% 
  cbind(sin_data) %>%
  gather(iteration, value, tree_0:tree_1000) %>% 
  filter(iteration %in% paste0("tree_", c(0, 5, 10, 15, 25, 50, 100, 500, 1000))) %>%
  mutate(
    iteration = paste("Iteration", stringr::str_extract(iteration, "[[:digit:]]+")),
    iteration = as.factor(iteration) %>% fct_inorder()
    ) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .1) +
  geom_line(aes(x, sin), color = "blue", size = 1) +
  geom_line(aes(y = value), color = "red", size = 1) +
  facet_wrap(~ iteration)
```

## Gradient descent {#gbm-gradient}

Many algorithms, including decision trees, focus on minimizing the residuals and, therefore, emphasize the MSE loss function.  The algorithm discussed above outlines the approach of sequentially fitting regression trees to minimize the errors.  This specific approach is how gradient boosting minimizes the mean squared error (MSE) loss function.  However, often we wish to focus on other loss functions such as mean absolute error (MAE) or to be able to apply the method to a classification problem with a loss function such as deviance or logloss. The name ___gradient___ boosting machine comes from the fact that this procedure can be generalized to loss functions other than MSE.

Gradient boosting is considered a ___gradient descent___ algorithm. Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a loss function. Suppose you are a downhill skier racing your friend.  A good strategy to beat your friend to the bottom is to take the path with the steepest slope. This is exactly what gradient descent does - it measures the local gradient of the loss  function for a given set of parameters ($\Theta$) and takes steps in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum.

```{r gradient-descent-fig, echo=FALSE, fig.align='center', fig.cap="Gradient descent is the process of gradually decreasing the cost function (i.e. MSE) by tweaking parameters iteratively until you have reached a minimum.", out.height="95%", out.width="95%"}

# create data to plot
x <- seq(-5, 5, by = .05)
y <- x^2 + 3
df <- data.frame(x, y)

step <- 5
step_size <- .2
for(i in seq_len(18)) {
  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)
  step <- c(step, next_step)
  next
}

steps <- df[step, ] %>%
  mutate(x2 = lag(x), y2 = lag(y)) %>%
  slice(1:18)

# plot
ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Loss function", limits = c(0, 30)) +
  xlab(expression(theta)) +
  geom_segment(data = df[c(5, which.min(df$y)), ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = filter(df, y == min(y)), aes(x, y), size = 4, shape = 21, fill = "yellow") +
  geom_point(data = steps, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = steps, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = df[5, "x"], y = 1, label = "Random initial value", hjust = -0.1, vjust = .8) +
  annotate("text", x = df[which.min(df$y), "x"], y = 1, label = "Minimium", hjust = -0.1, vjust = .8) +
  annotate("text", x = df[5, "x"], y = df[5, "y"], label = "Learning step", hjust = -.8, vjust = 0)
```

Gradient descent can be performed on any loss function that is differentiable.  Consequently, this allows GBMs to optimize different loss functions as desired (see @friedman2001elements, p. 360 for common loss functions). An important parameter in gradient descent is the size of the steps which is determined by the _learning rate_. If the learning rate is too small, then the algorithm will take many iterations to find the minimum. On the other hand, if the learning rate is too high, you might jump across the minimum and end up further away than when you started. 

```{r learning-rate-fig, echo=FALSE, fig.align='center', fig.cap="A learning rate that is too small will require many iterations to find the minimum. A learning rate too big may jump over the minimum.", fig.height=3.5, fig.width=9}
# create too small of a learning rate
step <- 5
step_size <- .05
for(i in seq_len(10)) {
  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)
  step <- c(step, next_step)
  next
}

too_small <- df[step, ] %>%
  mutate(x2 = lag(x), y2 = lag(y))

# plot
p1 <- ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Loss function", limits = c(0, 30)) +
  xlab(expression(theta)) +
  geom_segment(data = too_small[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = too_small, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = too_small, aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = df[5, "x"], y = 1, label = "Start", hjust = -0.1, vjust = .8) +
  ggtitle("a) too small")

# create too large of a learning rate
too_large <- df[round(which.min(df$y) * (1 + c(-.2, .3, -.4, .5, -.6, .7, -.8, .9)), 0), ] %>%
  mutate(x2 = lag(x), y2 = lag(y))

# plot
p2 <- ggplot(df, aes(x, y)) +
  geom_line(size = 1.5, alpha = .5) +
  theme_classic() +
  scale_y_continuous("Loss function", limits = c(0, 30)) +
  xlab(expression(theta)) +
  geom_segment(data = too_large[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +
  geom_point(data = too_large, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +
  geom_curve(data = too_large, aes(x = x, y = y, xend = x2, yend = y2), curvature = 0, lty = "dotted") +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  annotate("text", x = too_large[1, "x"], y = 1, label = "Start", hjust = -0.1, vjust = .8) +
  ggtitle("b) too big")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

Moreover, not all cost functions are convex (bowl shaped). There may be local minimas, plateaus, and other irregular terrain of the loss function that makes finding the global minimum difficult.  ___Stochastic gradient descent___ can help us address this problem by sampling a fraction of the training observations (typically without replacement) and growing the next tree using that subsample.  This makes the algorithm faster but the stochastic nature of random sampling also adds some random nature in descending the loss function gradient.  Although this randomness does not allow the algorithm to find the absolute global minimum, it can actually help the algorithm jump out of local minima and off plateaus and get near the global minimum. 

```{r stochastic-gradient-descent-fig, echo=FALSE, fig.align='center', fig.cap="Stochastic gradient descent will often find a near-optimal solution by jumping out of local minimas and off plateaus.", out.height="95%", out.width="95%"}
# create random walk data
set.seed(123)
x <- sample(seq(3, 5, by = .05), 10, replace = TRUE)
set.seed(123)
y <- seq(2, 28, length.out = 10)

random_walk <- data.frame(
  x = x,
  y = y[order(y, decreasing = TRUE)]
)

optimal <- data.frame(x = 0, y = 0)

# plot
ggplot(df, aes(x, y)) + 
  coord_polar() +
  theme_minimal() +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  xlab(expression(theta[1])) +
  ylab(expression(theta[2])) +
  geom_point(data = random_walk, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) + 
  geom_point(data = optimal, aes(x, y), size = 2, shape = 21, fill = "yellow") + 
  geom_path(data = random_walk, aes(x, y), lty = "dotted") +
  annotate("text", x = random_walk[1, "x"], y = random_walk[1, "y"], label = "Start", hjust = 1, vjust = -1) +
  annotate("text", x = optimal[1, "x"], y = optimal[1, "y"], label = "Minimum", hjust = -.2, vjust = 1) +
  ylim(c(0, 28)) + 
  xlim(-5, 5)
```

As we'll see in the next section, there are several hyperparameter tuning options that allow us to address how we approach the gradient descent of our loss function.

## Tuning {#gbm-tuning}

Part of the beauty and challenges of GBMs is that they offer several tuning parameters.  The beauty in this is GBMs are highly flexible.  The challenge is that they can be time consuming to tune and find the optimal combination of hyperparamters.  The most common hyperparameters that you will find in most GBM implementations include:

* __Number of trees:__ The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation.
* __Depth of trees:__ The number *d* of splits in each tree, which controls the complexity of the boosted ensemble. Often $d = 1$ works well, in which case each tree is a _stump_ consisting of a single split. More commonly, *d* is greater than 1 but it is unlikely $d > 10$ will be required.
* __Learning rate:__ Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called _shrinkage_.
* __Subsampling:__ Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing _stochastic gradient descent_.  This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradient.

Throughout this chapter you'll be exposed to these hyperparameters and, possibly others, that can improve performance and/or the efficiency of training and tuning models.


## Fitting a basic GBM

There are many packages that implement GBMs and GBM variants. You can find a fairly comprehensive list at the [CRAN Machine Learning Task View](https://cran.r-project.org/web/views/MachineLearning.html). However, the original implementation of Friedman's GBM algorithm [@friedman2001greedy; @friedman2002stochastic] is the __gbm__ package [@R-gbm].  

__gbm__ has two primary training functions - `gbm::gbm()` and `gbm::gbm.fit()`. The primary difference is that `gbm::gbm()` uses the formula interface to specify your model whereas `gbm::gbm.fit()` requires the separated `x` and `y` matrices.  When working with _many_ variables it is more efficient to use the matrix rather than formula interface.

The default settings in __gbm__ includes a learning rate (`shrinkage`) of 0.001. This is a very small learning rate and typically requires a large number of trees to find the minimum MSE.  However, __gbm__ uses a default number of trees of 100, which is rarely sufficient.  Consequently, if you use the learning rate you will likely want to significantly increase the number of trees applied.  The default depth of each tree (`interaction.depth`) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include `cv.folds` to perform a 5 fold cross validation.  

```{block, type="note"}
The model took 48 seconds to run and the results show that our MSE loss function is minimized with 5,000 trees.
```

```{r, eval=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_mod1 <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

print(gbm_mod1)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train, 
##     n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 5000 iterations were performed.
## The best cross-validation iteration was 5000.
## There were 80 predictors of which 29 had non-zero influence.
```

The output object is a list containing several modelling and results information.  We can access this information with regular indexing; it is recommended you take some time to dig around in the object to get comfortable with its components.  Here, we see that the minimum CV RMSE is \$33,079.61 but Figure \@ref(fig:gbm1-gradient-descent) also illustrates that the CV error (MSE) is still decreasing at 5,000 trees.  


```{r, eval=FALSE}
# get MSE and compute RMSE
sqrt(min(gbm_mod1$cv.error))
## [1] 33079.61

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod1, method = "cv")
```

```{r gbm1-gradient-descent, echo=FALSE, fig.cap="Training and cross-validated MSE as *n* trees are added to the GBM algorithm."}
knitr::include_graphics("illustrations/gbm1_gradient_descent.png")
```

In this case, the small learning rate is resulting in very small incremental improvements which means ___many___ trees are required.  In fact, with the default learning rate and tree depth settings, the CV error is still reducing after 10,000 trees!

## Tuning

As we've seen in the previous chapters, rarely do the default settings suffice.  We could tune parameters one at a time to see how the results change.  For example, here, I increase the learning rate to take larger steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 3 splits. Our RMSE (\$23,813.34) is lower than our initial model and the optimal number of trees required was 964.

```{r, eval=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_mod2 <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(gbm_mod2$cv.error)

# get MSE and compute RMSE
sqrt(gbm_mod2$cv.error[min_MSE])
## [1] 23813.34

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_mod2, method = "cv")
```

```{r gbm2-gradient-descent, echo=FALSE, fig.cap="Training and cross-validated MSE as *n* trees are added to the GBM algorithm. We can see that are new hyperparameter settings result in a much quicker progression down the gradient descent than our initial model."}
knitr::include_graphics("illustrations/gbm2_gradient_descent.png")
```

As in the other chapters, rather than manually tweaking hyperparameters one at a time, we'll perform a grid search to iterate over specified combinations of hyperparameter values.  The following constructs a grid search of 81 models with varying learning rates (`shrinkage`), tree depth (`interaction.depth`), and tree complexity (`n.minobsinnode`) parameter values.  

```{block, type = "note"}
Although we do not vary the number of trees (`n.trees`), `caret::train()` still requires us to include that parameter in the grid.
```


```{r}
gbm_grid <- expand.grid(
  interaction.depth = seq(1, 5, by = 2),
  shrinkage = c(.01, .1, .3),
  n.minobsinnode = c(5, 10, 15),
  n.trees = 5000
)
```

Our grid search revealed a few important attributes.  First, our top model has better performance than the models fit in previous chapters, with an RMSE of \$22,424. Second, looking at the top 10 models (`gbm_tune$results` or as illustrated in Figure \@ref(fig:gbm-tune-results)) you will see that:

- all the top models used a learning rate of 0.1 or smaller; small incremental steps down the gradient descent appears to work best,
- all the top models used deeper trees (`interaction.depth = 5`); there are likely some important interactions that the deeper trees are able to capture,
- allowing terminal nodes to have a smaller number of observations in them (`n.minobsinnode`) appears to improve model performance; there are likely pockets of price points or outliers that this allows our model to better capture. 

```{block, type="warning"}
This grid search took 51 minutes to complete.
```

```{r, eval=FALSE}
# create train() parameters
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)

# cross validated model
gbm_tune <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "gaussian",
  metric = "RMSE",
  tuneGrid = gbm_grid,
  trControl = kfold,
  verbose = FALSE
)

# plot results
ggplot(gbm_tune)
```

```{r gbm-tune-results, echo=FALSE, fig.cap="Model performance of our GBM grid search.  Smaller learning rate, greater tree depth, and terminal nodes with smaller number of observations all appear to help minimize error."}
knitr::include_graphics("illustrations/gbm-xval-tuned.png")
```


```{r, eval=FALSE}
# best model
gbm_tune$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 7    5000                 5      0.01              5
```

These results help us to zoom into areas where we can refine our search.  In practice, tuning is an iterative process so you would likely refine this search grid to analyze a search space around the top models.  For example, I would likely search the following values in my next grid search:

* learning rate: 0.01, 0.005
* interaction depth: 5, 7, 9
* minimum number of obs in terminal node: 1, 3, 5

You may also want to assess the impact of including a stochastic gradient descent with `bag.fraction`.  Also, since this grid search would use a smaller learning rate, you may want to increasethe number of trees to ensure you are converging to a minimal error.

```{block, type = "note"}
We'll leave this additional grid search to the reader to perform on their own!
```


```{r, eval=FALSE, echo=FALSE}
# used this code chunk to find minimum RMSE with additional parameters
gbm_grid2 <- expand.grid(
  interaction.depth = 5,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.trees = c(5000, 7500, 10000)
)

# create train() parameters
features <- subset(ames_train, select = -Sale_Price) %>% as.data.frame()
response <- ames_train$Sale_Price
kfold <- trainControl(method = "cv", number = 5)

# cross validated model
gbm_tune2 <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "gaussian",
  metric = "RMSE",
  tuneGrid = gbm_grid2,
  trControl = kfold,
  verbose = FALSE
)

gbm_tune2$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 3   10000                 5      0.01              5
min(gbm_tune2$results$RMSE) # --> CV RMSE was 22084.87
```

Once we have found our top model we want to re-train the model within GBM and with the specific parameters. This is because `caret::train()` does not provide you with the optimal number of trees but, rather, uses all the trees specified. Re-running a __gbm__ model with the optimal hyperparameters and performing cross validation actually shows the optimal number of trees is 4911.  Consequently, the following applies our top model parameters to get our final model.  

```{r, echo=FALSE}
# for reproducibility
set.seed(123)

# train GBM model
gbm_final_fit <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 4911,
  interaction.depth = 5,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.cores = NULL, 
  verbose = FALSE
  )  
```


## Feature Interpretation

Similar to random forests, GBMs make no assumption regarding the linearity and monoticity of the predictor-response relationship. So as we did in the random forest chapter (Chapter \@ref(RF)) we can understand the relationship between the features and the response using variable importance plots and partial dependence plots.

After re-running our final model we likely want to understand the variables that have the largest influence on our response variable.  The `summary()` method for __gbm__ will output a data frame and a plot that shows the most influential variables.  `cBars` allows you to adjust the number of variables to show (in order of influence). The default method for computing variable importance is with relative influence but your options include: 

1. `method = relative.influence`: At each split in each tree, `gbm` computes the improvement in the split-criterion (MSE for regression). `gbm` then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.
2. `method = permutation.test.gbm`: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important.

```{block, type = "tip"}
Each measure of importance has its strengths and weaknesses.  The objective is to find consistency among the top influential variables.  In the case of this model, it is pretty clear that `Overall_Qual`, `Gr_Liv_Area`, and `Neighborhood` are the top 3 most influential variables; however, the exact order of influence is debateable.
```


```{r vip1, eval=FALSE}
par(mfrow = c(1, 2), mar = c(5, 10, 1, 1))
# relative influence approach
summary(gbm_final_fit, cBars = 10, method = relative.influence, las = 2)
# permutation approach
summary(gbm_final_fit, cBars = 10, method = permutation.test.gbm, las = 2)
```

```{r gbm-gbm-vip-plot, echo=FALSE, out.height="100%", out.width="100%", fig.cap="Top 10 influential variables using the relative influence (left) and permutation (right) approach. We can see common themes among the top variables although in differing order."}
knitr::include_graphics("illustrations/gbm-gbm-vip.png")
```

As in the previous chapter, we can use PDP plots and ICE curves to better understand how these influential variables impact our model's performance. For example, we see a similar non-linear relationship between `Gr_Liv_Area` and the predicted sale value as we did with our random forest model.  By increasing the grid resolution to 100 we can see some unique behaviour when `Gr_Liv_Area` ranges from 2500-3500.  

```{r, fig.cap="As `Gr_Liv_Area` increases we see a nonlinear relationship to home sales price."}
# PDP plot
gbm_pdp <- gbm_final_fit %>%
  partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = ames_train) +
  ggtitle("PDP plot") +
  scale_y_continuous(labels = scales::dollar)

# ICE curves
gbm_ice <- gbm_final_fit %>%
  partial(pred.var = "Gr_Liv_Area", n.trees = gbm_final_fit$n.trees, grid.resolution = 100, ice = TRUE) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered ICE curves") +
  scale_y_continuous(labels = scales::dollar)

gridExtra::grid.arrange(gbm_pdp, gbm_ice, nrow = 1)
```

Looking at the other two top influential variables we see that as the quality of the home increases... 

```{r, echo=FALSE}
gbm_final_fit %>%
  partial(pred.var = "Overall_Qual", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train), ice = TRUE) %>%
  autoplot(center = TRUE, alpha = 0.03) +
  scale_y_continuous(labels = scales::dollar)
```


We also see that one neighbhorhood (Greens) sticks out above the others...

```{r, echo=FALSE, fig.height=6}
gbm_final_fit %>%
  partial(pred.var = "Neighborhood", n.trees = gbm_final_fit$n.trees, train = as.data.frame(ames_train)) %>%
  ggplot(aes(yhat, reorder(Neighborhood, yhat))) +
  geom_point() +
  ylab(NULL) +
  scale_x_continuous(labels = scales::dollar)

```


## Attrition data


```{r, eval=FALSE}
# get attrition data
df <- rsample::attrition %>% dplyr::mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets for the rsample::attrition data.
# Use set.seed for reproducibility
set.seed(123)
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)

# create a tuning grid
gbm_grid <- expand.grid(
  interaction.depth = seq(1, 5, by = 2),
  shrinkage = c(.01, .1, .3),
  n.minobsinnode = c(5, 10, 15),
  n.trees = c(5000, 10000)
)

# create train() parameters
features <- subset(churn_train, select = -Attrition) %>% as.data.frame()
response <- churn_train$Attrition
kfold <- trainControl(method = "cv", number = 10)

# cross validated model
gbm_churn <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "bernoulli",
  tuneGrid = gbm_grid,
  trControl = kfold,
  verbose = FALSE
)

gbm_churn$bestTune
##   n.trees interaction.depth shrinkage n.minobsinnode
## 1    5000                 1      0.01              5
min(gbm_churn$results$RMSE)
## [1] 0.8718578
```



```{r gbm-attrition-model-comparison, echo=FALSE, eval=FALSE}
# train logistic regression model
glm_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
  )

# train regularized logistic regression model
penalized_mod <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
  )

# train mars model
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
  )
tuned_mars <- train(
  x = subset(churn_train, select = -Attrition),
  y = churn_train$Attrition,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# train rf model
hyper_grid <- expand.grid(
  mtry            = seq(3, 18, by = 3),
  min.node.size   = seq(1, 10, by = 3),
  splitrule       = c("gini", "extratrees")
  )
tuned_rf <- train(
  x = subset(churn_train, select = -Attrition),
  y = churn_train$Attrition,
  method = "ranger",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid,
  num.trees = 500,
  seed = 123
)

# train gbm model
gbm_grid <- expand.grid(
  interaction.depth = 1,
  shrinkage = 0.01,
  n.minobsinnode = 5,
  n.trees = c(5000, 10000)
)
gbm_churn <- train(
  x = features,
  y = response,
  method = "gbm",
  distribution = "bernoulli",
  tuneGrid = gbm_grid,
  trControl = trainControl(method = "cv", number = 10),
  verbose = FALSE
)

# extract out of sample performance measures
gbm_attrition_model_comparison <- summary(resamples(list(
  Logistic_model = glm_mod, 
  Elastic_net = penalized_mod,
  MARS_model = tuned_mars,
  RF_model = tuned_rf,
  GBM_model = gbm_churn
  )))$statistics$Accuracy

saveRDS(gbm_attrition_model_comparison, file = "data/gbm_attrition_model_comparison.rds")
```

```{r, echo=FALSE}
readRDS("data/gbm_attrition_model_comparison.rds") %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```


## Final thoughts


## Learning more
