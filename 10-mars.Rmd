# Multivariate Adaptive Regression Splines {#MARS}

```{r ch10-setup, include=FALSE}

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE, 
  message = FALSE, 
  collapse = TRUE, 
  fig.align = "center",
  fig.height = 3.5
)
```

The previous chapters discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects); however, do so you must know the specific nature of the nonlinearity a priori.  Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy. 

This chapter discusses multivariate adaptive regression splines (MARS), an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of multiple linear regression. Future chapters will focus on other nonlinear algorithms.


## Prerequisites

For this section we will use the following packages:

```{r 10-pkgs, message=FALSE}
library(rsample)   # data splitting 
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
```

To illustrate various MARS modeling concepts we will use the Ames Housing data; however, at the end of the chapter we will also apply a MARS model to the employee attrition data.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

## The basic idea

In the previous chapters, we focused on linear models. We illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients.  However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictiv

The problem with linear models...making global assumptions...ISLR


```{r nonlinear-comparisons, echo=FALSE, fig.height=7, fig.width=8, fig.cap="Blue line represents predicted `Sale_Price` values as a function of `Year_Built` for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional nonlinear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting `Year_Built` into three categorical levels."}
p1 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("(A) Linear regression")

p2 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 2, raw = TRUE)) +
  ggtitle("(B) Degree-2 polynomial regression")

p3 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 3, raw = TRUE)) +
  ggtitle("(C) Degree-3 polynomial regression")

# fit step function model (3 steps)
step_fit <- lm(Sale_Price ~ cut(ames_train$Year_Built, 3), data = ames_train)
step_pred <- predict(step_fit, ames_train)

p4 <- ggplot(cbind(ames_train, step_pred), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = step_pred), size = 1, color = "blue") +
  ggtitle("(D) Step function regression")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```


### Basis functions



### Regression splines


```{r}
mars1 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 2, nprune = 2, pmethod = "forward")
pred1 <- predict(mars1, ames_train)

mars2 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 3, nprune = 3, pmethod = "forward")
pred2 <- predict(mars2, ames_train)

mars3 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 4, nprune = 4, pmethod = "forward")
pred3 <- predict(mars3, ames_train)

mars4 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 5, nprune = 5, pmethod = "forward")
pred4 <- predict(mars4, ames_train)

p1 <- ggplot(mutate(ames_train, predicted = pred1), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(A) One knot")

p2 <- ggplot(mutate(ames_train, predicted = pred2), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(B) Two knots")

p3 <- ggplot(mutate(ames_train, predicted = pred3), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(C) Three knots")

p4 <- ggplot(mutate(ames_train, predicted = pred4), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("(D) Four knots")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```



### Tuning


```{r}
mars9 <- earth::earth(Sale_Price ~ Year_Built, data = ames_train, degree = 10, nprune = 10, pmethod = "forward")
pred9 <- predict(mars9, ames_train)

ggplot(mutate(ames_train, predicted = pred9), aes(Year_Built, Sale_Price)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue")
```


## Fitting MARS models

```{r}
# Fit a second-degree MARS model
ames_mars <- earth(
  Sale_Price ~ .,  #<<
  data = ames_train,
  degree = 2  # tuning parameter #<<
)

# Print model summary
print(ames_mars)
```


```{r}
summary(ames_mars)
```


```{r}
plot(ames_mars)
```


## Tuning



```{r}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
  )

head(hyper_grid)
```


```{block, type="warning"}
This tuning took 5 minutes to complete.
```

```{r}
# for reproducibiity
set.seed(123)

# cross validated model
tuned_mars <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# best model
tuned_mars$bestTune

# plot results
ggplot(tuned_mars)
```



So how does this compare to our previous best model for the Ames data?  

```{r cv-model-comparison, echo=FALSE}
set.seed(123)
cv_model1 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

set.seed(123)
cv_model2 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train,
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# extract out of sample performance measures
summary(resamples(list(
  Multiple_regression = cv_model1, 
  PCR = cv_model2, 
  PLS = cv_model3,
  Elastic_net = cv_model4,
  MARS = tuned_mars
  )))$statistics$RMSE
```


## Feature interpretation


```{r}
vip(tuned_mars, num_features = 40, bar = FALSE)
```


```{r}
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% autoplot()
p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```




## Attrition data


## Final thoughts



__FIXME: refine this section__

__Advantages__:



__Disadvantages__:





## Learning more

This will get you up and running with MARS modeling.  Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)
- [Elements of Statistical Learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)

